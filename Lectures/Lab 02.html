<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.41">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Lab 02: Credit Scorecard Development</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="Lab 02_files/libs/clipboard/clipboard.min.js"></script>
<script src="Lab 02_files/libs/quarto-html/quarto.js"></script>
<script src="Lab 02_files/libs/quarto-html/popper.min.js"></script>
<script src="Lab 02_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="Lab 02_files/libs/quarto-html/anchor.min.js"></script>
<link href="Lab 02_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Lab 02_files/libs/quarto-html/quarto-syntax-highlighting-48ffa3e5b9d089919c6712c39e5b00f2.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="Lab 02_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="Lab 02_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="Lab 02_files/libs/bootstrap/bootstrap-b29227339076a9cddc7448887e597caa.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-full">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#setup" id="toc-setup" class="nav-link" data-scroll-target="#setup"><span class="header-section-number">2</span> Setup</a>
  <ul class="collapse">
  <li><a href="#import-python-libraries" id="toc-import-python-libraries" class="nav-link" data-scroll-target="#import-python-libraries"><span class="header-section-number">2.1</span> Import Python Libraries</a></li>
  <li><a href="#helper-functions-and-classes" id="toc-helper-functions-and-classes" class="nav-link" data-scroll-target="#helper-functions-and-classes"><span class="header-section-number">2.2</span> Helper Functions and Classes</a></li>
  </ul></li>
  <li><a href="#data-loading-eda" id="toc-data-loading-eda" class="nav-link" data-scroll-target="#data-loading-eda"><span class="header-section-number">3</span> Data Loading &amp; EDA</a>
  <ul class="collapse">
  <li><a href="#read-parquet-files" id="toc-read-parquet-files" class="nav-link" data-scroll-target="#read-parquet-files"><span class="header-section-number">3.1</span> Read Parquet Files</a></li>
  <li><a href="#define-target-variable" id="toc-define-target-variable" class="nav-link" data-scroll-target="#define-target-variable"><span class="header-section-number">3.2</span> Define Target Variable</a></li>
  <li><a href="#train-calibration-and-test-split" id="toc-train-calibration-and-test-split" class="nav-link" data-scroll-target="#train-calibration-and-test-split"><span class="header-section-number">3.3</span> Train, Calibration, and Test Split</a></li>
  <li><a href="#exploratory-data-analysis-eda" id="toc-exploratory-data-analysis-eda" class="nav-link" data-scroll-target="#exploratory-data-analysis-eda"><span class="header-section-number">3.4</span> Exploratory Data Analysis (EDA)</a></li>
  </ul></li>
  <li><a href="#monotonicity-checks" id="toc-monotonicity-checks" class="nav-link" data-scroll-target="#monotonicity-checks"><span class="header-section-number">4</span> Monotonicity Checks</a>
  <ul class="collapse">
  <li><a href="#identify-and-process-common-features" id="toc-identify-and-process-common-features" class="nav-link" data-scroll-target="#identify-and-process-common-features"><span class="header-section-number">4.1</span> Identify and Process Common Features</a></li>
  <li><a href="#compare-data-distributions" id="toc-compare-data-distributions" class="nav-link" data-scroll-target="#compare-data-distributions"><span class="header-section-number">4.2</span> Compare Data Distributions</a></li>
  <li><a href="#assess-feature-monotonicity-accepted-data" id="toc-assess-feature-monotonicity-accepted-data" class="nav-link" data-scroll-target="#assess-feature-monotonicity-accepted-data"><span class="header-section-number">4.3</span> Assess Feature Monotonicity (Accepted Data)</a></li>
  <li><a href="#define-modeling-features" id="toc-define-modeling-features" class="nav-link" data-scroll-target="#define-modeling-features"><span class="header-section-number">4.4</span> Define Modeling Features</a></li>
  </ul></li>
  <li><a href="#fuzzy-augmentation" id="toc-fuzzy-augmentation" class="nav-link" data-scroll-target="#fuzzy-augmentation"><span class="header-section-number">5</span> Fuzzy Augmentation</a>
  <ul class="collapse">
  <li><a href="#train-initial-model-for-weighting" id="toc-train-initial-model-for-weighting" class="nav-link" data-scroll-target="#train-initial-model-for-weighting"><span class="header-section-number">5.1</span> Train Initial Model for Weighting</a></li>
  <li><a href="#check-ri-model-coefficients" id="toc-check-ri-model-coefficients" class="nav-link" data-scroll-target="#check-ri-model-coefficients"><span class="header-section-number">5.2</span> Check RI Model Coefficients</a></li>
  <li><a href="#calculate-weights-and-create-ttd-dataset" id="toc-calculate-weights-and-create-ttd-dataset" class="nav-link" data-scroll-target="#calculate-weights-and-create-ttd-dataset"><span class="header-section-number">5.3</span> Calculate Weights and Create TTD Dataset</a></li>
  </ul></li>
  <li><a href="#building-the-ttd-scorecard-model-initial" id="toc-building-the-ttd-scorecard-model-initial" class="nav-link" data-scroll-target="#building-the-ttd-scorecard-model-initial"><span class="header-section-number">6</span> Building the TTD Scorecard Model (Initial)</a>
  <ul class="collapse">
  <li><a href="#configure-and-train-autogluon-model" id="toc-configure-and-train-autogluon-model" class="nav-link" data-scroll-target="#configure-and-train-autogluon-model"><span class="header-section-number">6.1</span> Configure and Train AutoGluon Model</a></li>
  </ul></li>
  <li><a href="#initial-model-diagnostics-pdpice" id="toc-initial-model-diagnostics-pdpice" class="nav-link" data-scroll-target="#initial-model-diagnostics-pdpice"><span class="header-section-number">7</span> Initial Model Diagnostics (PDP/ICE)</a></li>
  <li><a href="#applying-monotonic-constraints" id="toc-applying-monotonic-constraints" class="nav-link" data-scroll-target="#applying-monotonic-constraints"><span class="header-section-number">8</span> Applying Monotonic Constraints</a>
  <ul class="collapse">
  <li><a href="#define-constraints-and-retrain-model" id="toc-define-constraints-and-retrain-model" class="nav-link" data-scroll-target="#define-constraints-and-retrain-model"><span class="header-section-number">8.1</span> Define Constraints and Retrain Model</a></li>
  </ul></li>
  <li><a href="#verifying-constraints-pdpice" id="toc-verifying-constraints-pdpice" class="nav-link" data-scroll-target="#verifying-constraints-pdpice"><span class="header-section-number">9</span> Verifying Constraints (PDP/ICE)</a></li>
  <li><a href="#evaluation-scoring-the-test-set" id="toc-evaluation-scoring-the-test-set" class="nav-link" data-scroll-target="#evaluation-scoring-the-test-set"><span class="header-section-number">10</span> Evaluation &amp; Scoring the Test Set</a>
  <ul class="collapse">
  <li><a href="#prepare-test-set-and-evaluate" id="toc-prepare-test-set-and-evaluate" class="nav-link" data-scroll-target="#prepare-test-set-and-evaluate"><span class="header-section-number">10.1</span> Prepare Test Set and Evaluate</a></li>
  <li><a href="#convert-probability-pd-to-score" id="toc-convert-probability-pd-to-score" class="nav-link" data-scroll-target="#convert-probability-pd-to-score"><span class="header-section-number">10.2</span> Convert Probability (PD) to Score</a></li>
  <li><a href="#build-ks-table" id="toc-build-ks-table" class="nav-link" data-scroll-target="#build-ks-table"><span class="header-section-number">10.3</span> Build KS Table</a></li>
  </ul></li>
  <li><a href="#decision-threshold-selection" id="toc-decision-threshold-selection" class="nav-link" data-scroll-target="#decision-threshold-selection"><span class="header-section-number">11</span> Decision Threshold Selection</a>
  <ul class="collapse">
  <li><a href="#prepare-calibration-data-for-thresholding" id="toc-prepare-calibration-data-for-thresholding" class="nav-link" data-scroll-target="#prepare-calibration-data-for-thresholding"><span class="header-section-number">11.1</span> Prepare Calibration Data for Thresholding</a></li>
  <li><a href="#determine-score-threshold-using-ks-table" id="toc-determine-score-threshold-using-ks-table" class="nav-link" data-scroll-target="#determine-score-threshold-using-ks-table"><span class="header-section-number">11.2</span> Determine Score Threshold using KS Table</a></li>
  <li><a href="#evaluate-model-decisions" id="toc-evaluate-model-decisions" class="nav-link" data-scroll-target="#evaluate-model-decisions"><span class="header-section-number">11.3</span> Evaluate Model Decisions</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">12</span> Conclusion</a></li>
  </ul>
</nav>
</div>
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Lab 02: Credit Scorecard Development</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta column-page-left">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>The lab provides a hands-on guide to building and evaluating a credit scorecard using loan-level (accepts) and applicant-level (rejects) data from LendingClub (an unsecured lender). The primary goal is to develop a 3-digit score that ranks Through-the-Door (TTD) applicants based on their predicted credit risk. We will use Fuzzy Augmentation (a reject inference method) to incorporate data from rejected applicants, to reduce selection bias in the training data.</p>
<p>We will cover data preparation, assessing feature relationships, reject inference using fuzzy augmentation, training models with AutoGluon, diagnosing model behavior with ICE/PDP, applying monotonic constraints, evaluating performance using the KS statistic, and selecting decision thresholds.</p>
<p><strong>Learning Objectives:</strong></p>
<ul>
<li>Load and prepare accepted and rejected lending data.</li>
<li>Define a target variable for credit default.</li>
<li>Perform feature engineering and selection for scorecard modeling.</li>
<li>Assess feature monotonicity using binned probability plots.</li>
<li>Understand and implement reject inference (Fuzzy Augmentation).</li>
<li>Train weighted models using AutoGluon on the augmented TTD dataset.</li>
<li>Diagnose model behavior using ICE and PDP.</li>
<li>Apply monotonic constraints to align models with business logic.</li>
<li>Evaluate scorecard performance using the KS statistic.</li>
<li>Select an optimal decision threshold based on business objectives.</li>
</ul>
</section>
<section id="setup" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Setup</h1>
<section id="import-python-libraries" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="import-python-libraries"><span class="header-section-number">2.1</span> Import Python Libraries</h2>
<div id="setup-imports" class="cell" data-message="false" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># System utilities</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> os</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">import</span> shutil</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="im">import</span> random</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="im">import</span> warnings</span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="im">import</span> time</span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="im">import</span> gc</span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="im">import</span> psutil</span>
<span id="cb1-9"><a href="#cb1-9"></a></span>
<span id="cb1-10"><a href="#cb1-10"></a><span class="co"># Data manipulation and visualization</span></span>
<span id="cb1-11"><a href="#cb1-11"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-13"><a href="#cb1-13"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-14"><a href="#cb1-14"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-15"><a href="#cb1-15"></a><span class="im">from</span> IPython.display <span class="im">import</span> display <span class="co"># Explicit import for display</span></span>
<span id="cb1-16"><a href="#cb1-16"></a><span class="im">from</span> scipy <span class="im">import</span> stats, special</span>
<span id="cb1-17"><a href="#cb1-17"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> mutual_info_classif</span>
<span id="cb1-18"><a href="#cb1-18"></a><span class="im">import</span> re</span>
<span id="cb1-19"><a href="#cb1-19"></a><span class="im">import</span> duckdb</span>
<span id="cb1-20"><a href="#cb1-20"></a></span>
<span id="cb1-21"><a href="#cb1-21"></a><span class="co"># Machine learning - scikit-learn</span></span>
<span id="cb1-22"><a href="#cb1-22"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-23"><a href="#cb1-23"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression, LogisticRegressionCV</span>
<span id="cb1-24"><a href="#cb1-24"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_auc_score, f1_score, confusion_matrix, classification_report</span>
<span id="cb1-25"><a href="#cb1-25"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> LabelEncoder, StandardScaler, OneHotEncoder</span>
<span id="cb1-26"><a href="#cb1-26"></a><span class="im">from</span> sklearn.impute <span class="im">import</span> SimpleImputer</span>
<span id="cb1-27"><a href="#cb1-27"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb1-28"><a href="#cb1-28"></a><span class="im">from</span> sklearn.compose <span class="im">import</span> ColumnTransformer</span>
<span id="cb1-29"><a href="#cb1-29"></a><span class="im">from</span> sklearn.inspection <span class="im">import</span> PartialDependenceDisplay</span>
<span id="cb1-30"><a href="#cb1-30"></a><span class="im">from</span> sklearn.base <span class="im">import</span> BaseEstimator, ClassifierMixin</span>
<span id="cb1-31"><a href="#cb1-31"></a><span class="im">from</span> sklearn.utils.validation <span class="im">import</span> check_is_fitted, check_X_y, check_array</span>
<span id="cb1-32"><a href="#cb1-32"></a><span class="im">from</span> sklearn <span class="im">import</span> set_config</span>
<span id="cb1-33"><a href="#cb1-33"></a></span>
<span id="cb1-34"><a href="#cb1-34"></a><span class="co"># Specialized ML libraries</span></span>
<span id="cb1-35"><a href="#cb1-35"></a><span class="im">from</span> autogluon.tabular <span class="im">import</span> TabularPredictor, TabularDataset</span>
<span id="cb1-36"><a href="#cb1-36"></a><span class="im">from</span> autogluon.common.features.feature_metadata <span class="im">import</span> FeatureMetadata <span class="co"># For monotonic constraints</span></span>
<span id="cb1-37"><a href="#cb1-37"></a><span class="im">import</span> shap</span>
<span id="cb1-38"><a href="#cb1-38"></a><span class="im">from</span> ydata_profiling <span class="im">import</span> ProfileReport</span>
<span id="cb1-39"><a href="#cb1-39"></a></span>
<span id="cb1-40"><a href="#cb1-40"></a><span class="co"># Counterfactual Explanations (optional, install if needed: pip install dice-ml)</span></span>
<span id="cb1-41"><a href="#cb1-41"></a><span class="cf">try</span>:</span>
<span id="cb1-42"><a href="#cb1-42"></a>    <span class="im">import</span> dice_ml</span>
<span id="cb1-43"><a href="#cb1-43"></a>    <span class="im">from</span> dice_ml.utils <span class="im">import</span> helpers <span class="co"># Helper functions for DICE</span></span>
<span id="cb1-44"><a href="#cb1-44"></a><span class="cf">except</span> <span class="pp">ImportError</span>:</span>
<span id="cb1-45"><a href="#cb1-45"></a>    <span class="bu">print</span>(<span class="st">"""</span></span>
<span id="cb1-46"><a href="#cb1-46"></a><span class="st">    </span></span>
<span id="cb1-47"><a href="#cb1-47"></a><span class="st">    dice-ml not found. You need to update your conda env by running:</span></span>
<span id="cb1-48"><a href="#cb1-48"></a><span class="st">    </span></span>
<span id="cb1-49"><a href="#cb1-49"></a><span class="st">    conda activate env_AutoGluon_202502</span></span>
<span id="cb1-50"><a href="#cb1-50"></a><span class="st">    conda install -c conda-forge dice-ml</span></span>
<span id="cb1-51"><a href="#cb1-51"></a><span class="st">    </span></span>
<span id="cb1-52"><a href="#cb1-52"></a><span class="st">    """</span>)</span>
<span id="cb1-53"><a href="#cb1-53"></a>    dice_ml <span class="op">=</span> <span class="va">None</span></span>
<span id="cb1-54"><a href="#cb1-54"></a></span>
<span id="cb1-55"><a href="#cb1-55"></a><span class="co"># Settings</span></span>
<span id="cb1-56"><a href="#cb1-56"></a>pd.set_option(<span class="st">'display.max_columns'</span>, <span class="dv">50</span>)</span>
<span id="cb1-57"><a href="#cb1-57"></a>pd.set_option(<span class="st">'display.max_rows'</span>, <span class="dv">100</span>)</span>
<span id="cb1-58"><a href="#cb1-58"></a>warnings.filterwarnings(<span class="st">'ignore'</span>, category<span class="op">=</span><span class="pp">FutureWarning</span>) <span class="co"># Suppress specific FutureWarnings</span></span>
<span id="cb1-59"><a href="#cb1-59"></a>set_config(transform_output<span class="op">=</span><span class="st">"pandas"</span>) <span class="co"># Set sklearn output to pandas</span></span>
<span id="cb1-60"><a href="#cb1-60"></a></span>
<span id="cb1-61"><a href="#cb1-61"></a><span class="bu">print</span>(<span class="st">"Libraries imported successfully."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Libraries imported successfully.</code></pre>
</div>
</div>
</section>
<section id="helper-functions-and-classes" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="helper-functions-and-classes"><span class="header-section-number">2.2</span> Helper Functions and Classes</h2>
<p>Define helper functions for plotting, scoring, data transformation, and a wrapper for AutoGluon compatibility with scikit-learn.</p>
<div id="helper-funcs-classes" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="kw">def</span> binned_prob_plot(</span>
<span id="cb3-2"><a href="#cb3-2"></a>    data: pd.DataFrame,</span>
<span id="cb3-3"><a href="#cb3-3"></a>    feature: <span class="bu">str</span>,</span>
<span id="cb3-4"><a href="#cb3-4"></a>    target_binary: <span class="bu">str</span>,</span>
<span id="cb3-5"><a href="#cb3-5"></a>    cont_feat_flag: <span class="bu">bool</span> <span class="op">|</span> <span class="va">None</span> <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb3-6"><a href="#cb3-6"></a>    transform_log_odds: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb3-7"><a href="#cb3-7"></a>    num_bins: <span class="bu">int</span> <span class="op">=</span> <span class="dv">10</span>,</span>
<span id="cb3-8"><a href="#cb3-8"></a>    show_plot: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb3-9"><a href="#cb3-9"></a>):</span>
<span id="cb3-10"><a href="#cb3-10"></a>    <span class="co">"""</span></span>
<span id="cb3-11"><a href="#cb3-11"></a><span class="co">    Plots the average binary target against either bins of a feature or categories of the feature.</span></span>
<span id="cb3-12"><a href="#cb3-12"></a><span class="co">    If show_plot=False, skips plotting and only returns Spearman correlation (for continuous feature)</span></span>
<span id="cb3-13"><a href="#cb3-13"></a><span class="co">    or mutual information (for categorical feature).</span></span>
<span id="cb3-14"><a href="#cb3-14"></a></span>
<span id="cb3-15"><a href="#cb3-15"></a><span class="co">    Parameters:</span></span>
<span id="cb3-16"><a href="#cb3-16"></a><span class="co">        data (DataFrame): The DataFrame containing the data.</span></span>
<span id="cb3-17"><a href="#cb3-17"></a><span class="co">        feature (str): The name of the feature to be binned or used as is if categorical.</span></span>
<span id="cb3-18"><a href="#cb3-18"></a><span class="co">        target_binary (str): The name of the binary target variable.</span></span>
<span id="cb3-19"><a href="#cb3-19"></a><span class="co">        cont_feat_flag (bool): True if the feature is continuous, False if it's categorical.</span></span>
<span id="cb3-20"><a href="#cb3-20"></a><span class="co">                    The function will try to infer the feature type if not provided by user.</span></span>
<span id="cb3-21"><a href="#cb3-21"></a><span class="co">        transform_log_odds (bool): If True, transforms probabilities into log odds.</span></span>
<span id="cb3-22"><a href="#cb3-22"></a><span class="co">        num_bins (int): Number of bins for discretization if the feature is continuous.</span></span>
<span id="cb3-23"><a href="#cb3-23"></a><span class="co">        show_plot (bool): If True, plot the figure.</span></span>
<span id="cb3-24"><a href="#cb3-24"></a></span>
<span id="cb3-25"><a href="#cb3-25"></a><span class="co">    Returns:</span></span>
<span id="cb3-26"><a href="#cb3-26"></a><span class="co">        dict: {</span></span>
<span id="cb3-27"><a href="#cb3-27"></a><span class="co">            'feature': feature,</span></span>
<span id="cb3-28"><a href="#cb3-28"></a><span class="co">            'measure_name': string ("spearman_corr" if continuous; "mutual_info" if categorical),</span></span>
<span id="cb3-29"><a href="#cb3-29"></a><span class="co">            "measure_value": float,</span></span>
<span id="cb3-30"><a href="#cb3-30"></a><span class="co">            'p_value': float (or None for MI)</span></span>
<span id="cb3-31"><a href="#cb3-31"></a><span class="co">        }</span></span>
<span id="cb3-32"><a href="#cb3-32"></a><span class="co">    """</span></span>
<span id="cb3-33"><a href="#cb3-33"></a>    <span class="co"># Work on a copy to avoid modifying original</span></span>
<span id="cb3-34"><a href="#cb3-34"></a>    df <span class="op">=</span> data.copy()</span>
<span id="cb3-35"><a href="#cb3-35"></a>    <span class="co"># Infer cont_feat_flag if not provided: sample up to 100 obs, if &gt;60 unique values =&gt; continuous</span></span>
<span id="cb3-36"><a href="#cb3-36"></a>    <span class="cf">if</span> cont_feat_flag <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb3-37"><a href="#cb3-37"></a>        tmp <span class="op">=</span> df[feature].dropna()</span>
<span id="cb3-38"><a href="#cb3-38"></a>        <span class="co"># Ensure sample size does not exceed available data</span></span>
<span id="cb3-39"><a href="#cb3-39"></a>        sample_size <span class="op">=</span> <span class="bu">min</span>(<span class="dv">100</span>, <span class="bu">len</span>(tmp))</span>
<span id="cb3-40"><a href="#cb3-40"></a>        <span class="cf">if</span> sample_size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb3-41"><a href="#cb3-41"></a>            tmp <span class="op">=</span> tmp.sample(sample_size, replace<span class="op">=</span><span class="va">False</span>, random_state<span class="op">=</span><span class="dv">2025</span>)</span>
<span id="cb3-42"><a href="#cb3-42"></a>            cont_feat_flag <span class="op">=</span> tmp.nunique() <span class="op">&gt;</span> <span class="bu">min</span>(<span class="dv">60</span>, sample_size <span class="op">*</span> <span class="fl">0.5</span>) <span class="co"># Adjust threshold based on sample size</span></span>
<span id="cb3-43"><a href="#cb3-43"></a>        <span class="cf">else</span>:</span>
<span id="cb3-44"><a href="#cb3-44"></a>            cont_feat_flag <span class="op">=</span> <span class="va">False</span> <span class="co"># Default to categorical if no data</span></span>
<span id="cb3-45"><a href="#cb3-45"></a>        <span class="bu">print</span>(</span>
<span id="cb3-46"><a href="#cb3-46"></a>            <span class="ss">f"Feature </span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss"> is inferred as </span><span class="sc">{</span><span class="st">'continuous'</span> <span class="cf">if</span> cont_feat_flag <span class="cf">else</span> <span class="st">'categorical'</span><span class="sc">}</span><span class="ss">."</span></span>
<span id="cb3-47"><a href="#cb3-47"></a>        )</span>
<span id="cb3-48"><a href="#cb3-48"></a></span>
<span id="cb3-49"><a href="#cb3-49"></a>    <span class="co"># Bin or categorize</span></span>
<span id="cb3-50"><a href="#cb3-50"></a>    <span class="cf">if</span> cont_feat_flag:</span>
<span id="cb3-51"><a href="#cb3-51"></a>        <span class="co"># Use rank(method='first') to handle non-unique bin edges better</span></span>
<span id="cb3-52"><a href="#cb3-52"></a>        <span class="cf">try</span>:</span>
<span id="cb3-53"><a href="#cb3-53"></a>            df[<span class="st">"bin_label"</span>] <span class="op">=</span> pd.qcut(</span>
<span id="cb3-54"><a href="#cb3-54"></a>                df[feature].rank(method<span class="op">=</span><span class="st">'first'</span>), <span class="co"># Rank first</span></span>
<span id="cb3-55"><a href="#cb3-55"></a>                q<span class="op">=</span>num_bins,</span>
<span id="cb3-56"><a href="#cb3-56"></a>                duplicates<span class="op">=</span><span class="st">"drop"</span>,</span>
<span id="cb3-57"><a href="#cb3-57"></a>                labels<span class="op">=</span>[<span class="bu">str</span>(i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, num_bins <span class="op">+</span> <span class="dv">1</span>)],</span>
<span id="cb3-58"><a href="#cb3-58"></a>            )</span>
<span id="cb3-59"><a href="#cb3-59"></a>        <span class="cf">except</span> <span class="pp">ValueError</span> <span class="im">as</span> e:</span>
<span id="cb3-60"><a href="#cb3-60"></a>             <span class="co"># Fallback if qcut still fails (e.g., too few unique values)</span></span>
<span id="cb3-61"><a href="#cb3-61"></a>            <span class="bu">print</span>(<span class="ss">f"Warning: pd.qcut failed for </span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">). Using fewer bins or manual ranking."</span>)</span>
<span id="cb3-62"><a href="#cb3-62"></a>            ranks <span class="op">=</span> df[feature].rank(method<span class="op">=</span><span class="st">'first'</span>)</span>
<span id="cb3-63"><a href="#cb3-63"></a>            bin_size <span class="op">=</span> <span class="bu">max</span>(<span class="dv">1</span>, <span class="bu">len</span>(df) <span class="op">//</span> num_bins)</span>
<span id="cb3-64"><a href="#cb3-64"></a>            df[<span class="st">'bin_label'</span>] <span class="op">=</span> ((ranks <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> bin_size).clip(upper<span class="op">=</span>num_bins <span class="op">-</span> <span class="dv">1</span>).astype(<span class="bu">str</span>)</span>
<span id="cb3-65"><a href="#cb3-65"></a>    <span class="cf">else</span>:</span>
<span id="cb3-66"><a href="#cb3-66"></a>        df[<span class="st">"bin_label"</span>] <span class="op">=</span> df[feature].astype(<span class="st">"category"</span>)</span>
<span id="cb3-67"><a href="#cb3-67"></a>        <span class="co"># Convert original feature to category codes for MI calculation later</span></span>
<span id="cb3-68"><a href="#cb3-68"></a>        df[feature <span class="op">+</span> <span class="st">"_codes"</span>] <span class="op">=</span> df[feature].astype(<span class="st">"category"</span>).cat.codes</span>
<span id="cb3-69"><a href="#cb3-69"></a></span>
<span id="cb3-70"><a href="#cb3-70"></a>    <span class="co"># Group and compute mean &amp; count</span></span>
<span id="cb3-71"><a href="#cb3-71"></a>    grouped <span class="op">=</span> df.groupby(<span class="st">"bin_label"</span>, observed<span class="op">=</span><span class="va">False</span>).agg( <span class="co"># Use observed=False for category</span></span>
<span id="cb3-72"><a href="#cb3-72"></a>        <span class="op">**</span>{  <span class="co"># **{} unpacks the dict</span></span>
<span id="cb3-73"><a href="#cb3-73"></a>            <span class="st">"average_"</span> <span class="op">+</span> target_binary: (target_binary, <span class="st">"mean"</span>),  <span class="co"># proba</span></span>
<span id="cb3-74"><a href="#cb3-74"></a>            <span class="st">"count"</span>: (target_binary, <span class="st">"count"</span>),  <span class="co"># row count</span></span>
<span id="cb3-75"><a href="#cb3-75"></a>        }</span>
<span id="cb3-76"><a href="#cb3-76"></a>    )</span>
<span id="cb3-77"><a href="#cb3-77"></a></span>
<span id="cb3-78"><a href="#cb3-78"></a>    <span class="co"># Log-odds transform if requested</span></span>
<span id="cb3-79"><a href="#cb3-79"></a>    <span class="cf">if</span> transform_log_odds:</span>
<span id="cb3-80"><a href="#cb3-80"></a>        eps <span class="op">=</span> <span class="fl">1e-6</span></span>
<span id="cb3-81"><a href="#cb3-81"></a>        grouped[<span class="st">"transform_avg_prob"</span>] <span class="op">=</span> special.logit(</span>
<span id="cb3-82"><a href="#cb3-82"></a>            np.clip(grouped[<span class="st">"average_"</span> <span class="op">+</span> target_binary], eps, <span class="dv">1</span> <span class="op">-</span> eps)</span>
<span id="cb3-83"><a href="#cb3-83"></a>        )</span>
<span id="cb3-84"><a href="#cb3-84"></a></span>
<span id="cb3-85"><a href="#cb3-85"></a>    <span class="co"># Compute Spearman for continuous or Mutual Information for categorical</span></span>
<span id="cb3-86"><a href="#cb3-86"></a>    measure_name <span class="op">=</span> <span class="va">None</span></span>
<span id="cb3-87"><a href="#cb3-87"></a>    measure_value <span class="op">=</span> <span class="va">None</span></span>
<span id="cb3-88"><a href="#cb3-88"></a>    p_value <span class="op">=</span> <span class="va">None</span></span>
<span id="cb3-89"><a href="#cb3-89"></a></span>
<span id="cb3-90"><a href="#cb3-90"></a>    <span class="cf">if</span> cont_feat_flag:</span>
<span id="cb3-91"><a href="#cb3-91"></a>        measure_name <span class="op">=</span> <span class="st">"spearman_corr"</span></span>
<span id="cb3-92"><a href="#cb3-92"></a>        y <span class="op">=</span> <span class="st">"transform_avg_prob"</span> <span class="cf">if</span> transform_log_odds <span class="cf">else</span> <span class="st">"average_"</span> <span class="op">+</span> target_binary</span>
<span id="cb3-93"><a href="#cb3-93"></a>        <span class="co"># Ensure grouped index is numeric for correlation</span></span>
<span id="cb3-94"><a href="#cb3-94"></a>        grouped_idx_numeric <span class="op">=</span> pd.to_numeric(grouped.index, errors<span class="op">=</span><span class="st">'coerce'</span>).fillna(<span class="dv">0</span>)</span>
<span id="cb3-95"><a href="#cb3-95"></a>        <span class="cf">if</span> <span class="bu">len</span>(grouped) <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb3-96"><a href="#cb3-96"></a>            measure_value, p_value <span class="op">=</span> stats.spearmanr(grouped_idx_numeric, grouped[y])</span>
<span id="cb3-97"><a href="#cb3-97"></a>        <span class="cf">else</span>:</span>
<span id="cb3-98"><a href="#cb3-98"></a>            measure_value, p_value <span class="op">=</span> np.nan, np.nan <span class="co"># Cannot compute correlation with one group</span></span>
<span id="cb3-99"><a href="#cb3-99"></a>    <span class="cf">else</span>:</span>
<span id="cb3-100"><a href="#cb3-100"></a>        measure_name <span class="op">=</span> <span class="st">"mutual_info"</span></span>
<span id="cb3-101"><a href="#cb3-101"></a>        <span class="co"># Compute mutual information using the category codes</span></span>
<span id="cb3-102"><a href="#cb3-102"></a>        <span class="co"># Ensure no NaNs in target or feature codes</span></span>
<span id="cb3-103"><a href="#cb3-103"></a>        df_mi <span class="op">=</span> df[[feature <span class="op">+</span> <span class="st">"_codes"</span>, target_binary]].dropna()</span>
<span id="cb3-104"><a href="#cb3-104"></a>        <span class="cf">if</span> <span class="kw">not</span> df_mi.empty:</span>
<span id="cb3-105"><a href="#cb3-105"></a>            measure_value <span class="op">=</span> mutual_info_classif(df_mi[[feature <span class="op">+</span> <span class="st">"_codes"</span>]], df_mi[target_binary], discrete_features<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]</span>
<span id="cb3-106"><a href="#cb3-106"></a>        <span class="cf">else</span>:</span>
<span id="cb3-107"><a href="#cb3-107"></a>            measure_value <span class="op">=</span> np.nan</span>
<span id="cb3-108"><a href="#cb3-108"></a>        p_value <span class="op">=</span> <span class="va">None</span> <span class="co"># MI doesn't have a standard p-value like correlation</span></span>
<span id="cb3-109"><a href="#cb3-109"></a></span>
<span id="cb3-110"><a href="#cb3-110"></a>    <span class="co"># Plotting</span></span>
<span id="cb3-111"><a href="#cb3-111"></a>    <span class="cf">if</span> show_plot:</span>
<span id="cb3-112"><a href="#cb3-112"></a>        y_col <span class="op">=</span> (</span>
<span id="cb3-113"><a href="#cb3-113"></a>            <span class="st">"transform_avg_prob"</span> <span class="cf">if</span> transform_log_odds <span class="cf">else</span> <span class="st">"average_"</span> <span class="op">+</span> target_binary</span>
<span id="cb3-114"><a href="#cb3-114"></a>        )</span>
<span id="cb3-115"><a href="#cb3-115"></a></span>
<span id="cb3-116"><a href="#cb3-116"></a>        fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb3-117"><a href="#cb3-117"></a>        <span class="co"># Use numeric index for plotting if continuous, otherwise use category labels</span></span>
<span id="cb3-118"><a href="#cb3-118"></a>        plot_x <span class="op">=</span> <span class="bu">range</span>(<span class="bu">len</span>(grouped)) <span class="cf">if</span> cont_feat_flag <span class="cf">else</span> grouped.index</span>
<span id="cb3-119"><a href="#cb3-119"></a>        ax.plot(</span>
<span id="cb3-120"><a href="#cb3-120"></a>            plot_x,</span>
<span id="cb3-121"><a href="#cb3-121"></a>            grouped[y_col],</span>
<span id="cb3-122"><a href="#cb3-122"></a>            marker<span class="op">=</span><span class="st">"o"</span>,</span>
<span id="cb3-123"><a href="#cb3-123"></a>            linestyle<span class="op">=</span><span class="st">"-"</span>,</span>
<span id="cb3-124"><a href="#cb3-124"></a>            label<span class="op">=</span><span class="st">"Log Odds"</span> <span class="cf">if</span> transform_log_odds <span class="cf">else</span> <span class="st">"Probability"</span>,</span>
<span id="cb3-125"><a href="#cb3-125"></a>        )</span>
<span id="cb3-126"><a href="#cb3-126"></a>        ax.set_xlabel(feature, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb3-127"><a href="#cb3-127"></a>        ax.set_ylabel(<span class="st">"Log Odds"</span> <span class="cf">if</span> transform_log_odds <span class="cf">else</span> <span class="st">"Probability"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb3-128"><a href="#cb3-128"></a>        ax.tick_params(axis<span class="op">=</span><span class="st">"both"</span>, labelsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb3-129"><a href="#cb3-129"></a></span>
<span id="cb3-130"><a href="#cb3-130"></a>        ax2 <span class="op">=</span> ax.twinx()</span>
<span id="cb3-131"><a href="#cb3-131"></a>        ax2.bar(</span>
<span id="cb3-132"><a href="#cb3-132"></a>            plot_x,</span>
<span id="cb3-133"><a href="#cb3-133"></a>            grouped[<span class="st">"count"</span>],</span>
<span id="cb3-134"><a href="#cb3-134"></a>            alpha<span class="op">=</span><span class="fl">0.25</span>,</span>
<span id="cb3-135"><a href="#cb3-135"></a>            color<span class="op">=</span><span class="st">"gray"</span>,</span>
<span id="cb3-136"><a href="#cb3-136"></a>            align<span class="op">=</span><span class="st">"center"</span>,</span>
<span id="cb3-137"><a href="#cb3-137"></a>            label<span class="op">=</span><span class="st">"Counts"</span>,</span>
<span id="cb3-138"><a href="#cb3-138"></a>        )</span>
<span id="cb3-139"><a href="#cb3-139"></a>        ax2.set_ylabel(<span class="st">"Counts"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb3-140"><a href="#cb3-140"></a>        ax2.tick_params(axis<span class="op">=</span><span class="st">"y"</span>, labelsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb3-141"><a href="#cb3-141"></a>        <span class="co"># Adjust secondary axis limits to 0 and 10x its current maximum</span></span>
<span id="cb3-142"><a href="#cb3-142"></a>        y_max <span class="op">=</span> ax2.get_ylim()[<span class="dv">1</span>]</span>
<span id="cb3-143"><a href="#cb3-143"></a>        ax2.set_ylim(<span class="dv">0</span>, y_max <span class="op">*</span> <span class="dv">10</span>)</span>
<span id="cb3-144"><a href="#cb3-144"></a></span>
<span id="cb3-145"><a href="#cb3-145"></a>        <span class="co"># Set x-ticks and labels</span></span>
<span id="cb3-146"><a href="#cb3-146"></a>        ax.set_xticks(plot_x)</span>
<span id="cb3-147"><a href="#cb3-147"></a>        ax.set_xticklabels(grouped.index, rotation<span class="op">=</span><span class="dv">45</span>, ha<span class="op">=</span><span class="st">"right"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb3-148"><a href="#cb3-148"></a></span>
<span id="cb3-149"><a href="#cb3-149"></a>        <span class="co"># Legend</span></span>
<span id="cb3-150"><a href="#cb3-150"></a>        h1, l1 <span class="op">=</span> ax.get_legend_handles_labels()</span>
<span id="cb3-151"><a href="#cb3-151"></a>        h2, l2 <span class="op">=</span> ax2.get_legend_handles_labels()</span>
<span id="cb3-152"><a href="#cb3-152"></a>        ax.legend(h1 <span class="op">+</span> h2, l1 <span class="op">+</span> l2, loc<span class="op">=</span><span class="st">"upper right"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb3-153"><a href="#cb3-153"></a></span>
<span id="cb3-154"><a href="#cb3-154"></a>        title_suffix <span class="op">=</span> <span class="ss">f" (Spearman: </span><span class="sc">{</span>measure_value<span class="sc">:.3f}</span><span class="ss">)"</span> <span class="cf">if</span> cont_feat_flag <span class="kw">and</span> measure_value <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> <span class="ss">f" (MI: </span><span class="sc">{</span>measure_value<span class="sc">:.3f}</span><span class="ss">)"</span> <span class="cf">if</span> <span class="kw">not</span> cont_feat_flag <span class="kw">and</span> measure_value <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> <span class="st">""</span></span>
<span id="cb3-155"><a href="#cb3-155"></a>        ax.set_title(<span class="ss">f"Binned Probability Plot for </span><span class="sc">{</span>feature<span class="sc">}{</span>title_suffix<span class="sc">}</span><span class="ss">"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb3-156"><a href="#cb3-156"></a>        ax.grid(alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb3-157"><a href="#cb3-157"></a>        plt.tight_layout()</span>
<span id="cb3-158"><a href="#cb3-158"></a>        plt.show() <span class="co"># Removed: Let Quarto handle plot display</span></span>
<span id="cb3-159"><a href="#cb3-159"></a></span>
<span id="cb3-160"><a href="#cb3-160"></a>    <span class="cf">return</span> {</span>
<span id="cb3-161"><a href="#cb3-161"></a>        <span class="st">"feature"</span>: feature,</span>
<span id="cb3-162"><a href="#cb3-162"></a>        <span class="st">"measure_name"</span>: measure_name,</span>
<span id="cb3-163"><a href="#cb3-163"></a>        <span class="st">"measure_value"</span>: measure_value,</span>
<span id="cb3-164"><a href="#cb3-164"></a>        <span class="st">"p_value"</span>: p_value,</span>
<span id="cb3-165"><a href="#cb3-165"></a>        <span class="st">"log_odds"</span>: transform_log_odds,</span>
<span id="cb3-166"><a href="#cb3-166"></a>    }</span>
<span id="cb3-167"><a href="#cb3-167"></a></span>
<span id="cb3-168"><a href="#cb3-168"></a></span>
<span id="cb3-169"><a href="#cb3-169"></a><span class="kw">def</span> global_set_seed(seed_value<span class="op">=</span><span class="dv">2025</span>):</span>
<span id="cb3-170"><a href="#cb3-170"></a>    <span class="co">"""Sets random seeds for reproducibility."""</span></span>
<span id="cb3-171"><a href="#cb3-171"></a>    random.seed(seed_value)</span>
<span id="cb3-172"><a href="#cb3-172"></a>    np.random.seed(seed_value)</span>
<span id="cb3-173"><a href="#cb3-173"></a></span>
<span id="cb3-174"><a href="#cb3-174"></a><span class="kw">def</span> remove_ag_folder(mdl_folder: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb3-175"><a href="#cb3-175"></a>    <span class="co">"""Removes the AutoGluon model folder if it exists."""</span></span>
<span id="cb3-176"><a href="#cb3-176"></a>    <span class="cf">if</span> os.path.exists(mdl_folder):</span>
<span id="cb3-177"><a href="#cb3-177"></a>        shutil.rmtree(mdl_folder)</span>
<span id="cb3-178"><a href="#cb3-178"></a>        <span class="bu">print</span>(<span class="ss">f"Removed existing AutoGluon folder: </span><span class="sc">{</span>mdl_folder<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-179"><a href="#cb3-179"></a></span>
<span id="cb3-180"><a href="#cb3-180"></a><span class="kw">class</span> AutoGluonSklearnWrapper(BaseEstimator, ClassifierMixin):</span>
<span id="cb3-181"><a href="#cb3-181"></a>    <span class="co">"""</span></span>
<span id="cb3-182"><a href="#cb3-182"></a><span class="co">    Scikit-learn compatible wrapper for AutoGluon TabularPredictor</span></span>
<span id="cb3-183"><a href="#cb3-183"></a><span class="co">    </span></span>
<span id="cb3-184"><a href="#cb3-184"></a><span class="co">    Inherits from scikit-learn's BaseEstimator and ClassifierMixin to provide</span></span>
<span id="cb3-185"><a href="#cb3-185"></a><span class="co">    full compatibility with scikit-learn tools like PartialDependenceDisplay().</span></span>
<span id="cb3-186"><a href="#cb3-186"></a><span class="co">    </span></span>
<span id="cb3-187"><a href="#cb3-187"></a><span class="co">    Parameters</span></span>
<span id="cb3-188"><a href="#cb3-188"></a><span class="co">    ----------</span></span>
<span id="cb3-189"><a href="#cb3-189"></a><span class="co">    label : str</span></span>
<span id="cb3-190"><a href="#cb3-190"></a><span class="co">        Name of the target column</span></span>
<span id="cb3-191"><a href="#cb3-191"></a></span>
<span id="cb3-192"><a href="#cb3-192"></a><span class="co">    **predictor_args : dict</span></span>
<span id="cb3-193"><a href="#cb3-193"></a><span class="co">        Additional arguments passed to TabularPredictor()</span></span>
<span id="cb3-194"><a href="#cb3-194"></a><span class="co">        (e.g., problem_type, eval_metric, path)</span></span>
<span id="cb3-195"><a href="#cb3-195"></a></span>
<span id="cb3-196"><a href="#cb3-196"></a><span class="co">    **fit_args : dict</span></span>
<span id="cb3-197"><a href="#cb3-197"></a><span class="co">        Additional arguments passed to TabularPredictor.fit() method</span></span>
<span id="cb3-198"><a href="#cb3-198"></a><span class="co">        (e.g., holdout_frac, presets, time_limit, excluded_model_types)</span></span>
<span id="cb3-199"><a href="#cb3-199"></a></span>
<span id="cb3-200"><a href="#cb3-200"></a></span>
<span id="cb3-201"><a href="#cb3-201"></a><span class="co">    Attributes</span></span>
<span id="cb3-202"><a href="#cb3-202"></a><span class="co">    ----------</span></span>
<span id="cb3-203"><a href="#cb3-203"></a><span class="co">    predictor : TabularPredictor</span></span>
<span id="cb3-204"><a href="#cb3-204"></a><span class="co">        The trained AutoGluon predictor</span></span>
<span id="cb3-205"><a href="#cb3-205"></a></span>
<span id="cb3-206"><a href="#cb3-206"></a><span class="co">    classes_ : ndarray</span></span>
<span id="cb3-207"><a href="#cb3-207"></a><span class="co">        Class labels (for classification tasks)</span></span>
<span id="cb3-208"><a href="#cb3-208"></a></span>
<span id="cb3-209"><a href="#cb3-209"></a><span class="co">    n_features_in_ : int</span></span>
<span id="cb3-210"><a href="#cb3-210"></a><span class="co">        Number of features seen during fit</span></span>
<span id="cb3-211"><a href="#cb3-211"></a></span>
<span id="cb3-212"><a href="#cb3-212"></a><span class="co">    feature_names_ : list</span></span>
<span id="cb3-213"><a href="#cb3-213"></a><span class="co">        Feature names inferred during fitting</span></span>
<span id="cb3-214"><a href="#cb3-214"></a></span>
<span id="cb3-215"><a href="#cb3-215"></a><span class="co">    is_fitted_ : bool</span></span>
<span id="cb3-216"><a href="#cb3-216"></a><span class="co">        Whether the estimator has been fitted</span></span>
<span id="cb3-217"><a href="#cb3-217"></a><span class="co">    """</span></span>
<span id="cb3-218"><a href="#cb3-218"></a>    </span>
<span id="cb3-219"><a href="#cb3-219"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, label, predictor_args<span class="op">=</span><span class="va">None</span>, fit_args<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb3-220"><a href="#cb3-220"></a>        <span class="va">self</span>.label <span class="op">=</span> label</span>
<span id="cb3-221"><a href="#cb3-221"></a>        <span class="va">self</span>.predictor_args <span class="op">=</span> predictor_args <span class="cf">if</span> predictor_args <span class="cf">else</span> {}</span>
<span id="cb3-222"><a href="#cb3-222"></a>        <span class="va">self</span>.fit_args <span class="op">=</span> fit_args <span class="cf">if</span> fit_args <span class="cf">else</span> {}</span>
<span id="cb3-223"><a href="#cb3-223"></a>        <span class="va">self</span>.predictor <span class="op">=</span> <span class="va">None</span></span>
<span id="cb3-224"><a href="#cb3-224"></a>        <span class="va">self</span>.classes_ <span class="op">=</span> <span class="va">None</span></span>
<span id="cb3-225"><a href="#cb3-225"></a>        <span class="va">self</span>.n_features_in_ <span class="op">=</span> <span class="va">None</span></span>
<span id="cb3-226"><a href="#cb3-226"></a>        <span class="va">self</span>.feature_names_ <span class="op">=</span> <span class="va">None</span></span>
<span id="cb3-227"><a href="#cb3-227"></a>        <span class="va">self</span>.is_fitted_ <span class="op">=</span> <span class="va">False</span></span>
<span id="cb3-228"><a href="#cb3-228"></a></span>
<span id="cb3-229"><a href="#cb3-229"></a>    <span class="kw">def</span> __sklearn_is_fitted__(<span class="va">self</span>):</span>
<span id="cb3-230"><a href="#cb3-230"></a>        <span class="co">"""Official scikit-learn API for checking fitted status"""</span></span>
<span id="cb3-231"><a href="#cb3-231"></a>        <span class="cf">return</span> <span class="va">self</span>.is_fitted_</span>
<span id="cb3-232"><a href="#cb3-232"></a></span>
<span id="cb3-233"><a href="#cb3-233"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X, y, sample_weight<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb3-234"><a href="#cb3-234"></a>        <span class="co">"""</span></span>
<span id="cb3-235"><a href="#cb3-235"></a><span class="co">        Fit AutoGluon model using scikit-learn interface.</span></span>
<span id="cb3-236"><a href="#cb3-236"></a><span class="co">        If sample_weight is provided, it is added as a column to X for AutoGluon.</span></span>
<span id="cb3-237"><a href="#cb3-237"></a><span class="co">        """</span></span>
<span id="cb3-238"><a href="#cb3-238"></a>        <span class="va">self</span>._check_feature_names(X, reset<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-239"><a href="#cb3-239"></a>        <span class="va">self</span>._check_n_features(X, reset<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-240"><a href="#cb3-240"></a></span>
<span id="cb3-241"><a href="#cb3-241"></a>        <span class="co"># Convert to DataFrame with preserved feature names</span></span>
<span id="cb3-242"><a href="#cb3-242"></a>        train_data <span class="op">=</span> pd.DataFrame(X, columns<span class="op">=</span><span class="va">self</span>.feature_names_)</span>
<span id="cb3-243"><a href="#cb3-243"></a>        train_data[<span class="va">self</span>.label] <span class="op">=</span> y</span>
<span id="cb3-244"><a href="#cb3-244"></a></span>
<span id="cb3-245"><a href="#cb3-245"></a>        <span class="co"># If sample_weight is provided, add it as a column (name must match predictor_args['sample_weight'])</span></span>
<span id="cb3-246"><a href="#cb3-246"></a>        weight_col_name <span class="op">=</span> <span class="va">self</span>.predictor_args.get(<span class="st">'sample_weight'</span>, <span class="va">None</span>)</span>
<span id="cb3-247"><a href="#cb3-247"></a>        <span class="cf">if</span> sample_weight <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb3-248"><a href="#cb3-248"></a>            <span class="cf">if</span> weight_col_name:</span>
<span id="cb3-249"><a href="#cb3-249"></a>                train_data[weight_col_name] <span class="op">=</span> sample_weight</span>
<span id="cb3-250"><a href="#cb3-250"></a>            <span class="cf">else</span>:</span>
<span id="cb3-251"><a href="#cb3-251"></a>                <span class="bu">print</span>(<span class="st">"Warning: sample_weight provided to fit, but 'sample_weight' key not found in predictor_args. Weights will be ignored by AutoGluon."</span>)</span>
<span id="cb3-252"><a href="#cb3-252"></a></span>
<span id="cb3-253"><a href="#cb3-253"></a>        train_data <span class="op">=</span> TabularDataset(train_data)</span>
<span id="cb3-254"><a href="#cb3-254"></a></span>
<span id="cb3-255"><a href="#cb3-255"></a>        <span class="co"># Remove sample_weight from fit_args if present (TabularPredictor.fit does not accept it)</span></span>
<span id="cb3-256"><a href="#cb3-256"></a>        fit_args_clean <span class="op">=</span> {k: v <span class="cf">for</span> k, v <span class="kw">in</span> <span class="va">self</span>.fit_args.items() <span class="cf">if</span> k <span class="op">!=</span> <span class="st">'sample_weight'</span>}</span>
<span id="cb3-257"><a href="#cb3-257"></a></span>
<span id="cb3-258"><a href="#cb3-258"></a>        <span class="va">self</span>.predictor <span class="op">=</span> TabularPredictor(</span>
<span id="cb3-259"><a href="#cb3-259"></a>            label<span class="op">=</span><span class="va">self</span>.label,</span>
<span id="cb3-260"><a href="#cb3-260"></a>            <span class="op">**</span><span class="va">self</span>.predictor_args</span>
<span id="cb3-261"><a href="#cb3-261"></a>        ).fit(train_data, <span class="op">**</span>fit_args_clean)</span>
<span id="cb3-262"><a href="#cb3-262"></a></span>
<span id="cb3-263"><a href="#cb3-263"></a>        <span class="cf">if</span> <span class="va">self</span>.predictor.problem_type <span class="kw">in</span> [<span class="st">'binary'</span>, <span class="st">'multiclass'</span>]:</span>
<span id="cb3-264"><a href="#cb3-264"></a>            <span class="va">self</span>.classes_ <span class="op">=</span> np.array(<span class="va">self</span>.predictor.class_labels)</span>
<span id="cb3-265"><a href="#cb3-265"></a></span>
<span id="cb3-266"><a href="#cb3-266"></a>        <span class="va">self</span>.is_fitted_ <span class="op">=</span> <span class="va">True</span></span>
<span id="cb3-267"><a href="#cb3-267"></a>        <span class="cf">return</span> <span class="va">self</span></span>
<span id="cb3-268"><a href="#cb3-268"></a></span>
<span id="cb3-269"><a href="#cb3-269"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb3-270"><a href="#cb3-270"></a>        <span class="co">"""</span></span>
<span id="cb3-271"><a href="#cb3-271"></a><span class="co">        Make class predictions</span></span>
<span id="cb3-272"><a href="#cb3-272"></a><span class="co">        </span></span>
<span id="cb3-273"><a href="#cb3-273"></a><span class="co">        Parameters</span></span>
<span id="cb3-274"><a href="#cb3-274"></a><span class="co">        ----------</span></span>
<span id="cb3-275"><a href="#cb3-275"></a><span class="co">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span></span>
<span id="cb3-276"><a href="#cb3-276"></a><span class="co">            Input data</span></span>
<span id="cb3-277"><a href="#cb3-277"></a><span class="co">            </span></span>
<span id="cb3-278"><a href="#cb3-278"></a><span class="co">        Returns</span></span>
<span id="cb3-279"><a href="#cb3-279"></a><span class="co">        -------</span></span>
<span id="cb3-280"><a href="#cb3-280"></a><span class="co">        y_pred : ndarray of shape (n_samples,)</span></span>
<span id="cb3-281"><a href="#cb3-281"></a><span class="co">            Predicted class labels</span></span>
<span id="cb3-282"><a href="#cb3-282"></a><span class="co">        """</span></span>
<span id="cb3-283"><a href="#cb3-283"></a>        check_is_fitted(<span class="va">self</span>)</span>
<span id="cb3-284"><a href="#cb3-284"></a>        <span class="va">self</span>._check_feature_names(X)</span>
<span id="cb3-285"><a href="#cb3-285"></a>        <span class="va">self</span>._check_n_features(X)</span>
<span id="cb3-286"><a href="#cb3-286"></a></span>
<span id="cb3-287"><a href="#cb3-287"></a>        df <span class="op">=</span> pd.DataFrame(X, columns<span class="op">=</span><span class="va">self</span>.feature_names_)</span>
<span id="cb3-288"><a href="#cb3-288"></a>        df <span class="op">=</span> TabularDataset(df)</span>
<span id="cb3-289"><a href="#cb3-289"></a></span>
<span id="cb3-290"><a href="#cb3-290"></a>        <span class="cf">return</span> <span class="va">self</span>.predictor.predict(df).values</span>
<span id="cb3-291"><a href="#cb3-291"></a></span>
<span id="cb3-292"><a href="#cb3-292"></a>    <span class="kw">def</span> predict_proba(<span class="va">self</span>, X):</span>
<span id="cb3-293"><a href="#cb3-293"></a>        <span class="co">"""</span></span>
<span id="cb3-294"><a href="#cb3-294"></a><span class="co">        Predict class probabilities</span></span>
<span id="cb3-295"><a href="#cb3-295"></a><span class="co">        </span></span>
<span id="cb3-296"><a href="#cb3-296"></a><span class="co">        Parameters</span></span>
<span id="cb3-297"><a href="#cb3-297"></a><span class="co">        ----------</span></span>
<span id="cb3-298"><a href="#cb3-298"></a><span class="co">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span></span>
<span id="cb3-299"><a href="#cb3-299"></a><span class="co">            Input data</span></span>
<span id="cb3-300"><a href="#cb3-300"></a><span class="co">            </span></span>
<span id="cb3-301"><a href="#cb3-301"></a><span class="co">        Returns</span></span>
<span id="cb3-302"><a href="#cb3-302"></a><span class="co">        -------</span></span>
<span id="cb3-303"><a href="#cb3-303"></a><span class="co">        proba : ndarray of shape (n_samples, n_classes)</span></span>
<span id="cb3-304"><a href="#cb3-304"></a><span class="co">            Class probabilities</span></span>
<span id="cb3-305"><a href="#cb3-305"></a><span class="co">        """</span></span>
<span id="cb3-306"><a href="#cb3-306"></a>        check_is_fitted(<span class="va">self</span>)</span>
<span id="cb3-307"><a href="#cb3-307"></a>        <span class="va">self</span>._check_feature_names(X)</span>
<span id="cb3-308"><a href="#cb3-308"></a>        <span class="va">self</span>._check_n_features(X)</span>
<span id="cb3-309"><a href="#cb3-309"></a></span>
<span id="cb3-310"><a href="#cb3-310"></a>        df <span class="op">=</span> pd.DataFrame(X, columns<span class="op">=</span><span class="va">self</span>.feature_names_)</span>
<span id="cb3-311"><a href="#cb3-311"></a>        df <span class="op">=</span> TabularDataset(df)</span>
<span id="cb3-312"><a href="#cb3-312"></a></span>
<span id="cb3-313"><a href="#cb3-313"></a>        <span class="cf">return</span> <span class="va">self</span>.predictor.predict_proba(df).values</span>
<span id="cb3-314"><a href="#cb3-314"></a></span>
<span id="cb3-315"><a href="#cb3-315"></a>    <span class="kw">def</span> get_params(<span class="va">self</span>, deep<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb3-316"><a href="#cb3-316"></a>        <span class="co">"""Get parameters for this estimator"""</span></span>
<span id="cb3-317"><a href="#cb3-317"></a>        <span class="cf">return</span> {</span>
<span id="cb3-318"><a href="#cb3-318"></a>            <span class="st">'label'</span>: <span class="va">self</span>.label,</span>
<span id="cb3-319"><a href="#cb3-319"></a>            <span class="st">'predictor_args'</span>: <span class="va">self</span>.predictor_args,</span>
<span id="cb3-320"><a href="#cb3-320"></a>            <span class="st">'fit_args'</span>: <span class="va">self</span>.fit_args</span>
<span id="cb3-321"><a href="#cb3-321"></a>        }</span>
<span id="cb3-322"><a href="#cb3-322"></a></span>
<span id="cb3-323"><a href="#cb3-323"></a>    <span class="kw">def</span> set_params(<span class="va">self</span>, <span class="op">**</span>params):</span>
<span id="cb3-324"><a href="#cb3-324"></a>        <span class="co">"""Set parameters for this estimator"""</span></span>
<span id="cb3-325"><a href="#cb3-325"></a>        <span class="cf">for</span> param, value <span class="kw">in</span> params.items():</span>
<span id="cb3-326"><a href="#cb3-326"></a>            <span class="cf">if</span> param <span class="op">==</span> <span class="st">'label'</span>:</span>
<span id="cb3-327"><a href="#cb3-327"></a>                <span class="va">self</span>.label <span class="op">=</span> value</span>
<span id="cb3-328"><a href="#cb3-328"></a>            <span class="cf">else</span>:</span>
<span id="cb3-329"><a href="#cb3-329"></a>                <span class="va">self</span>.predictor_args[param] <span class="op">=</span> value</span>
<span id="cb3-330"><a href="#cb3-330"></a>        <span class="cf">return</span> <span class="va">self</span></span>
<span id="cb3-331"><a href="#cb3-331"></a></span>
<span id="cb3-332"><a href="#cb3-332"></a>    <span class="kw">def</span> _check_n_features(<span class="va">self</span>, X, reset<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb3-333"><a href="#cb3-333"></a>        <span class="co">"""Validate number of features"""</span></span>
<span id="cb3-334"><a href="#cb3-334"></a>        n_features <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb3-335"><a href="#cb3-335"></a>        <span class="cf">if</span> reset:</span>
<span id="cb3-336"><a href="#cb3-336"></a>            <span class="va">self</span>.n_features_in_ <span class="op">=</span> n_features</span>
<span id="cb3-337"><a href="#cb3-337"></a>        <span class="cf">elif</span> n_features <span class="op">!=</span> <span class="va">self</span>.n_features_in_:</span>
<span id="cb3-338"><a href="#cb3-338"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"Expected </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>n_features_in_<span class="sc">}</span><span class="ss"> features, got </span><span class="sc">{</span>n_features<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-339"><a href="#cb3-339"></a></span>
<span id="cb3-340"><a href="#cb3-340"></a>    <span class="kw">def</span> _check_feature_names(<span class="va">self</span>, X, reset<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb3-341"><a href="#cb3-341"></a>        <span class="co">"""Validate feature names (AutoGluon requirement)"""</span></span>
<span id="cb3-342"><a href="#cb3-342"></a>        <span class="cf">if</span> reset:</span>
<span id="cb3-343"><a href="#cb3-343"></a>            <span class="cf">if</span> <span class="bu">isinstance</span>(X, np.ndarray):</span>
<span id="cb3-344"><a href="#cb3-344"></a>                <span class="va">self</span>.feature_names_ <span class="op">=</span> [<span class="ss">f'feat_</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(X.shape[<span class="dv">1</span>])]</span>
<span id="cb3-345"><a href="#cb3-345"></a>            <span class="cf">else</span>:</span>
<span id="cb3-346"><a href="#cb3-346"></a>                <span class="va">self</span>.feature_names_ <span class="op">=</span> X.columns.tolist()</span>
<span id="cb3-347"><a href="#cb3-347"></a>        <span class="cf">elif</span> <span class="bu">hasattr</span>(X, <span class="st">'columns'</span>):</span>
<span id="cb3-348"><a href="#cb3-348"></a>            <span class="cf">if</span> <span class="bu">list</span>(X.columns) <span class="op">!=</span> <span class="va">self</span>.feature_names_:</span>
<span id="cb3-349"><a href="#cb3-349"></a>                <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Feature names mismatch between fit and predict"</span>)</span>
<span id="cb3-350"><a href="#cb3-350"></a></span>
<span id="cb3-351"><a href="#cb3-351"></a><span class="kw">def</span> calculate_score(prob_default, pdo<span class="op">=</span><span class="dv">40</span>, base_score<span class="op">=</span><span class="dv">600</span>):</span>
<span id="cb3-352"><a href="#cb3-352"></a>    <span class="co">"""Converts probability of default to a 3-digit score using OddsBad."""</span></span>
<span id="cb3-353"><a href="#cb3-353"></a>    <span class="co"># Avoid log(0) or division by zero</span></span>
<span id="cb3-354"><a href="#cb3-354"></a>    eps <span class="op">=</span> <span class="fl">1e-9</span></span>
<span id="cb3-355"><a href="#cb3-355"></a>    prob_default <span class="op">=</span> np.clip(prob_default, eps, <span class="dv">1</span> <span class="op">-</span> eps)</span>
<span id="cb3-356"><a href="#cb3-356"></a>    odds_bad <span class="op">=</span> prob_default <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> prob_default)  <span class="co"># Bad/Good odds</span></span>
<span id="cb3-357"><a href="#cb3-357"></a>    factor <span class="op">=</span> pdo <span class="op">/</span> np.log(<span class="dv">2</span>)</span>
<span id="cb3-358"><a href="#cb3-358"></a>    <span class="co"># Score = Base - Factor * log(OddsBad)</span></span>
<span id="cb3-359"><a href="#cb3-359"></a>    score <span class="op">=</span> base_score <span class="op">-</span> factor <span class="op">*</span> np.log(odds_bad)</span>
<span id="cb3-360"><a href="#cb3-360"></a>    <span class="co"># Clip score to a reasonable range, e.g., 300-850</span></span>
<span id="cb3-361"><a href="#cb3-361"></a>    <span class="cf">return</span> np.clip(score, <span class="dv">300</span>, <span class="dv">850</span>).astype(<span class="bu">int</span>)</span>
<span id="cb3-362"><a href="#cb3-362"></a></span>
<span id="cb3-363"><a href="#cb3-363"></a><span class="kw">def</span> score_to_probability(score, pdo<span class="op">=</span><span class="dv">40</span>, base_score<span class="op">=</span><span class="dv">600</span>):</span>
<span id="cb3-364"><a href="#cb3-364"></a>    <span class="co">"""Converts a credit score back to probability of default.</span></span>
<span id="cb3-365"><a href="#cb3-365"></a><span class="co">    </span></span>
<span id="cb3-366"><a href="#cb3-366"></a><span class="co">    This is the inverse of calculate_score function.</span></span>
<span id="cb3-367"><a href="#cb3-367"></a><span class="co">    </span></span>
<span id="cb3-368"><a href="#cb3-368"></a><span class="co">    Parameters:</span></span>
<span id="cb3-369"><a href="#cb3-369"></a><span class="co">        score (int or float): Credit score to convert</span></span>
<span id="cb3-370"><a href="#cb3-370"></a><span class="co">        pdo (int): Points to Double the Odds, default 40</span></span>
<span id="cb3-371"><a href="#cb3-371"></a><span class="co">        base_score (int): Base score, default 600</span></span>
<span id="cb3-372"><a href="#cb3-372"></a><span class="co">    </span></span>
<span id="cb3-373"><a href="#cb3-373"></a><span class="co">    Returns:</span></span>
<span id="cb3-374"><a href="#cb3-374"></a><span class="co">        float: Probability of default [0, 1]</span></span>
<span id="cb3-375"><a href="#cb3-375"></a><span class="co">    """</span></span>
<span id="cb3-376"><a href="#cb3-376"></a>    <span class="co"># Calculate factor same as in the score calculation</span></span>
<span id="cb3-377"><a href="#cb3-377"></a>    factor <span class="op">=</span> pdo <span class="op">/</span> np.log(<span class="dv">2</span>)</span>
<span id="cb3-378"><a href="#cb3-378"></a>    </span>
<span id="cb3-379"><a href="#cb3-379"></a>    <span class="co"># Calculate the odds_bad from the score</span></span>
<span id="cb3-380"><a href="#cb3-380"></a>    odds_bad <span class="op">=</span> np.exp((score <span class="op">-</span> base_score) <span class="op">/</span> <span class="op">-</span>factor)</span>
<span id="cb3-381"><a href="#cb3-381"></a>    </span>
<span id="cb3-382"><a href="#cb3-382"></a>    <span class="co"># Convert odds to probability</span></span>
<span id="cb3-383"><a href="#cb3-383"></a>    prob_default <span class="op">=</span> odds_bad <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> odds_bad)</span>
<span id="cb3-384"><a href="#cb3-384"></a>    </span>
<span id="cb3-385"><a href="#cb3-385"></a>    <span class="cf">return</span> prob_default</span>
<span id="cb3-386"><a href="#cb3-386"></a></span>
<span id="cb3-387"><a href="#cb3-387"></a><span class="kw">def</span> parse_emp_length(x):</span>
<span id="cb3-388"><a href="#cb3-388"></a>    <span class="co">"""Convert employment length string to numeric years.</span></span>
<span id="cb3-389"><a href="#cb3-389"></a><span class="co">    </span></span>
<span id="cb3-390"><a href="#cb3-390"></a><span class="co">    Returns:</span></span>
<span id="cb3-391"><a href="#cb3-391"></a><span class="co">        int: Numeric representation of employment length.</span></span>
<span id="cb3-392"><a href="#cb3-392"></a><span class="co">             - 0 for "n/a" or "&lt; 1 year".</span></span>
<span id="cb3-393"><a href="#cb3-393"></a><span class="co">             - 11 for "10+ years".</span></span>
<span id="cb3-394"><a href="#cb3-394"></a><span class="co">             - Extracted number for other valid formats.</span></span>
<span id="cb3-395"><a href="#cb3-395"></a><span class="co">             - -1 for unexpected formats.</span></span>
<span id="cb3-396"><a href="#cb3-396"></a><span class="co">    """</span></span>
<span id="cb3-397"><a href="#cb3-397"></a>    <span class="cf">if</span> pd.isna(x) <span class="kw">or</span> x <span class="op">==</span> <span class="st">"n/a"</span>:</span>
<span id="cb3-398"><a href="#cb3-398"></a>        <span class="cf">return</span> <span class="dv">0</span></span>
<span id="cb3-399"><a href="#cb3-399"></a>    <span class="cf">elif</span> <span class="st">"&lt; 1 year"</span> <span class="kw">in</span> x:</span>
<span id="cb3-400"><a href="#cb3-400"></a>        <span class="cf">return</span> <span class="dv">0</span></span>
<span id="cb3-401"><a href="#cb3-401"></a>    <span class="cf">elif</span> <span class="st">"10+ years"</span> <span class="kw">in</span> x:</span>
<span id="cb3-402"><a href="#cb3-402"></a>        <span class="cf">return</span> <span class="dv">10</span></span>
<span id="cb3-403"><a href="#cb3-403"></a>    <span class="cf">try</span>:</span>
<span id="cb3-404"><a href="#cb3-404"></a>        <span class="cf">return</span> <span class="bu">int</span>(re.findall(<span class="vs">r'\d+'</span>, <span class="bu">str</span>(x))[<span class="dv">0</span>])</span>
<span id="cb3-405"><a href="#cb3-405"></a>    <span class="cf">except</span>:</span>
<span id="cb3-406"><a href="#cb3-406"></a>        <span class="cf">return</span> <span class="op">-</span><span class="dv">1</span>  <span class="co"># Fallback for unexpected formats</span></span>
<span id="cb3-407"><a href="#cb3-407"></a></span>
<span id="cb3-408"><a href="#cb3-408"></a></span>
<span id="cb3-409"><a href="#cb3-409"></a></span>
<span id="cb3-410"><a href="#cb3-410"></a><span class="kw">def</span> ks_table(data: pd.DataFrame, y_true_col: <span class="bu">str</span>, y_pred_col: <span class="bu">str</span>, n_bins: <span class="bu">int</span> <span class="op">=</span> <span class="dv">10</span>, is_score: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>, sample_weight_col: <span class="bu">str</span> <span class="op">|</span> <span class="va">None</span> <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb3-411"><a href="#cb3-411"></a>    <span class="co">"""</span></span>
<span id="cb3-412"><a href="#cb3-412"></a><span class="co">    Generates a KS table from a DataFrame using either probabilities or scores,</span></span>
<span id="cb3-413"><a href="#cb3-413"></a><span class="co">    optionally using sample weights.</span></span>
<span id="cb3-414"><a href="#cb3-414"></a></span>
<span id="cb3-415"><a href="#cb3-415"></a><span class="co">    Parameters:</span></span>
<span id="cb3-416"><a href="#cb3-416"></a><span class="co">        data (pd.DataFrame): DataFrame containing the true labels and predicted probabilities (or scores).</span></span>
<span id="cb3-417"><a href="#cb3-417"></a><span class="co">        y_true_col (str): Name of the column with the true binary labels (0 or 1).</span></span>
<span id="cb3-418"><a href="#cb3-418"></a><span class="co">        y_pred_col (str): Name of the column with the predicted probabilities (higher=riskier) or scores (lower=riskier).</span></span>
<span id="cb3-419"><a href="#cb3-419"></a><span class="co">        n_bins (int): Number of bins to divide the values into.</span></span>
<span id="cb3-420"><a href="#cb3-420"></a><span class="co">        is_score (bool): Set to True if y_pred_col contains scores (lower=riskier),</span></span>
<span id="cb3-421"><a href="#cb3-421"></a><span class="co">                            False if it contains probabilities (higher=riskier). Default is False.</span></span>
<span id="cb3-422"><a href="#cb3-422"></a><span class="co">        sample_weight_col (str | None): Name of the column containing sample weights.</span></span>
<span id="cb3-423"><a href="#cb3-423"></a><span class="co">                                        If None, all samples have weight 1. Default is None.</span></span>
<span id="cb3-424"><a href="#cb3-424"></a></span>
<span id="cb3-425"><a href="#cb3-425"></a><span class="co">    Returns:</span></span>
<span id="cb3-426"><a href="#cb3-426"></a><span class="co">        pd.DataFrame: The KS table.</span></span>
<span id="cb3-427"><a href="#cb3-427"></a><span class="co">    """</span></span>
<span id="cb3-428"><a href="#cb3-428"></a>    <span class="co"># Select relevant columns and work on a copy</span></span>
<span id="cb3-429"><a href="#cb3-429"></a>    cols_to_select <span class="op">=</span> [y_true_col, y_pred_col]</span>
<span id="cb3-430"><a href="#cb3-430"></a>    <span class="cf">if</span> sample_weight_col:</span>
<span id="cb3-431"><a href="#cb3-431"></a>        cols_to_select.append(sample_weight_col)</span>
<span id="cb3-432"><a href="#cb3-432"></a></span>
<span id="cb3-433"><a href="#cb3-433"></a>    df <span class="op">=</span> data[cols_to_select].copy()</span>
<span id="cb3-434"><a href="#cb3-434"></a></span>
<span id="cb3-435"><a href="#cb3-435"></a></span>
<span id="cb3-436"><a href="#cb3-436"></a>    <span class="co"># Rename columns for internal consistency</span></span>
<span id="cb3-437"><a href="#cb3-437"></a>    rename_map <span class="op">=</span> {y_true_col: <span class="st">'y_true'</span>, y_pred_col: <span class="st">'y_pred'</span>}</span>
<span id="cb3-438"><a href="#cb3-438"></a>    <span class="cf">if</span> sample_weight_col <span class="kw">in</span> df.columns:</span>
<span id="cb3-439"><a href="#cb3-439"></a>        rename_map[sample_weight_col] <span class="op">=</span> <span class="st">'weight'</span></span>
<span id="cb3-440"><a href="#cb3-440"></a>        <span class="co"># Ensure weights are numeric and fill NaNs with 1 (or raise error if preferred)</span></span>
<span id="cb3-441"><a href="#cb3-441"></a>        df[<span class="st">'weight'</span>] <span class="op">=</span> pd.to_numeric(df[sample_weight_col], errors<span class="op">=</span><span class="st">'coerce'</span>).fillna(<span class="fl">1.0</span>)</span>
<span id="cb3-442"><a href="#cb3-442"></a>    <span class="cf">else</span>:</span>
<span id="cb3-443"><a href="#cb3-443"></a>        <span class="co"># Assign weight of 1 if no weight column provided or if column name was invalid</span></span>
<span id="cb3-444"><a href="#cb3-444"></a>        df[<span class="st">'weight'</span>] <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb3-445"><a href="#cb3-445"></a>        <span class="cf">if</span> sample_weight_col <span class="kw">not</span> <span class="kw">in</span> data.columns:</span>
<span id="cb3-446"><a href="#cb3-446"></a>             <span class="bu">print</span>(<span class="ss">f"Warning: sample_weight_col '</span><span class="sc">{</span>sample_weight_col<span class="sc">}</span><span class="ss">' not found. Using weight=1 for all samples."</span>)</span>
<span id="cb3-447"><a href="#cb3-447"></a></span>
<span id="cb3-448"><a href="#cb3-448"></a>    df.rename(columns<span class="op">=</span>rename_map, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-449"><a href="#cb3-449"></a></span>
<span id="cb3-450"><a href="#cb3-450"></a></span>
<span id="cb3-451"><a href="#cb3-451"></a>    <span class="co"># Handle potential NaN values - drop rows where prediction is NaN</span></span>
<span id="cb3-452"><a href="#cb3-452"></a>    df.dropna(subset<span class="op">=</span>[<span class="st">'y_pred'</span>], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-453"><a href="#cb3-453"></a>    <span class="cf">if</span> df.empty:</span>
<span id="cb3-454"><a href="#cb3-454"></a>        <span class="bu">print</span>(<span class="st">"Warning: No valid data points after dropping NaN values for KS table."</span>)</span>
<span id="cb3-455"><a href="#cb3-455"></a>        <span class="cf">return</span> pd.DataFrame() <span class="co"># Return empty DataFrame</span></span>
<span id="cb3-456"><a href="#cb3-456"></a></span>
<span id="cb3-457"><a href="#cb3-457"></a>    <span class="co"># Bin values</span></span>
<span id="cb3-458"><a href="#cb3-458"></a>    <span class="co"># Use rank(method='first') to handle non-unique bin edges better if duplicates='drop' fails</span></span>
<span id="cb3-459"><a href="#cb3-459"></a>    <span class="cf">try</span>:</span>
<span id="cb3-460"><a href="#cb3-460"></a>        <span class="co"># Create bins based on quantiles of the value column</span></span>
<span id="cb3-461"><a href="#cb3-461"></a>        df[<span class="st">"bin"</span>] <span class="op">=</span> pd.qcut(df[<span class="st">"y_pred"</span>].rank(method<span class="op">=</span><span class="st">'first'</span>), q<span class="op">=</span>n_bins, labels<span class="op">=</span><span class="va">False</span>, duplicates<span class="op">=</span><span class="st">'drop'</span>)</span>
<span id="cb3-462"><a href="#cb3-462"></a>    <span class="cf">except</span> <span class="pp">ValueError</span> <span class="im">as</span> e:</span>
<span id="cb3-463"><a href="#cb3-463"></a>        <span class="co"># Fallback if qcut still fails (e.g., too few unique values)</span></span>
<span id="cb3-464"><a href="#cb3-464"></a>        <span class="bu">print</span>(<span class="ss">f"Warning: pd.qcut failed (</span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">). Trying with fewer bins or manual ranking."</span>)</span>
<span id="cb3-465"><a href="#cb3-465"></a>        <span class="co"># As a simple fallback, use ranking and divide into bins manually</span></span>
<span id="cb3-466"><a href="#cb3-466"></a>        ranks <span class="op">=</span> df[<span class="st">"y_pred"</span>].rank(method<span class="op">=</span><span class="st">'first'</span>)</span>
<span id="cb3-467"><a href="#cb3-467"></a>        bin_size <span class="op">=</span> <span class="bu">max</span>(<span class="dv">1</span>, <span class="bu">len</span>(df) <span class="op">//</span> n_bins) <span class="co"># Ensure bin_size is at least 1</span></span>
<span id="cb3-468"><a href="#cb3-468"></a>        df[<span class="st">'bin'</span>] <span class="op">=</span> ((ranks <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> bin_size).clip(upper<span class="op">=</span>n_bins <span class="op">-</span> <span class="dv">1</span>).astype(<span class="bu">int</span>)</span>
<span id="cb3-469"><a href="#cb3-469"></a></span>
<span id="cb3-470"><a href="#cb3-470"></a></span>
<span id="cb3-471"><a href="#cb3-471"></a>    <span class="co"># Determine sorting order based on whether input is score or probability</span></span>
<span id="cb3-472"><a href="#cb3-472"></a>    <span class="co"># If score (lower=riskier), sort bins by ascending min_value (lowest scores first)</span></span>
<span id="cb3-473"><a href="#cb3-473"></a>    <span class="co"># If probability (higher=riskier), sort bins by descending min_value (highest probs first)</span></span>
<span id="cb3-474"><a href="#cb3-474"></a>    sort_ascending <span class="op">=</span> is_score</span>
<span id="cb3-475"><a href="#cb3-475"></a></span>
<span id="cb3-476"><a href="#cb3-476"></a>    <span class="co"># Use duckdb to perform aggregations</span></span>
<span id="cb3-477"><a href="#cb3-477"></a></span>
<span id="cb3-478"><a href="#cb3-478"></a>    ks_df <span class="op">=</span> duckdb.query(</span>
<span id="cb3-479"><a href="#cb3-479"></a>        <span class="st">"""</span></span>
<span id="cb3-480"><a href="#cb3-480"></a><span class="st">        SELECT</span></span>
<span id="cb3-481"><a href="#cb3-481"></a><span class="st">            bin,</span></span>
<span id="cb3-482"><a href="#cb3-482"></a><span class="st">            MIN(y_pred) AS min_value,</span></span>
<span id="cb3-483"><a href="#cb3-483"></a><span class="st">            MAX(y_pred) AS max_value,</span></span>
<span id="cb3-484"><a href="#cb3-484"></a><span class="st">            AVG(y_pred) AS avg_value,</span></span>
<span id="cb3-485"><a href="#cb3-485"></a><span class="st">            SUM(weight) AS count,</span></span>
<span id="cb3-486"><a href="#cb3-486"></a><span class="st">            SUM(y_true * weight) AS bads</span></span>
<span id="cb3-487"><a href="#cb3-487"></a><span class="st">        FROM df</span></span>
<span id="cb3-488"><a href="#cb3-488"></a><span class="st">        GROUP BY bin</span></span>
<span id="cb3-489"><a href="#cb3-489"></a><span class="st">        """</span></span>
<span id="cb3-490"><a href="#cb3-490"></a>    ).to_df()</span>
<span id="cb3-491"><a href="#cb3-491"></a></span>
<span id="cb3-492"><a href="#cb3-492"></a>    <span class="co"># Reset index and sort</span></span>
<span id="cb3-493"><a href="#cb3-493"></a>    ks_df <span class="op">=</span> ks_df.reset_index().sort_values(<span class="st">"min_value"</span>, ascending<span class="op">=</span>sort_ascending)</span>
<span id="cb3-494"><a href="#cb3-494"></a></span>
<span id="cb3-495"><a href="#cb3-495"></a></span>
<span id="cb3-496"><a href="#cb3-496"></a>    ks_df[<span class="st">"goods"</span>] <span class="op">=</span> ks_df[<span class="st">"count"</span>] <span class="op">-</span> ks_df[<span class="st">"bads"</span>] <span class="co"># Total weight - bad weight = good weight</span></span>
<span id="cb3-497"><a href="#cb3-497"></a>    <span class="co"># Avoid division by zero if a bin has zero total weight</span></span>
<span id="cb3-498"><a href="#cb3-498"></a>    <span class="co"># Use np.where for safer division</span></span>
<span id="cb3-499"><a href="#cb3-499"></a>    ks_df[<span class="st">"bad_rate"</span>] <span class="op">=</span> np.where(ks_df[<span class="st">"count"</span>] <span class="op">&gt;</span> <span class="fl">1e-9</span>, ks_df[<span class="st">"bads"</span>] <span class="op">/</span> ks_df[<span class="st">"count"</span>], <span class="dv">0</span>)</span>
<span id="cb3-500"><a href="#cb3-500"></a></span>
<span id="cb3-501"><a href="#cb3-501"></a></span>
<span id="cb3-502"><a href="#cb3-502"></a>    total_bads <span class="op">=</span> ks_df[<span class="st">"bads"</span>].<span class="bu">sum</span>() <span class="co"># Total bad weight</span></span>
<span id="cb3-503"><a href="#cb3-503"></a>    total_goods <span class="op">=</span> ks_df[<span class="st">"goods"</span>].<span class="bu">sum</span>() <span class="co"># Total good weight</span></span>
<span id="cb3-504"><a href="#cb3-504"></a></span>
<span id="cb3-505"><a href="#cb3-505"></a>    <span class="co"># Avoid division by zero if there are no bads or no goods (based on weight)</span></span>
<span id="cb3-506"><a href="#cb3-506"></a>    <span class="cf">if</span> total_bads <span class="op">&lt;=</span> <span class="fl">1e-9</span> <span class="kw">or</span> total_goods <span class="op">&lt;=</span> <span class="fl">1e-9</span>: <span class="co"># Use small threshold for float comparison</span></span>
<span id="cb3-507"><a href="#cb3-507"></a>        ks_df[<span class="st">"cum_bads_pct"</span>] <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb3-508"><a href="#cb3-508"></a>        ks_df[<span class="st">"cum_goods_pct"</span>] <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb3-509"><a href="#cb3-509"></a>        ks_df[<span class="st">"ks"</span>] <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb3-510"><a href="#cb3-510"></a>        <span class="bu">print</span>(<span class="st">"Warning: KS calculation skipped as total weighted goods or bads is effectively zero."</span>)</span>
<span id="cb3-511"><a href="#cb3-511"></a>    <span class="cf">else</span>:</span>
<span id="cb3-512"><a href="#cb3-512"></a>        ks_df[<span class="st">"cum_bads_pct"</span>] <span class="op">=</span> (ks_df[<span class="st">"bads"</span>].cumsum() <span class="op">/</span> total_bads) <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb3-513"><a href="#cb3-513"></a>        ks_df[<span class="st">"cum_goods_pct"</span>] <span class="op">=</span> (ks_df[<span class="st">"goods"</span>].cumsum() <span class="op">/</span> total_goods) <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb3-514"><a href="#cb3-514"></a>        ks_df[<span class="st">"ks"</span>] <span class="op">=</span> np.<span class="bu">abs</span>(ks_df[<span class="st">"cum_bads_pct"</span>] <span class="op">-</span> ks_df[<span class="st">"cum_goods_pct"</span>])</span>
<span id="cb3-515"><a href="#cb3-515"></a></span>
<span id="cb3-516"><a href="#cb3-516"></a>    <span class="co"># Rename value columns back for clarity in output *before* printing max KS info</span></span>
<span id="cb3-517"><a href="#cb3-517"></a>    ks_df.rename(columns<span class="op">=</span>{</span>
<span id="cb3-518"><a href="#cb3-518"></a>        <span class="st">'min_value'</span>: <span class="ss">f'min_</span><span class="sc">{</span>y_pred_col<span class="sc">}</span><span class="ss">'</span>,</span>
<span id="cb3-519"><a href="#cb3-519"></a>        <span class="st">'max_value'</span>: <span class="ss">f'max_</span><span class="sc">{</span>y_pred_col<span class="sc">}</span><span class="ss">'</span>,</span>
<span id="cb3-520"><a href="#cb3-520"></a>        <span class="st">'avg_value'</span>: <span class="ss">f'avg_</span><span class="sc">{</span>y_pred_col<span class="sc">}</span><span class="ss">'</span> <span class="co"># Rename average column</span></span>
<span id="cb3-521"><a href="#cb3-521"></a>        }, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-522"><a href="#cb3-522"></a></span>
<span id="cb3-523"><a href="#cb3-523"></a>    <span class="co"># Print the KS statistic and associated bin info before returning</span></span>
<span id="cb3-524"><a href="#cb3-524"></a>    <span class="cf">if</span> <span class="st">'ks'</span> <span class="kw">in</span> ks_df.columns <span class="kw">and</span> <span class="kw">not</span> ks_df[<span class="st">'ks'</span>].empty <span class="kw">and</span> total_bads <span class="op">&gt;</span> <span class="fl">1e-9</span> <span class="kw">and</span> total_goods <span class="op">&gt;</span> <span class="fl">1e-9</span>:</span>
<span id="cb3-525"><a href="#cb3-525"></a>        max_ks <span class="op">=</span> ks_df[<span class="st">'ks'</span>].<span class="bu">max</span>()</span>
<span id="cb3-526"><a href="#cb3-526"></a>        <span class="co"># Handle potential multiple max KS values - take the first one</span></span>
<span id="cb3-527"><a href="#cb3-527"></a>        max_ks_row <span class="op">=</span> ks_df.loc[ks_df[<span class="st">'ks'</span>].idxmax()]</span>
<span id="cb3-528"><a href="#cb3-528"></a>        min_val_at_max_ks <span class="op">=</span> max_ks_row[<span class="ss">f'min_</span><span class="sc">{</span>y_pred_col<span class="sc">}</span><span class="ss">'</span>]</span>
<span id="cb3-529"><a href="#cb3-529"></a>        max_val_at_max_ks <span class="op">=</span> max_ks_row[<span class="ss">f'max_</span><span class="sc">{</span>y_pred_col<span class="sc">}</span><span class="ss">'</span>]</span>
<span id="cb3-530"><a href="#cb3-530"></a>        avg_val_at_max_ks <span class="op">=</span> max_ks_row[<span class="ss">f'avg_</span><span class="sc">{</span>y_pred_col<span class="sc">}</span><span class="ss">'</span>]</span>
<span id="cb3-531"><a href="#cb3-531"></a></span>
<span id="cb3-532"><a href="#cb3-532"></a>        <span class="bu">print</span>(<span class="ss">f"KS Statistic (Max KS): </span><span class="sc">{</span>max_ks<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb3-533"><a href="#cb3-533"></a>        <span class="bu">print</span>(<span class="ss">f"  Occurs in bin with </span><span class="sc">{</span>y_pred_col<span class="sc">}</span><span class="ss"> range: [</span><span class="sc">{</span>min_val_at_max_ks<span class="sc">:.4f}</span><span class="ss"> - </span><span class="sc">{</span>max_val_at_max_ks<span class="sc">:.4f}</span><span class="ss">]"</span>)</span>
<span id="cb3-534"><a href="#cb3-534"></a>        <span class="bu">print</span>(<span class="ss">f"  Average </span><span class="sc">{</span>y_pred_col<span class="sc">}</span><span class="ss"> in this bin: </span><span class="sc">{</span>avg_val_at_max_ks<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb3-535"><a href="#cb3-535"></a>    <span class="cf">elif</span> total_bads <span class="op">&lt;=</span> <span class="fl">1e-9</span> <span class="kw">or</span> total_goods <span class="op">&lt;=</span> <span class="fl">1e-9</span>:</span>
<span id="cb3-536"><a href="#cb3-536"></a>        <span class="bu">print</span>(<span class="st">"KS Statistic is 0 because total weighted goods or bads is effectively zero."</span>)</span>
<span id="cb3-537"><a href="#cb3-537"></a>    <span class="cf">else</span>:</span>
<span id="cb3-538"><a href="#cb3-538"></a>        <span class="bu">print</span>(<span class="st">"KS Statistic could not be calculated (check input data and binning)."</span>)</span>
<span id="cb3-539"><a href="#cb3-539"></a></span>
<span id="cb3-540"><a href="#cb3-540"></a></span>
<span id="cb3-541"><a href="#cb3-541"></a>    <span class="co"># Reorder columns for final output</span></span>
<span id="cb3-542"><a href="#cb3-542"></a>    final_cols <span class="op">=</span> [</span>
<span id="cb3-543"><a href="#cb3-543"></a>            <span class="ss">f"min_</span><span class="sc">{</span>y_pred_col<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb3-544"><a href="#cb3-544"></a>            <span class="ss">f"max_</span><span class="sc">{</span>y_pred_col<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb3-545"><a href="#cb3-545"></a>            <span class="ss">f"avg_</span><span class="sc">{</span>y_pred_col<span class="sc">}</span><span class="ss">"</span>, <span class="co"># Add average column to output list</span></span>
<span id="cb3-546"><a href="#cb3-546"></a>            <span class="st">"count"</span>, <span class="co"># Represents total weight (or count if unweighted)</span></span>
<span id="cb3-547"><a href="#cb3-547"></a>            <span class="st">"bads"</span>,  <span class="co"># Represents total bad weight (or bad count if unweighted)</span></span>
<span id="cb3-548"><a href="#cb3-548"></a>            <span class="st">"goods"</span>, <span class="co"># Represents total good weight (or good count if unweighted)</span></span>
<span id="cb3-549"><a href="#cb3-549"></a>            <span class="st">"bad_rate"</span>, <span class="co"># Weighted bad rate</span></span>
<span id="cb3-550"><a href="#cb3-550"></a>            <span class="st">"cum_bads_pct"</span>,</span>
<span id="cb3-551"><a href="#cb3-551"></a>            <span class="st">"cum_goods_pct"</span>,</span>
<span id="cb3-552"><a href="#cb3-552"></a>            <span class="st">"ks"</span>,</span>
<span id="cb3-553"><a href="#cb3-553"></a>        ]</span>
<span id="cb3-554"><a href="#cb3-554"></a>    <span class="co"># Ensure all expected columns exist before selecting</span></span>
<span id="cb3-555"><a href="#cb3-555"></a>    final_cols <span class="op">=</span> [col <span class="cf">for</span> col <span class="kw">in</span> final_cols <span class="cf">if</span> col <span class="kw">in</span> ks_df.columns]</span>
<span id="cb3-556"><a href="#cb3-556"></a>    <span class="cf">return</span> ks_df[final_cols].reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-557"><a href="#cb3-557"></a></span>
<span id="cb3-558"><a href="#cb3-558"></a></span>
<span id="cb3-559"><a href="#cb3-559"></a><span class="kw">def</span> show_pdp(wrappedAGModel: BaseEstimator,</span>
<span id="cb3-560"><a href="#cb3-560"></a>            list_features: <span class="bu">list</span>,</span>
<span id="cb3-561"><a href="#cb3-561"></a>            list_categ_features: <span class="bu">list</span>,</span>
<span id="cb3-562"><a href="#cb3-562"></a>            df: pd.DataFrame,</span>
<span id="cb3-563"><a href="#cb3-563"></a>            xGTzero: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb3-564"><a href="#cb3-564"></a>            sampSize: <span class="bu">int</span> <span class="op">=</span> <span class="dv">40000</span>,</span>
<span id="cb3-565"><a href="#cb3-565"></a>            show_ice: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>) <span class="op">-&gt;</span> <span class="va">None</span>: <span class="co"># Added show_ice parameter</span></span>
<span id="cb3-566"><a href="#cb3-566"></a></span>
<span id="cb3-567"><a href="#cb3-567"></a>    <span class="cf">for</span> feature <span class="kw">in</span> list_features:</span>
<span id="cb3-568"><a href="#cb3-568"></a></span>
<span id="cb3-569"><a href="#cb3-569"></a>        fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">4</span>))</span>
<span id="cb3-570"><a href="#cb3-570"></a>        ax <span class="op">=</span> fig.add_subplot(<span class="dv">111</span>)</span>
<span id="cb3-571"><a href="#cb3-571"></a></span>
<span id="cb3-572"><a href="#cb3-572"></a>        plt.rcParams.update({<span class="st">'font.size'</span>: <span class="dv">16</span>})</span>
<span id="cb3-573"><a href="#cb3-573"></a></span>
<span id="cb3-574"><a href="#cb3-574"></a>        <span class="co"># Determine kind and subsample based on show_ice</span></span>
<span id="cb3-575"><a href="#cb3-575"></a>        plot_kind <span class="op">=</span> <span class="st">'both'</span> <span class="cf">if</span> show_ice <span class="cf">else</span> <span class="st">'average'</span></span>
<span id="cb3-576"><a href="#cb3-576"></a>        ice_subsample <span class="op">=</span> <span class="dv">250</span> <span class="cf">if</span> show_ice <span class="cf">else</span> <span class="va">None</span> <span class="co"># Subsample for ICE lines</span></span>
<span id="cb3-577"><a href="#cb3-577"></a>        ice_lines_kw <span class="op">=</span> {<span class="st">"color"</span>: <span class="st">"tab:blue"</span>, <span class="st">"alpha"</span>: <span class="fl">0.2</span>, <span class="st">"linewidth"</span>: <span class="fl">0.5</span>} <span class="cf">if</span> show_ice <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb3-578"><a href="#cb3-578"></a>        pd_line_kw <span class="op">=</span> {<span class="st">"color"</span>: <span class="st">"tab:red"</span>, <span class="st">"linestyle"</span>: <span class="st">"--"</span>, <span class="st">"linewidth"</span>: <span class="dv">2</span>}</span>
<span id="cb3-579"><a href="#cb3-579"></a></span>
<span id="cb3-580"><a href="#cb3-580"></a>        <span class="co"># Get the expected feature order from the fitted model</span></span>
<span id="cb3-581"><a href="#cb3-581"></a>        feature_order <span class="op">=</span> wrappedAGModel.feature_names_</span>
<span id="cb3-582"><a href="#cb3-582"></a>        <span class="co"># Sample data for the main PDP calculation, ensuring correct column order</span></span>
<span id="cb3-583"><a href="#cb3-583"></a>        X_sample <span class="op">=</span> df[feature_order].sample(<span class="bu">min</span>(sampSize, <span class="bu">len</span>(df)), random_state<span class="op">=</span><span class="dv">2025</span>) <span class="co"># Reorder columns before sampling</span></span>
<span id="cb3-584"><a href="#cb3-584"></a></span>
<span id="cb3-585"><a href="#cb3-585"></a>        disp <span class="op">=</span> PartialDependenceDisplay.from_estimator(</span>
<span id="cb3-586"><a href="#cb3-586"></a>            estimator <span class="op">=</span> wrappedAGModel,</span>
<span id="cb3-587"><a href="#cb3-587"></a>            X <span class="op">=</span> X_sample, <span class="co"># Use the sampled data with correct column order</span></span>
<span id="cb3-588"><a href="#cb3-588"></a>            features <span class="op">=</span> [feature],</span>
<span id="cb3-589"><a href="#cb3-589"></a>            categorical_features <span class="op">=</span> list_categ_features,</span>
<span id="cb3-590"><a href="#cb3-590"></a>            method <span class="op">=</span> <span class="st">'brute'</span>,</span>
<span id="cb3-591"><a href="#cb3-591"></a>            kind <span class="op">=</span> plot_kind, <span class="co"># Use 'both' or 'average'</span></span>
<span id="cb3-592"><a href="#cb3-592"></a>            subsample <span class="op">=</span> ice_subsample, <span class="co"># Subsample for ICE lines if kind='both'</span></span>
<span id="cb3-593"><a href="#cb3-593"></a>            ice_lines_kw <span class="op">=</span> ice_lines_kw, <span class="co"># Style for ICE lines</span></span>
<span id="cb3-594"><a href="#cb3-594"></a>            pd_line_kw <span class="op">=</span> pd_line_kw, <span class="co"># Style for PDP line</span></span>
<span id="cb3-595"><a href="#cb3-595"></a>            percentiles<span class="op">=</span>(<span class="fl">0.0001</span>, <span class="fl">0.9999</span>),</span>
<span id="cb3-596"><a href="#cb3-596"></a>            grid_resolution<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb3-597"><a href="#cb3-597"></a>            ax <span class="op">=</span> ax,</span>
<span id="cb3-598"><a href="#cb3-598"></a>            random_state<span class="op">=</span><span class="dv">2025</span>,</span>
<span id="cb3-599"><a href="#cb3-599"></a>            n_jobs <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb3-600"><a href="#cb3-600"></a>        )</span>
<span id="cb3-601"><a href="#cb3-601"></a></span>
<span id="cb3-602"><a href="#cb3-602"></a>        plot_title <span class="op">=</span> <span class="ss">f"Partial Dependence for </span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb3-603"><a href="#cb3-603"></a>        <span class="cf">if</span> show_ice:</span>
<span id="cb3-604"><a href="#cb3-604"></a>            plot_title <span class="op">+=</span> <span class="st">" (with ICE)"</span></span>
<span id="cb3-605"><a href="#cb3-605"></a>        ax.set_title(plot_title)</span>
<span id="cb3-606"><a href="#cb3-606"></a></span>
<span id="cb3-607"><a href="#cb3-607"></a>        <span class="co"># Set y-axis lower limit for all axes in the current figure</span></span>
<span id="cb3-608"><a href="#cb3-608"></a>        <span class="cf">for</span> a <span class="kw">in</span> fig.get_axes():</span>
<span id="cb3-609"><a href="#cb3-609"></a>            coordy <span class="op">=</span> a.get_ylim()</span>
<span id="cb3-610"><a href="#cb3-610"></a>            <span class="co"># Adjust y-axis limits, potentially making space for ICE lines</span></span>
<span id="cb3-611"><a href="#cb3-611"></a>            y_bottom <span class="op">=</span> <span class="dv">0</span> <span class="cf">if</span> <span class="kw">not</span> show_ice <span class="cf">else</span> <span class="bu">min</span>(<span class="dv">0</span>, coordy[<span class="dv">0</span>]) <span class="co"># Allow negative if ICE shown</span></span>
<span id="cb3-612"><a href="#cb3-612"></a>            a.set_ylim(bottom<span class="op">=</span>y_bottom, top<span class="op">=</span>coordy[<span class="dv">1</span>]<span class="op">*</span><span class="fl">1.1</span>)</span>
<span id="cb3-613"><a href="#cb3-613"></a></span>
<span id="cb3-614"><a href="#cb3-614"></a>        <span class="cf">if</span> xGTzero:</span>
<span id="cb3-615"><a href="#cb3-615"></a>            <span class="co"># Set x-axis from 0 to the max</span></span>
<span id="cb3-616"><a href="#cb3-616"></a>            <span class="cf">for</span> a <span class="kw">in</span> fig.get_axes():</span>
<span id="cb3-617"><a href="#cb3-617"></a>                <span class="co"># Calculate percentile on the original feature column if possible</span></span>
<span id="cb3-618"><a href="#cb3-618"></a>                <span class="cf">if</span> feature <span class="kw">in</span> df.columns:</span>
<span id="cb3-619"><a href="#cb3-619"></a>                    max_val <span class="op">=</span> np.percentile(df[feature].dropna().values, <span class="fl">99.99</span>)</span>
<span id="cb3-620"><a href="#cb3-620"></a>                    a.set_xlim(left<span class="op">=</span><span class="dv">0</span>, right<span class="op">=</span>max_val)</span>
<span id="cb3-621"><a href="#cb3-621"></a>                <span class="cf">else</span>: <span class="co"># Fallback if feature not directly in df (e.g., transformed)</span></span>
<span id="cb3-622"><a href="#cb3-622"></a>                        max_val <span class="op">=</span> a.get_xlim()[<span class="dv">1</span>] <span class="co"># Use current max</span></span>
<span id="cb3-623"><a href="#cb3-623"></a>                        a.set_xlim(left<span class="op">=</span><span class="dv">0</span>, right<span class="op">=</span>max_val)</span>
<span id="cb3-624"><a href="#cb3-624"></a></span>
<span id="cb3-625"><a href="#cb3-625"></a></span>
<span id="cb3-626"><a href="#cb3-626"></a>        plt.show()</span>
<span id="cb3-627"><a href="#cb3-627"></a>        plt.close(<span class="st">'all'</span>)  <span class="co"># Prevent figure overload</span></span>
<span id="cb3-628"><a href="#cb3-628"></a></span>
<span id="cb3-629"><a href="#cb3-629"></a></span>
<span id="cb3-630"><a href="#cb3-630"></a><span class="kw">def</span> generate_eda_report(df: pd.DataFrame, title: <span class="bu">str</span>, output_path: <span class="bu">str</span>, sample_frac: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.1</span>, random_state: <span class="bu">int</span> <span class="op">=</span> <span class="dv">2025</span>):</span>
<span id="cb3-631"><a href="#cb3-631"></a>    <span class="co">"""Generates and saves a ydata-profiling report for a DataFrame."""</span></span>
<span id="cb3-632"><a href="#cb3-632"></a>    <span class="bu">print</span>(<span class="ss">f"Generating EDA report: </span><span class="sc">{</span>title<span class="sc">}</span><span class="ss">..."</span>)</span>
<span id="cb3-633"><a href="#cb3-633"></a>    <span class="cf">try</span>:</span>
<span id="cb3-634"><a href="#cb3-634"></a>        profile <span class="op">=</span> ProfileReport(</span>
<span id="cb3-635"><a href="#cb3-635"></a>            df.sample(frac<span class="op">=</span>sample_frac, random_state<span class="op">=</span>random_state) <span class="cf">if</span> sample_frac <span class="op">&lt;</span> <span class="fl">1.0</span> <span class="cf">else</span> df,</span>
<span id="cb3-636"><a href="#cb3-636"></a>            title<span class="op">=</span>title,</span>
<span id="cb3-637"><a href="#cb3-637"></a>            progress_bar<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb3-638"><a href="#cb3-638"></a>            duplicates<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb3-639"><a href="#cb3-639"></a>            interactions<span class="op">=</span><span class="va">None</span></span>
<span id="cb3-640"><a href="#cb3-640"></a>        )</span>
<span id="cb3-641"><a href="#cb3-641"></a>        profile.to_file(output_path)</span>
<span id="cb3-642"><a href="#cb3-642"></a>        <span class="bu">print</span>(<span class="ss">f"Report saved to: </span><span class="sc">{</span>output_path<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-643"><a href="#cb3-643"></a>    <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb3-644"><a href="#cb3-644"></a>        <span class="bu">print</span>(<span class="ss">f"Error generating report '</span><span class="sc">{</span>title<span class="sc">}</span><span class="ss">': </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-645"><a href="#cb3-645"></a></span>
<span id="cb3-646"><a href="#cb3-646"></a><span class="kw">def</span> split_data(df: pd.DataFrame, target_col: <span class="bu">str</span> <span class="op">|</span> <span class="va">None</span> <span class="op">=</span> <span class="va">None</span>, train_size: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.6</span>, calib_size_rel: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.5</span>, random_state: <span class="bu">int</span> <span class="op">=</span> <span class="dv">2025</span>) <span class="op">-&gt;</span> <span class="bu">tuple</span>[pd.DataFrame, pd.DataFrame, pd.DataFrame]:</span>
<span id="cb3-647"><a href="#cb3-647"></a>    <span class="co">"""Splits data into train, calibration, and test sets."""</span></span>
<span id="cb3-648"><a href="#cb3-648"></a>    <span class="bu">print</span>(<span class="ss">f"Splitting data (stratify=</span><span class="sc">{</span><span class="st">'Yes'</span> <span class="cf">if</span> target_col <span class="cf">else</span> <span class="st">'No'</span><span class="sc">}</span><span class="ss">)..."</span>)</span>
<span id="cb3-649"><a href="#cb3-649"></a>    y <span class="op">=</span> df[target_col] <span class="cf">if</span> target_col <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb3-650"><a href="#cb3-650"></a></span>
<span id="cb3-651"><a href="#cb3-651"></a>    <span class="cf">if</span> target_col:</span>
<span id="cb3-652"><a href="#cb3-652"></a>        stratify_param <span class="op">=</span> y</span>
<span id="cb3-653"><a href="#cb3-653"></a>        <span class="co"># First split: Train and Temp (with stratification)</span></span>
<span id="cb3-654"><a href="#cb3-654"></a>        df_train, df_temp, y_train, y_temp <span class="op">=</span> train_test_split(</span>
<span id="cb3-655"><a href="#cb3-655"></a>            df, y, train_size<span class="op">=</span>train_size, random_state<span class="op">=</span>random_state, stratify<span class="op">=</span>stratify_param</span>
<span id="cb3-656"><a href="#cb3-656"></a>        )</span>
<span id="cb3-657"><a href="#cb3-657"></a>        <span class="co"># Second split: Temp into Calibration and Test (with stratification)</span></span>
<span id="cb3-658"><a href="#cb3-658"></a>        stratify_param_temp <span class="op">=</span> y_temp</span>
<span id="cb3-659"><a href="#cb3-659"></a>        df_calib, df_test, y_calib, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb3-660"><a href="#cb3-660"></a>            df_temp, y_temp, test_size<span class="op">=</span>calib_size_rel, random_state<span class="op">=</span>random_state, stratify<span class="op">=</span>stratify_param_temp</span>
<span id="cb3-661"><a href="#cb3-661"></a>        )</span>
<span id="cb3-662"><a href="#cb3-662"></a>    <span class="cf">else</span>:</span>
<span id="cb3-663"><a href="#cb3-663"></a>        <span class="co"># First split: Train and Temp (without stratification)</span></span>
<span id="cb3-664"><a href="#cb3-664"></a>        df_train, df_temp <span class="op">=</span> train_test_split(</span>
<span id="cb3-665"><a href="#cb3-665"></a>            df, train_size<span class="op">=</span>train_size, random_state<span class="op">=</span>random_state, stratify<span class="op">=</span><span class="va">None</span></span>
<span id="cb3-666"><a href="#cb3-666"></a>        )</span>
<span id="cb3-667"><a href="#cb3-667"></a>        <span class="co"># Second split: Temp into Calibration and Test (without stratification)</span></span>
<span id="cb3-668"><a href="#cb3-668"></a>        df_calib, df_test <span class="op">=</span> train_test_split(</span>
<span id="cb3-669"><a href="#cb3-669"></a>            df_temp, test_size<span class="op">=</span>calib_size_rel, random_state<span class="op">=</span>random_state, stratify<span class="op">=</span><span class="va">None</span></span>
<span id="cb3-670"><a href="#cb3-670"></a>        )</span>
<span id="cb3-671"><a href="#cb3-671"></a>        <span class="co"># Assign None to y splits as they don't exist</span></span>
<span id="cb3-672"><a href="#cb3-672"></a>        y_train, y_temp, y_calib, y_test <span class="op">=</span> <span class="va">None</span>, <span class="va">None</span>, <span class="va">None</span>, <span class="va">None</span></span>
<span id="cb3-673"><a href="#cb3-673"></a></span>
<span id="cb3-674"><a href="#cb3-674"></a></span>
<span id="cb3-675"><a href="#cb3-675"></a>    <span class="bu">print</span>(<span class="ss">f"  Train shape: </span><span class="sc">{</span>df_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-676"><a href="#cb3-676"></a>    <span class="bu">print</span>(<span class="ss">f"  Calibration shape: </span><span class="sc">{</span>df_calib<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-677"><a href="#cb3-677"></a>    <span class="bu">print</span>(<span class="ss">f"  Test shape: </span><span class="sc">{</span>df_test<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-678"><a href="#cb3-678"></a>    <span class="cf">if</span> target_col:</span>
<span id="cb3-679"><a href="#cb3-679"></a>        <span class="bu">print</span>(<span class="ss">f"  Train target mean: </span><span class="sc">{</span>y_train<span class="sc">.</span>mean()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb3-680"><a href="#cb3-680"></a>        <span class="bu">print</span>(<span class="ss">f"  Calibration target mean: </span><span class="sc">{</span>y_calib<span class="sc">.</span>mean()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb3-681"><a href="#cb3-681"></a>        <span class="bu">print</span>(<span class="ss">f"  Test target mean: </span><span class="sc">{</span>y_test<span class="sc">.</span>mean()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb3-682"><a href="#cb3-682"></a></span>
<span id="cb3-683"><a href="#cb3-683"></a>    <span class="co"># Return copies</span></span>
<span id="cb3-684"><a href="#cb3-684"></a>    <span class="cf">return</span> df_train.copy(), df_calib.copy(), df_test.copy()</span>
<span id="cb3-685"><a href="#cb3-685"></a></span>
<span id="cb3-686"><a href="#cb3-686"></a></span>
<span id="cb3-687"><a href="#cb3-687"></a><span class="kw">def</span> train_autogluon_model(</span>
<span id="cb3-688"><a href="#cb3-688"></a>    df_train: pd.DataFrame,</span>
<span id="cb3-689"><a href="#cb3-689"></a>    label: <span class="bu">str</span>,</span>
<span id="cb3-690"><a href="#cb3-690"></a>    weight_col: <span class="bu">str</span> <span class="op">|</span> <span class="va">None</span>,</span>
<span id="cb3-691"><a href="#cb3-691"></a>    modeling_features: <span class="bu">list</span>[<span class="bu">str</span>],</span>
<span id="cb3-692"><a href="#cb3-692"></a>    model_folder: <span class="bu">str</span>,</span>
<span id="cb3-693"><a href="#cb3-693"></a>    predictor_args: <span class="bu">dict</span>,</span>
<span id="cb3-694"><a href="#cb3-694"></a>    fit_args: <span class="bu">dict</span></span>
<span id="cb3-695"><a href="#cb3-695"></a>) <span class="op">-&gt;</span> AutoGluonSklearnWrapper:</span>
<span id="cb3-696"><a href="#cb3-696"></a>    <span class="co">"""Trains an AutoGluon model using the Sklearn wrapper."""</span></span>
<span id="cb3-697"><a href="#cb3-697"></a>    <span class="bu">print</span>(<span class="ss">f"--- Training AutoGluon Model in: </span><span class="sc">{</span>model_folder<span class="sc">}</span><span class="ss"> ---"</span>)</span>
<span id="cb3-698"><a href="#cb3-698"></a>    remove_ag_folder(model_folder) <span class="co"># Clean up previous runs</span></span>
<span id="cb3-699"><a href="#cb3-699"></a></span>
<span id="cb3-700"><a href="#cb3-700"></a>    <span class="bu">print</span>(<span class="ss">f"Using features: </span><span class="sc">{</span>modeling_features<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-701"><a href="#cb3-701"></a>    <span class="bu">print</span>(<span class="ss">f"Training data shape: </span><span class="sc">{</span>df_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-702"><a href="#cb3-702"></a>    <span class="cf">if</span> weight_col:</span>
<span id="cb3-703"><a href="#cb3-703"></a>        <span class="bu">print</span>(<span class="ss">f"Sum of weights in training data: </span><span class="sc">{</span>df_train[weight_col]<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb3-704"><a href="#cb3-704"></a></span>
<span id="cb3-705"><a href="#cb3-705"></a>    start_time <span class="op">=</span> time.time()</span>
<span id="cb3-706"><a href="#cb3-706"></a></span>
<span id="cb3-707"><a href="#cb3-707"></a>    <span class="co"># Prepare X, y, and weights for the wrapper's fit method</span></span>
<span id="cb3-708"><a href="#cb3-708"></a>    X_train_ag <span class="op">=</span> df_train[modeling_features]</span>
<span id="cb3-709"><a href="#cb3-709"></a>    y_train_ag <span class="op">=</span> df_train[label]</span>
<span id="cb3-710"><a href="#cb3-710"></a>    weights_train_ag <span class="op">=</span> df_train[weight_col] <span class="cf">if</span> weight_col <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb3-711"><a href="#cb3-711"></a></span>
<span id="cb3-712"><a href="#cb3-712"></a>    <span class="co"># Update predictor_args with sample_weight if provided</span></span>
<span id="cb3-713"><a href="#cb3-713"></a>    <span class="cf">if</span> weight_col:</span>
<span id="cb3-714"><a href="#cb3-714"></a>        predictor_args[<span class="st">'sample_weight'</span>] <span class="op">=</span> weight_col</span>
<span id="cb3-715"><a href="#cb3-715"></a></span>
<span id="cb3-716"><a href="#cb3-716"></a>    ag_model_wrapped <span class="op">=</span> AutoGluonSklearnWrapper(</span>
<span id="cb3-717"><a href="#cb3-717"></a>        label<span class="op">=</span>label,</span>
<span id="cb3-718"><a href="#cb3-718"></a>        predictor_args<span class="op">=</span>predictor_args,</span>
<span id="cb3-719"><a href="#cb3-719"></a>        fit_args<span class="op">=</span>fit_args</span>
<span id="cb3-720"><a href="#cb3-720"></a>    )</span>
<span id="cb3-721"><a href="#cb3-721"></a></span>
<span id="cb3-722"><a href="#cb3-722"></a>    <span class="co"># Fit the model</span></span>
<span id="cb3-723"><a href="#cb3-723"></a>    ag_model_wrapped.fit(X_train_ag, y_train_ag, sample_weight<span class="op">=</span>weights_train_ag)</span>
<span id="cb3-724"><a href="#cb3-724"></a></span>
<span id="cb3-725"><a href="#cb3-725"></a>    end_time <span class="op">=</span> time.time()</span>
<span id="cb3-726"><a href="#cb3-726"></a>    <span class="bu">print</span>(<span class="ss">f"AutoGluon training completed in </span><span class="sc">{</span>end_time <span class="op">-</span> start_time<span class="sc">:.2f}</span><span class="ss"> seconds."</span>)</span>
<span id="cb3-727"><a href="#cb3-727"></a></span>
<span id="cb3-728"><a href="#cb3-728"></a>    <span class="co"># Display leaderboard</span></span>
<span id="cb3-729"><a href="#cb3-729"></a>    ag_predictor <span class="op">=</span> ag_model_wrapped.predictor</span>
<span id="cb3-730"><a href="#cb3-730"></a>    <span class="cf">if</span> ag_predictor:</span>
<span id="cb3-731"><a href="#cb3-731"></a>        <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">AutoGluon Leaderboard:"</span>)</span>
<span id="cb3-732"><a href="#cb3-732"></a>        leaderboard <span class="op">=</span> ag_predictor.leaderboard(silent<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-733"><a href="#cb3-733"></a>        display(leaderboard)</span>
<span id="cb3-734"><a href="#cb3-734"></a>    <span class="cf">else</span>:</span>
<span id="cb3-735"><a href="#cb3-735"></a>        <span class="bu">print</span>(<span class="st">"Could not access underlying predictor to display leaderboard."</span>)</span>
<span id="cb3-736"><a href="#cb3-736"></a></span>
<span id="cb3-737"><a href="#cb3-737"></a>    <span class="cf">return</span> ag_model_wrapped</span>
<span id="cb3-738"><a href="#cb3-738"></a></span>
<span id="cb3-739"><a href="#cb3-739"></a><span class="kw">def</span> summarize_ttd_by_source(df_ttd: pd.DataFrame, target_col: <span class="bu">str</span> <span class="op">=</span> <span class="st">'default_flag'</span>, weight_col: <span class="bu">str</span> <span class="op">=</span> <span class="st">'sample_weight'</span>, source_col: <span class="bu">str</span> <span class="op">=</span> <span class="st">'source'</span>) <span class="op">-&gt;</span> pd.DataFrame:</span>
<span id="cb3-740"><a href="#cb3-740"></a>    <span class="co">"""Calculates summary statistics (counts, weights, rates) for a TTD DataFrame grouped by source."""</span></span>
<span id="cb3-741"><a href="#cb3-741"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">--- Summarizing TTD Data by '</span><span class="sc">{</span>source_col<span class="sc">}</span><span class="ss">' ---"</span>)</span>
<span id="cb3-742"><a href="#cb3-742"></a>    <span class="co"># Step 1: Calculate standard aggregations</span></span>
<span id="cb3-743"><a href="#cb3-743"></a>    summary_stats <span class="op">=</span> df_ttd.groupby(source_col).agg(</span>
<span id="cb3-744"><a href="#cb3-744"></a>        row_count<span class="op">=</span>(target_col, <span class="st">'size'</span>),</span>
<span id="cb3-745"><a href="#cb3-745"></a>        sum_weights<span class="op">=</span>(weight_col, <span class="st">'sum'</span>),</span>
<span id="cb3-746"><a href="#cb3-746"></a>        unweighted_default_rate<span class="op">=</span>(target_col, <span class="st">'mean'</span>)</span>
<span id="cb3-747"><a href="#cb3-747"></a>    ).reset_index()</span>
<span id="cb3-748"><a href="#cb3-748"></a></span>
<span id="cb3-749"><a href="#cb3-749"></a>    <span class="co"># Step 2: Calculate weighted default rate separately using apply</span></span>
<span id="cb3-750"><a href="#cb3-750"></a>    weighted_rates <span class="op">=</span> df_ttd.groupby(source_col).<span class="bu">apply</span>(</span>
<span id="cb3-751"><a href="#cb3-751"></a>        <span class="kw">lambda</span> x: np.average(x[target_col], weights<span class="op">=</span>x[weight_col]) <span class="cf">if</span> x[weight_col].<span class="bu">sum</span>() <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> np.nan</span>
<span id="cb3-752"><a href="#cb3-752"></a>    ).reset_index(name<span class="op">=</span><span class="st">'weighted_default_rate'</span>)</span>
<span id="cb3-753"><a href="#cb3-753"></a></span>
<span id="cb3-754"><a href="#cb3-754"></a>    <span class="co"># Step 3: Merge the results</span></span>
<span id="cb3-755"><a href="#cb3-755"></a>    summary_df <span class="op">=</span> pd.merge(summary_stats, weighted_rates, on<span class="op">=</span>source_col)</span>
<span id="cb3-756"><a href="#cb3-756"></a></span>
<span id="cb3-757"><a href="#cb3-757"></a>    <span class="co"># Display the summary</span></span>
<span id="cb3-758"><a href="#cb3-758"></a>    <span class="bu">print</span>(<span class="st">"Summary of Default Rates by Source:"</span>)</span>
<span id="cb3-759"><a href="#cb3-759"></a>    display(summary_df[[source_col, <span class="st">'row_count'</span>, <span class="st">'unweighted_default_rate'</span>, <span class="st">'sum_weights'</span>, <span class="st">'weighted_default_rate'</span>]])</span>
<span id="cb3-760"><a href="#cb3-760"></a>    <span class="cf">return</span> summary_df</span>
<span id="cb3-761"><a href="#cb3-761"></a></span>
<span id="cb3-762"><a href="#cb3-762"></a><span class="kw">def</span> create_TTD_data(</span>
<span id="cb3-763"><a href="#cb3-763"></a>    ri_model: BaseEstimator <span class="op">|</span> <span class="va">None</span>,  <span class="co"># Modified to accept None</span></span>
<span id="cb3-764"><a href="#cb3-764"></a>    df_rejected: pd.DataFrame,</span>
<span id="cb3-765"><a href="#cb3-765"></a>    df_accepted: pd.DataFrame,</span>
<span id="cb3-766"><a href="#cb3-766"></a>    ri_features: <span class="bu">list</span>[<span class="bu">str</span>],</span>
<span id="cb3-767"><a href="#cb3-767"></a>    modeling_features: <span class="bu">list</span>[<span class="bu">str</span>],</span>
<span id="cb3-768"><a href="#cb3-768"></a>    target_col: <span class="bu">str</span> <span class="op">=</span> <span class="st">'default_flag'</span>,</span>
<span id="cb3-769"><a href="#cb3-769"></a>    clone_rejected: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span></span>
<span id="cb3-770"><a href="#cb3-770"></a>) <span class="op">-&gt;</span> pd.DataFrame:</span>
<span id="cb3-771"><a href="#cb3-771"></a>    <span class="co">"""</span></span>
<span id="cb3-772"><a href="#cb3-772"></a><span class="co">    Creates a Through-the-Door (TTD) dataset using Fuzzy Augmentation.</span></span>
<span id="cb3-773"><a href="#cb3-773"></a></span>
<span id="cb3-774"><a href="#cb3-774"></a><span class="co">    Applies a trained reject inference (RI) model to rejected applicants to estimate</span></span>
<span id="cb3-775"><a href="#cb3-775"></a><span class="co">    their probability of default, then creates weighted copies of the rejected data</span></span>
<span id="cb3-776"><a href="#cb3-776"></a><span class="co">    and combines them with the accepted data. Adds a 'source' column to indicate</span></span>
<span id="cb3-777"><a href="#cb3-777"></a><span class="co">    origin ('Accepted' or 'Rejected').</span></span>
<span id="cb3-778"><a href="#cb3-778"></a></span>
<span id="cb3-779"><a href="#cb3-779"></a><span class="co">    If ri_model is None, skips fuzzy augmentation and uses weight=1 for all records.</span></span>
<span id="cb3-780"><a href="#cb3-780"></a></span>
<span id="cb3-781"><a href="#cb3-781"></a><span class="co">    Args:</span></span>
<span id="cb3-782"><a href="#cb3-782"></a><span class="co">        ri_model: A trained scikit-learn compatible model used for reject inference.</span></span>
<span id="cb3-783"><a href="#cb3-783"></a><span class="co">                    Must have a `predict_proba` method. If None, fuzzy augmentation</span></span>
<span id="cb3-784"><a href="#cb3-784"></a><span class="co">                    is skipped and all weights are set to 1. Setting ri_model to None is intended for production-use, where a decision has yet to be made on an applicant.</span></span>
<span id="cb3-785"><a href="#cb3-785"></a><span class="co">        df_rejected: DataFrame containing rejected applicant data with common features.</span></span>
<span id="cb3-786"><a href="#cb3-786"></a><span class="co">        df_accepted: DataFrame containing accepted applicant data with common features</span></span>
<span id="cb3-787"><a href="#cb3-787"></a><span class="co">                        and the target variable.</span></span>
<span id="cb3-788"><a href="#cb3-788"></a><span class="co">        ri_features: List of feature names used by the ri_model.</span></span>
<span id="cb3-789"><a href="#cb3-789"></a><span class="co">        modeling_features: List of feature names to include in the final TTD dataset</span></span>
<span id="cb3-790"><a href="#cb3-790"></a><span class="co">                            (should be present in both df_rejected and df_accepted).</span></span>
<span id="cb3-791"><a href="#cb3-791"></a><span class="co">        target_col: Name of the target variable column in df_accepted.</span></span>
<span id="cb3-792"><a href="#cb3-792"></a><span class="co">        clone_rejected: If True and using fuzzy augmentation, creates two copies of</span></span>
<span id="cb3-793"><a href="#cb3-793"></a><span class="co">                        rejected data (one for each class). If False or not using</span></span>
<span id="cb3-794"><a href="#cb3-794"></a><span class="co">                        fuzzy augmentation, only includes one copy of rejected data.</span></span>
<span id="cb3-795"><a href="#cb3-795"></a></span>
<span id="cb3-796"><a href="#cb3-796"></a><span class="co">    Returns:</span></span>
<span id="cb3-797"><a href="#cb3-797"></a><span class="co">        A pandas DataFrame representing the augmented TTD dataset with features,</span></span>
<span id="cb3-798"><a href="#cb3-798"></a><span class="co">        the target column, a 'sample_weight' column, and a 'source' column.</span></span>
<span id="cb3-799"><a href="#cb3-799"></a><span class="co">    """</span></span>
<span id="cb3-800"><a href="#cb3-800"></a>    <span class="co"># Ensure dataframes are copies to avoid modifying originals</span></span>
<span id="cb3-801"><a href="#cb3-801"></a>    df_rejected_proc <span class="op">=</span> df_rejected.copy()</span>
<span id="cb3-802"><a href="#cb3-802"></a>    df_accepted_proc <span class="op">=</span> df_accepted.copy()</span>
<span id="cb3-803"><a href="#cb3-803"></a></span>
<span id="cb3-804"><a href="#cb3-804"></a>    <span class="co"># Check if ri_model is None - skip fuzzy augmentation if so</span></span>
<span id="cb3-805"><a href="#cb3-805"></a>    <span class="cf">if</span> ri_model <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb3-806"><a href="#cb3-806"></a>        <span class="bu">print</span>(<span class="st">"No RI model provided. Creating TTD data with uniform weights (sample_weight=1)..."</span>)</span>
<span id="cb3-807"><a href="#cb3-807"></a>        </span>
<span id="cb3-808"><a href="#cb3-808"></a>        <span class="co"># Prepare Accepted Data - select modeling features + target, assign weight = 1</span></span>
<span id="cb3-809"><a href="#cb3-809"></a>        df_accepted_weighted <span class="op">=</span> df_accepted_proc[modeling_features <span class="op">+</span> [target_col]].copy()</span>
<span id="cb3-810"><a href="#cb3-810"></a>        df_accepted_weighted[<span class="st">'sample_weight'</span>] <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb3-811"><a href="#cb3-811"></a>        df_accepted_weighted[<span class="st">'source'</span>] <span class="op">=</span> <span class="st">'Accepted'</span></span>
<span id="cb3-812"><a href="#cb3-812"></a>        </span>
<span id="cb3-813"><a href="#cb3-813"></a>        <span class="co"># Prepare Rejected Data - all assigned target_col = 1 (assume bad) with weight = 1</span></span>
<span id="cb3-814"><a href="#cb3-814"></a>        df_rejected_weighted <span class="op">=</span> df_rejected_proc[modeling_features].copy()</span>
<span id="cb3-815"><a href="#cb3-815"></a>        df_rejected_weighted[<span class="st">'sample_weight'</span>] <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb3-816"><a href="#cb3-816"></a>        df_rejected_weighted[target_col] <span class="op">=</span> <span class="va">None</span>  <span class="co"># Target col does not exist for rejects</span></span>
<span id="cb3-817"><a href="#cb3-817"></a>        df_rejected_weighted[<span class="st">'source'</span>] <span class="op">=</span> <span class="st">'Rejected'</span></span>
<span id="cb3-818"><a href="#cb3-818"></a>        </span>
<span id="cb3-819"><a href="#cb3-819"></a>        <span class="co"># Define columns for the final TTD dataset</span></span>
<span id="cb3-820"><a href="#cb3-820"></a>        cols_for_ttd <span class="op">=</span> modeling_features <span class="op">+</span> [target_col, <span class="st">'sample_weight'</span>, <span class="st">'source'</span>]</span>
<span id="cb3-821"><a href="#cb3-821"></a>        </span>
<span id="cb3-822"><a href="#cb3-822"></a>        <span class="co"># Concatenate accepted and rejected data</span></span>
<span id="cb3-823"><a href="#cb3-823"></a>        df_ttd <span class="op">=</span> pd.concat(</span>
<span id="cb3-824"><a href="#cb3-824"></a>            [df_accepted_weighted[cols_for_ttd], df_rejected_weighted[cols_for_ttd]],</span>
<span id="cb3-825"><a href="#cb3-825"></a>            ignore_index<span class="op">=</span><span class="va">True</span></span>
<span id="cb3-826"><a href="#cb3-826"></a>        )</span>
<span id="cb3-827"><a href="#cb3-827"></a>    <span class="cf">else</span>:</span>
<span id="cb3-828"><a href="#cb3-828"></a>        <span class="bu">print</span>(<span class="st">"Applying RI model to rejected data and calculating weights..."</span>)</span>
<span id="cb3-829"><a href="#cb3-829"></a>        </span>
<span id="cb3-830"><a href="#cb3-830"></a>        <span class="co"># 1. Prepare Rejected Data for RI Prediction</span></span>
<span id="cb3-831"><a href="#cb3-831"></a>        X_rej_ri <span class="op">=</span> df_rejected_proc[ri_features]</span>
<span id="cb3-832"><a href="#cb3-832"></a>        </span>
<span id="cb3-833"><a href="#cb3-833"></a>        <span class="co"># 2. Predict Probabilities for Rejected Applicants</span></span>
<span id="cb3-834"><a href="#cb3-834"></a>        prob_default_rejected <span class="op">=</span> ri_model.predict_proba(X_rej_ri)[:, <span class="dv">1</span>]</span>
<span id="cb3-835"><a href="#cb3-835"></a>        prob_good_rejected <span class="op">=</span> <span class="fl">1.0</span> <span class="op">-</span> prob_default_rejected  <span class="co"># P(default=0)</span></span>
<span id="cb3-836"><a href="#cb3-836"></a>        </span>
<span id="cb3-837"><a href="#cb3-837"></a>        <span class="co"># --- Fuzzy Augmentation Implementation ---</span></span>
<span id="cb3-838"><a href="#cb3-838"></a>        <span class="co"># 3. Create two weighted copies of rejected data</span></span>
<span id="cb3-839"><a href="#cb3-839"></a>        </span>
<span id="cb3-840"><a href="#cb3-840"></a>        <span class="co"># Copy 1: Assumed Bad (target_col = 1)</span></span>
<span id="cb3-841"><a href="#cb3-841"></a>        df_rejected_bad <span class="op">=</span> df_rejected_proc[modeling_features].copy()</span>
<span id="cb3-842"><a href="#cb3-842"></a>        df_rejected_bad[<span class="st">'sample_weight'</span>] <span class="op">=</span> prob_default_rejected  <span class="co"># Weight = P(default=1)</span></span>
<span id="cb3-843"><a href="#cb3-843"></a>        df_rejected_bad[target_col] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb3-844"><a href="#cb3-844"></a>        df_rejected_bad[<span class="st">'source'</span>] <span class="op">=</span> <span class="st">'Rejected'</span></span>
<span id="cb3-845"><a href="#cb3-845"></a>        </span>
<span id="cb3-846"><a href="#cb3-846"></a>        <span class="co"># Copy 2: Assumed Good (target_col = 0)</span></span>
<span id="cb3-847"><a href="#cb3-847"></a>        df_rejected_good <span class="op">=</span> df_rejected_proc[modeling_features].copy()</span>
<span id="cb3-848"><a href="#cb3-848"></a>        df_rejected_good[<span class="st">'sample_weight'</span>] <span class="op">=</span> prob_good_rejected  <span class="co"># Weight = P(default=0)</span></span>
<span id="cb3-849"><a href="#cb3-849"></a>        df_rejected_good[target_col] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-850"><a href="#cb3-850"></a>        df_rejected_good[<span class="st">'source'</span>] <span class="op">=</span> <span class="st">'Rejected'</span></span>
<span id="cb3-851"><a href="#cb3-851"></a>        </span>
<span id="cb3-852"><a href="#cb3-852"></a>        <span class="co"># 4. Prepare Accepted Data</span></span>
<span id="cb3-853"><a href="#cb3-853"></a>        <span class="co"># Select modeling features + target, assign weight = 1</span></span>
<span id="cb3-854"><a href="#cb3-854"></a>        df_accepted_weighted <span class="op">=</span> df_accepted_proc[modeling_features <span class="op">+</span> [target_col]].copy()</span>
<span id="cb3-855"><a href="#cb3-855"></a>        df_accepted_weighted[<span class="st">'sample_weight'</span>] <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb3-856"><a href="#cb3-856"></a>        df_accepted_weighted[<span class="st">'source'</span>] <span class="op">=</span> <span class="st">'Accepted'</span></span>
<span id="cb3-857"><a href="#cb3-857"></a>        </span>
<span id="cb3-858"><a href="#cb3-858"></a>        <span class="co"># 5. Define columns needed for the final TTD dataset</span></span>
<span id="cb3-859"><a href="#cb3-859"></a>        cols_for_ttd <span class="op">=</span> modeling_features <span class="op">+</span> [target_col, <span class="st">'sample_weight'</span>, <span class="st">'source'</span>]</span>
<span id="cb3-860"><a href="#cb3-860"></a>        </span>
<span id="cb3-861"><a href="#cb3-861"></a>        <span class="co"># 6. Concatenate accepted data and the weighted rejected datasets</span></span>
<span id="cb3-862"><a href="#cb3-862"></a>        <span class="cf">if</span> clone_rejected:</span>
<span id="cb3-863"><a href="#cb3-863"></a>            df_ttd <span class="op">=</span> pd.concat(</span>
<span id="cb3-864"><a href="#cb3-864"></a>                [df_accepted_weighted[cols_for_ttd],</span>
<span id="cb3-865"><a href="#cb3-865"></a>                    df_rejected_bad[cols_for_ttd],</span>
<span id="cb3-866"><a href="#cb3-866"></a>                    df_rejected_good[cols_for_ttd]],</span>
<span id="cb3-867"><a href="#cb3-867"></a>                ignore_index<span class="op">=</span><span class="va">True</span></span>
<span id="cb3-868"><a href="#cb3-868"></a>            )</span>
<span id="cb3-869"><a href="#cb3-869"></a>        <span class="cf">else</span>:</span>
<span id="cb3-870"><a href="#cb3-870"></a>            <span class="co"># If not cloning, just use the bad copy</span></span>
<span id="cb3-871"><a href="#cb3-871"></a>            df_ttd <span class="op">=</span> pd.concat(</span>
<span id="cb3-872"><a href="#cb3-872"></a>                [df_accepted_weighted[cols_for_ttd],</span>
<span id="cb3-873"><a href="#cb3-873"></a>                    df_rejected_bad[cols_for_ttd]],</span>
<span id="cb3-874"><a href="#cb3-874"></a>                ignore_index<span class="op">=</span><span class="va">True</span></span>
<span id="cb3-875"><a href="#cb3-875"></a>            )</span>
<span id="cb3-876"><a href="#cb3-876"></a></span>
<span id="cb3-877"><a href="#cb3-877"></a>    <span class="bu">print</span>(<span class="ss">f"TTD dataset created. Shape: </span><span class="sc">{</span>df_ttd<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-878"><a href="#cb3-878"></a>    <span class="bu">print</span>(<span class="ss">f"Sum of weights in TTD dataset: </span><span class="sc">{</span>df_ttd[<span class="st">'sample_weight'</span>]<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb3-879"><a href="#cb3-879"></a>    <span class="bu">print</span>(<span class="ss">f"Source distribution:</span><span class="ch">\n</span><span class="sc">{</span>df_ttd[<span class="st">'source'</span>]<span class="sc">.</span>value_counts(normalize<span class="op">=</span><span class="va">True</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-880"><a href="#cb3-880"></a></span>
<span id="cb3-881"><a href="#cb3-881"></a>    <span class="cf">return</span> df_ttd</span>
<span id="cb3-882"><a href="#cb3-882"></a></span>
<span id="cb3-883"><a href="#cb3-883"></a><span class="bu">print</span>(<span class="st">"Helper functions and classes defined."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Helper functions and classes defined.</code></pre>
</div>
</div>
</section>
</section>
<section id="data-loading-eda" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Data Loading &amp; EDA</h1>
<p><strong>Goal:</strong> Load the LendingClub accepted and rejected datasets, define the target variable (<code>default_flag</code>), split the data, and perform basic EDA.</p>
<section id="read-parquet-files" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="read-parquet-files"><span class="header-section-number">3.1</span> Read Parquet Files</h2>
<p>Load the datasets containing information on accepted loans and rejected applications.</p>
<div id="data-load" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="co"># Define file paths relative to the current script location</span></span>
<span id="cb5-2"><a href="#cb5-2"></a>accepted_path <span class="op">=</span> <span class="st">'../Data/lendingclub/accepted_2007_to_2018Q4.parquet'</span></span>
<span id="cb5-3"><a href="#cb5-3"></a>rejected_path <span class="op">=</span> <span class="st">'../Data/lendingclub/rejected_2007_to_2018Q4.parquet'</span></span>
<span id="cb5-4"><a href="#cb5-4"></a></span>
<span id="cb5-5"><a href="#cb5-5"></a><span class="co"># Load data using pandas</span></span>
<span id="cb5-6"><a href="#cb5-6"></a><span class="cf">try</span>:</span>
<span id="cb5-7"><a href="#cb5-7"></a>    df_accepted <span class="op">=</span> pd.read_parquet(accepted_path)</span>
<span id="cb5-8"><a href="#cb5-8"></a>    df_rejected <span class="op">=</span> pd.read_parquet(rejected_path)</span>
<span id="cb5-9"><a href="#cb5-9"></a></span>
<span id="cb5-10"><a href="#cb5-10"></a>    <span class="co"># Sample rejected data to reduce memory usage</span></span>
<span id="cb5-11"><a href="#cb5-11"></a>    max_rejected_rows <span class="op">=</span> <span class="dv">500_000</span></span>
<span id="cb5-12"><a href="#cb5-12"></a>    df_rejected <span class="op">=</span> df_rejected.sample(n<span class="op">=</span>max_rejected_rows, random_state<span class="op">=</span><span class="dv">2025</span>)</span>
<span id="cb5-13"><a href="#cb5-13"></a></span>
<span id="cb5-14"><a href="#cb5-14"></a>    <span class="bu">print</span>(<span class="st">"Data loaded successfully."</span>)</span>
<span id="cb5-15"><a href="#cb5-15"></a>    <span class="bu">print</span>(<span class="ss">f"Accepted data shape: </span><span class="sc">{</span>df_accepted<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-16"><a href="#cb5-16"></a>    <span class="bu">print</span>(<span class="ss">f"Rejected data shape: </span><span class="sc">{</span>df_rejected<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-17"><a href="#cb5-17"></a><span class="cf">except</span> <span class="pp">FileNotFoundError</span>:</span>
<span id="cb5-18"><a href="#cb5-18"></a>    <span class="bu">print</span>(<span class="st">"Error: Parquet files not found. Make sure the paths are correct and the data generation scripts have been run."</span>)</span>
<span id="cb5-19"><a href="#cb5-19"></a>    <span class="co"># Stop execution or handle error appropriately</span></span>
<span id="cb5-20"><a href="#cb5-20"></a>    df_accepted, df_rejected <span class="op">=</span> <span class="va">None</span>, <span class="va">None</span> <span class="co"># Set to None to avoid errors later</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Data loaded successfully.
Accepted data shape: (2260701, 151)
Rejected data shape: (500000, 9)</code></pre>
</div>
</div>
</section>
<section id="define-target-variable" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="define-target-variable"><span class="header-section-number">3.2</span> Define Target Variable</h2>
<p>Create the binary <code>default_flag</code> (1 for default, 0 for non-default) based on the <code>loan_status</code> in the accepted dataset. Rejected applicants do not have an observed <code>default_flag</code>.</p>
<div id="define-target" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="co"># Define default status based on 'loan_status'</span></span>
<span id="cb7-2"><a href="#cb7-2"></a>default_statuses <span class="op">=</span> [</span>
<span id="cb7-3"><a href="#cb7-3"></a>    <span class="st">"Charged Off"</span>, </span>
<span id="cb7-4"><a href="#cb7-4"></a>    <span class="st">"Late (31-120 days)"</span>, </span>
<span id="cb7-5"><a href="#cb7-5"></a>    <span class="st">"Does not meet the credit policy. Status:Charged Off"</span>, </span>
<span id="cb7-6"><a href="#cb7-6"></a>    <span class="st">"Default"</span></span>
<span id="cb7-7"><a href="#cb7-7"></a>] </span>
<span id="cb7-8"><a href="#cb7-8"></a>df_accepted[<span class="st">'default_flag'</span>] <span class="op">=</span> df_accepted[<span class="st">'loan_status'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x <span class="kw">in</span> default_statuses <span class="cf">else</span> <span class="dv">0</span>)</span>
<span id="cb7-9"><a href="#cb7-9"></a></span>
<span id="cb7-10"><a href="#cb7-10"></a><span class="bu">print</span>(<span class="st">"Target variable 'default_flag' created."</span>)</span>
<span id="cb7-11"><a href="#cb7-11"></a><span class="bu">print</span>(<span class="st">"Default Flag Distribution (Accepted):"</span>)</span>
<span id="cb7-12"><a href="#cb7-12"></a><span class="bu">print</span>(df_accepted[<span class="st">'default_flag'</span>].value_counts(normalize<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb7-13"><a href="#cb7-13"></a></span>
<span id="cb7-14"><a href="#cb7-14"></a><span class="co"># Display loan status counts for context</span></span>
<span id="cb7-15"><a href="#cb7-15"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Loan Status Distribution (Accepted):"</span>)</span>
<span id="cb7-16"><a href="#cb7-16"></a><span class="bu">print</span>(df_accepted[<span class="st">'loan_status'</span>].value_counts())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Target variable 'default_flag' created.
Default Flag Distribution (Accepted):
default_flag
0    0.871355
1    0.128645
Name: proportion, dtype: float64

Loan Status Distribution (Accepted):
loan_status
Fully Paid                                             1076751
Current                                                 878317
Charged Off                                             268559
Late (31-120 days)                                       21467
In Grace Period                                           8436
Late (16-30 days)                                         4349
Does not meet the credit policy. Status:Fully Paid        1988
Does not meet the credit policy. Status:Charged Off        761
Default                                                     40
Name: count, dtype: int64</code></pre>
</div>
</div>
</section>
<section id="train-calibration-and-test-split" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="train-calibration-and-test-split"><span class="header-section-number">3.3</span> Train, Calibration, and Test Split</h2>
<p>Split both accepted and rejected datasets into training (60%), calibration (20%), and test (20%) sets. We use stratification for the accepted data based on the <code>default_flag</code> to maintain similar default rates across splits.</p>
<div id="cell-train-test-split" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="co"># Define split proportions</span></span>
<span id="cb9-2"><a href="#cb9-2"></a>train_size <span class="op">=</span> <span class="fl">0.6</span></span>
<span id="cb9-3"><a href="#cb9-3"></a>calib_size_rel_to_remaining <span class="op">=</span> <span class="fl">0.5</span> <span class="co"># 0.5 * (1 - 0.6) = 0.2 -&gt; test_size for split_data</span></span>
<span id="cb9-4"><a href="#cb9-4"></a>random_seed <span class="op">=</span> <span class="dv">2025</span></span>
<span id="cb9-5"><a href="#cb9-5"></a>global_set_seed(random_seed) <span class="co"># Set global seed</span></span>
<span id="cb9-6"><a href="#cb9-6"></a></span>
<span id="cb9-7"><a href="#cb9-7"></a><span class="co"># --- Split Accepted Data (Stratified) ---</span></span>
<span id="cb9-8"><a href="#cb9-8"></a><span class="bu">print</span>(<span class="st">"Splitting Accepted Data..."</span>)</span>
<span id="cb9-9"><a href="#cb9-9"></a>df_accepted_train, df_accepted_calib, df_accepted_test <span class="op">=</span> split_data(</span>
<span id="cb9-10"><a href="#cb9-10"></a>    df<span class="op">=</span>df_accepted,</span>
<span id="cb9-11"><a href="#cb9-11"></a>    target_col<span class="op">=</span><span class="st">'default_flag'</span>,</span>
<span id="cb9-12"><a href="#cb9-12"></a>    train_size<span class="op">=</span>train_size,</span>
<span id="cb9-13"><a href="#cb9-13"></a>    calib_size_rel<span class="op">=</span>calib_size_rel_to_remaining, <span class="co"># This is test_size in split_data context</span></span>
<span id="cb9-14"><a href="#cb9-14"></a>    random_state<span class="op">=</span>random_seed</span>
<span id="cb9-15"><a href="#cb9-15"></a>)</span>
<span id="cb9-16"><a href="#cb9-16"></a></span>
<span id="cb9-17"><a href="#cb9-17"></a><span class="co"># --- Split Rejected Data (Not Stratified) ---</span></span>
<span id="cb9-18"><a href="#cb9-18"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Splitting Rejected Data..."</span>)</span>
<span id="cb9-19"><a href="#cb9-19"></a>df_rejected_train, df_rejected_calib, df_rejected_test <span class="op">=</span> split_data(</span>
<span id="cb9-20"><a href="#cb9-20"></a>    df<span class="op">=</span>df_rejected,</span>
<span id="cb9-21"><a href="#cb9-21"></a>    target_col<span class="op">=</span><span class="va">None</span>, <span class="co"># No stratification</span></span>
<span id="cb9-22"><a href="#cb9-22"></a>    train_size<span class="op">=</span>train_size,</span>
<span id="cb9-23"><a href="#cb9-23"></a>    calib_size_rel<span class="op">=</span>calib_size_rel_to_remaining, <span class="co"># This is test_size in split_data context</span></span>
<span id="cb9-24"><a href="#cb9-24"></a>    random_state<span class="op">=</span>random_seed</span>
<span id="cb9-25"><a href="#cb9-25"></a>)</span>
<span id="cb9-26"><a href="#cb9-26"></a></span>
<span id="cb9-27"><a href="#cb9-27"></a><span class="co"># Clean up the original large dataframes</span></span>
<span id="cb9-28"><a href="#cb9-28"></a><span class="kw">del</span> df_accepted</span>
<span id="cb9-29"><a href="#cb9-29"></a><span class="kw">del</span> df_rejected</span>
<span id="cb9-30"><a href="#cb9-30"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Removed original df_accepted and df_rejected to free up memory."</span>)</span>
<span id="cb9-31"><a href="#cb9-31"></a>gc.collect() <span class="co"># Call garbage collector</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Splitting Accepted Data...
Splitting data (stratify=Yes)...
  Train shape: (1356420, 152)
  Calibration shape: (452140, 152)
  Test shape: (452141, 152)
  Train target mean: 0.1286
  Calibration target mean: 0.1286
  Test target mean: 0.1286

Splitting Rejected Data...
Splitting data (stratify=No)...
  Train shape: (300000, 9)
  Calibration shape: (100000, 9)
  Test shape: (100000, 9)

Removed original df_accepted and df_rejected to free up memory.</code></pre>
</div>
<div id="train-test-split" class="cell-output cell-output-display" data-execution_count="5">
<pre><code>0</code></pre>
</div>
</div>
</section>
<section id="exploratory-data-analysis-eda" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="exploratory-data-analysis-eda"><span class="header-section-number">3.4</span> Exploratory Data Analysis (EDA)</h2>
<p>Perform initial EDA on the <em>training</em> portions of the accepted and rejected datasets to understand distributions, missing values, and potential issues. We use <code>ydata-profiling</code> for automated report generation.</p>
<div id="eda-accepted-rejected" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="bu">print</span>(<span class="st">"Generating EDA reports (this might take a while)..."</span>)</span>
<span id="cb12-2"><a href="#cb12-2"></a></span>
<span id="cb12-3"><a href="#cb12-3"></a><span class="co"># Define sampling fraction for large datasets</span></span>
<span id="cb12-4"><a href="#cb12-4"></a>p_frac <span class="op">=</span> <span class="fl">0.05</span> <span class="co"># Sample 5% for faster report generation</span></span>
<span id="cb12-5"><a href="#cb12-5"></a></span>
<span id="cb12-6"><a href="#cb12-6"></a><span class="co"># --- EDA for Accepted Training Data ---</span></span>
<span id="cb12-7"><a href="#cb12-7"></a>generate_eda_report(</span>
<span id="cb12-8"><a href="#cb12-8"></a>    df<span class="op">=</span>df_accepted_train,</span>
<span id="cb12-9"><a href="#cb12-9"></a>    title<span class="op">=</span><span class="st">"Accepted Train Data Profile"</span>,</span>
<span id="cb12-10"><a href="#cb12-10"></a>    output_path<span class="op">=</span><span class="st">"Lab02_eda_report_accepted_train.html"</span>,</span>
<span id="cb12-11"><a href="#cb12-11"></a>    sample_frac<span class="op">=</span>p_frac,</span>
<span id="cb12-12"><a href="#cb12-12"></a>    random_state<span class="op">=</span>random_seed</span>
<span id="cb12-13"><a href="#cb12-13"></a>)</span>
<span id="cb12-14"><a href="#cb12-14"></a></span>
<span id="cb12-15"><a href="#cb12-15"></a><span class="co"># --- EDA for Rejected Training Data ---</span></span>
<span id="cb12-16"><a href="#cb12-16"></a>generate_eda_report(</span>
<span id="cb12-17"><a href="#cb12-17"></a>    df<span class="op">=</span>df_rejected_train,</span>
<span id="cb12-18"><a href="#cb12-18"></a>    title<span class="op">=</span><span class="st">"Rejected Train Data Profile"</span>,</span>
<span id="cb12-19"><a href="#cb12-19"></a>    output_path<span class="op">=</span><span class="st">"Lab02_eda_report_rejected_train.html"</span>,</span>
<span id="cb12-20"><a href="#cb12-20"></a>    sample_frac<span class="op">=</span>p_frac,</span>
<span id="cb12-21"><a href="#cb12-21"></a>    random_state<span class="op">=</span>random_seed</span>
<span id="cb12-22"><a href="#cb12-22"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Generating EDA reports (this might take a while)...
Generating EDA report: Accepted Train Data Profile...</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Report saved to: Lab02_eda_report_accepted_train.html
Generating EDA report: Rejected Train Data Profile...
Report saved to: Lab02_eda_report_rejected_train.html</code></pre>
</div>
</div>
</section>
</section>
<section id="monotonicity-checks" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Monotonicity Checks</h1>
<p><strong>Goal:</strong> Process the raw data to create a set of common features present in both accepted and rejected datasets. Then, analyze the relationship between these features and the default outcome (using accepted data) to check for expected monotonic trends.</p>
<section id="identify-and-process-common-features" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="identify-and-process-common-features"><span class="header-section-number">4.1</span> Identify and Process Common Features</h2>
<p>Select relevant features, rename columns for consistency, handle missing values, and perform necessary transformations (e.g., parsing employment length, calculating FICO score).</p>
<div id="common-features" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a><span class="co"># Define common feature names globally</span></span>
<span id="cb15-2"><a href="#cb15-2"></a>common_feature_names <span class="op">=</span> [<span class="st">'loan_amnt'</span>, <span class="st">'emp_length'</span>, <span class="st">'addr_state'</span>, <span class="st">'dti'</span>, <span class="st">'credit_score'</span>]</span>
<span id="cb15-3"><a href="#cb15-3"></a>target_col <span class="op">=</span> <span class="st">'default_flag'</span></span>
<span id="cb15-4"><a href="#cb15-4"></a></span>
<span id="cb15-5"><a href="#cb15-5"></a><span class="co"># Remove non-numeric chars from dti in df_rejected_train</span></span>
<span id="cb15-6"><a href="#cb15-6"></a>df_rejected_train[<span class="st">'Debt-To-Income Ratio'</span>] <span class="op">=</span> df_rejected_train[<span class="st">'Debt-To-Income Ratio'</span>].astype(<span class="bu">str</span>).<span class="bu">str</span>.replace(<span class="vs">r'[^0-9\.\-]'</span>, <span class="st">''</span>, regex<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-7"><a href="#cb15-7"></a>df_rejected_train[<span class="st">'Debt-To-Income Ratio'</span>] <span class="op">=</span> pd.to_numeric(df_rejected_train[<span class="st">'Debt-To-Income Ratio'</span>], errors<span class="op">=</span><span class="st">'coerce'</span>)</span>
<span id="cb15-8"><a href="#cb15-8"></a></span>
<span id="cb15-9"><a href="#cb15-9"></a>rejected_train_median_loan_amnt <span class="op">=</span> df_rejected_train[<span class="st">'Amount Requested'</span>].median(skipna<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-10"><a href="#cb15-10"></a>rejected_train_median_emp_length <span class="op">=</span> df_rejected_train[<span class="st">'Employment Length'</span>].<span class="bu">apply</span>(parse_emp_length).median(skipna<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-11"><a href="#cb15-11"></a>rejected_train_p90_dti <span class="op">=</span> df_rejected_train[<span class="st">'Debt-To-Income Ratio'</span>].quantile(<span class="fl">0.90</span>, interpolation<span class="op">=</span><span class="st">'nearest'</span>)</span>
<span id="cb15-12"><a href="#cb15-12"></a>rejected_train_p10_credit_score <span class="op">=</span> df_rejected_train[<span class="st">'Risk_Score'</span>].quantile(<span class="fl">0.10</span>, interpolation<span class="op">=</span><span class="st">'nearest'</span>)</span>
<span id="cb15-13"><a href="#cb15-13"></a></span>
<span id="cb15-14"><a href="#cb15-14"></a><span class="kw">def</span> process_lending_data(df: pd.DataFrame,</span>
<span id="cb15-15"><a href="#cb15-15"></a>                        source_type: <span class="bu">str</span>,</span>
<span id="cb15-16"><a href="#cb15-16"></a>                        fillMissing_loanamnt: <span class="bu">int</span>,</span>
<span id="cb15-17"><a href="#cb15-17"></a>                        fillMissing_emp_length: <span class="bu">int</span>, </span>
<span id="cb15-18"><a href="#cb15-18"></a>                        fillMissing_dti: <span class="bu">float</span>,</span>
<span id="cb15-19"><a href="#cb15-19"></a>                        fillMissing_credit_score: <span class="bu">int</span>) <span class="op">-&gt;</span> pd.DataFrame <span class="op">|</span> <span class="va">None</span>:</span>
<span id="cb15-20"><a href="#cb15-20"></a>    <span class="co">"""</span></span>
<span id="cb15-21"><a href="#cb15-21"></a><span class="co">    Processes LendingClub data (accepted or rejected) to extract common features.</span></span>
<span id="cb15-22"><a href="#cb15-22"></a><span class="co">    </span></span>
<span id="cb15-23"><a href="#cb15-23"></a><span class="co">    This function standardizes column names, handles missing values, transforms</span></span>
<span id="cb15-24"><a href="#cb15-24"></a><span class="co">    data types, and applies validations to ensure consistent data formats across</span></span>
<span id="cb15-25"><a href="#cb15-25"></a><span class="co">    accepted and rejected loan applications.</span></span>
<span id="cb15-26"><a href="#cb15-26"></a></span>
<span id="cb15-27"><a href="#cb15-27"></a><span class="co">    Args:</span></span>
<span id="cb15-28"><a href="#cb15-28"></a><span class="co">        df (pd.DataFrame): Input DataFrame (either accepted or rejected).</span></span>
<span id="cb15-29"><a href="#cb15-29"></a><span class="co">        source_type (str): 'accepted' or 'rejected'.</span></span>
<span id="cb15-30"><a href="#cb15-30"></a><span class="co">        fillMissing_loanamnt (int): Value to fill missing loan amount entries.</span></span>
<span id="cb15-31"><a href="#cb15-31"></a><span class="co">        fillMissing_emp_length (int): Value to fill missing employment length entries.</span></span>
<span id="cb15-32"><a href="#cb15-32"></a><span class="co">        fillMissing_dti (float): Value to fill missing debt-to-income ratio entries.</span></span>
<span id="cb15-33"><a href="#cb15-33"></a><span class="co">        fillMissing_credit_score (int): Value to fill missing credit score entries.</span></span>
<span id="cb15-34"><a href="#cb15-34"></a></span>
<span id="cb15-35"><a href="#cb15-35"></a><span class="co">    Returns:</span></span>
<span id="cb15-36"><a href="#cb15-36"></a><span class="co">        pd.DataFrame | None: Processed DataFrame with common features (and target for accepted),</span></span>
<span id="cb15-37"><a href="#cb15-37"></a><span class="co">                             or None if input df is None.</span></span>
<span id="cb15-38"><a href="#cb15-38"></a><span class="co">                             </span></span>
<span id="cb15-39"><a href="#cb15-39"></a><span class="co">    Raises:</span></span>
<span id="cb15-40"><a href="#cb15-40"></a><span class="co">        ValueError: If source_type is not 'accepted' or 'rejected'.</span></span>
<span id="cb15-41"><a href="#cb15-41"></a><span class="co">    """</span></span>
<span id="cb15-42"><a href="#cb15-42"></a>    <span class="cf">if</span> df <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb15-43"><a href="#cb15-43"></a>        <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb15-44"><a href="#cb15-44"></a></span>
<span id="cb15-45"><a href="#cb15-45"></a>    df_proc <span class="op">=</span> df.copy()</span>
<span id="cb15-46"><a href="#cb15-46"></a></span>
<span id="cb15-47"><a href="#cb15-47"></a>    <span class="co"># Column mapping and credit_score logic</span></span>
<span id="cb15-48"><a href="#cb15-48"></a>    <span class="cf">if</span> source_type <span class="op">==</span> <span class="st">'accepted'</span>:</span>
<span id="cb15-49"><a href="#cb15-49"></a>        rename_map <span class="op">=</span> {</span>
<span id="cb15-50"><a href="#cb15-50"></a>            <span class="st">'loan_amnt'</span>: <span class="st">'loan_amnt'</span>,</span>
<span id="cb15-51"><a href="#cb15-51"></a>            <span class="st">'emp_length'</span>: <span class="st">'emp_length'</span>,</span>
<span id="cb15-52"><a href="#cb15-52"></a>            <span class="st">'addr_state'</span>: <span class="st">'addr_state'</span>,</span>
<span id="cb15-53"><a href="#cb15-53"></a>            <span class="st">'dti'</span>: <span class="st">'dti'</span>,</span>
<span id="cb15-54"><a href="#cb15-54"></a>            <span class="st">'fico_range_low'</span>: <span class="st">'fico_range_low'</span>,</span>
<span id="cb15-55"><a href="#cb15-55"></a>            <span class="st">'fico_range_high'</span>: <span class="st">'fico_range_high'</span></span>
<span id="cb15-56"><a href="#cb15-56"></a>        }</span>
<span id="cb15-57"><a href="#cb15-57"></a>    <span class="cf">elif</span> source_type <span class="op">==</span> <span class="st">'rejected'</span>:</span>
<span id="cb15-58"><a href="#cb15-58"></a>        rename_map <span class="op">=</span> {</span>
<span id="cb15-59"><a href="#cb15-59"></a>            <span class="st">'Amount Requested'</span>: <span class="st">'loan_amnt'</span>,</span>
<span id="cb15-60"><a href="#cb15-60"></a>            <span class="st">'Employment Length'</span>: <span class="st">'emp_length'</span>,</span>
<span id="cb15-61"><a href="#cb15-61"></a>            <span class="st">'State'</span>: <span class="st">'addr_state'</span>,</span>
<span id="cb15-62"><a href="#cb15-62"></a>            <span class="st">'Debt-To-Income Ratio'</span>: <span class="st">'dti'</span>,</span>
<span id="cb15-63"><a href="#cb15-63"></a>            <span class="st">'Risk_Score'</span>: <span class="st">'credit_score'</span></span>
<span id="cb15-64"><a href="#cb15-64"></a>        }</span>
<span id="cb15-65"><a href="#cb15-65"></a>    <span class="cf">else</span>:</span>
<span id="cb15-66"><a href="#cb15-66"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"source_type must be 'accepted' or 'rejected'"</span>)</span>
<span id="cb15-67"><a href="#cb15-67"></a></span>
<span id="cb15-68"><a href="#cb15-68"></a>    <span class="co"># Warn if missing columns</span></span>
<span id="cb15-69"><a href="#cb15-69"></a>    missing_cols <span class="op">=</span> [col <span class="cf">for</span> col <span class="kw">in</span> rename_map.keys() <span class="cf">if</span> col <span class="kw">not</span> <span class="kw">in</span> df_proc.columns]</span>
<span id="cb15-70"><a href="#cb15-70"></a>    <span class="cf">if</span> missing_cols:</span>
<span id="cb15-71"><a href="#cb15-71"></a>        <span class="bu">print</span>(<span class="ss">f"Warning: Missing columns in </span><span class="sc">{</span>source_type<span class="sc">}</span><span class="ss"> data: </span><span class="sc">{</span>missing_cols<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-72"><a href="#cb15-72"></a>    df_proc <span class="op">=</span> df_proc.rename(columns<span class="op">=</span>rename_map)</span>
<span id="cb15-73"><a href="#cb15-73"></a></span>
<span id="cb15-74"><a href="#cb15-74"></a>    <span class="co"># loan_amnt</span></span>
<span id="cb15-75"><a href="#cb15-75"></a>    df_proc[<span class="st">'loan_amnt'</span>] <span class="op">=</span> pd.to_numeric(df_proc.get(<span class="st">'loan_amnt'</span>), errors<span class="op">=</span><span class="st">'coerce'</span>).fillna(fillMissing_loanamnt).clip(lower<span class="op">=</span><span class="dv">500</span>)</span>
<span id="cb15-76"><a href="#cb15-76"></a></span>
<span id="cb15-77"><a href="#cb15-77"></a>    <span class="co"># emp_length</span></span>
<span id="cb15-78"><a href="#cb15-78"></a>    df_proc[<span class="st">'emp_length'</span>] <span class="op">=</span> df_proc.get(<span class="st">'emp_length'</span>).<span class="bu">apply</span>(parse_emp_length).fillna(fillMissing_emp_length) </span>
<span id="cb15-79"><a href="#cb15-79"></a></span>
<span id="cb15-80"><a href="#cb15-80"></a>    <span class="co"># dti</span></span>
<span id="cb15-81"><a href="#cb15-81"></a>    dti_series <span class="op">=</span> df_proc.get(<span class="st">'dti'</span>).astype(<span class="bu">str</span>).<span class="bu">str</span>.replace(<span class="vs">r'[^0-9\.\-]'</span>, <span class="st">''</span>, regex<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-82"><a href="#cb15-82"></a>    df_proc[<span class="st">'dti'</span>] <span class="op">=</span> pd.to_numeric(dti_series, errors<span class="op">=</span><span class="st">'coerce'</span>).fillna(fillMissing_dti).astype(<span class="st">"float32"</span>).clip(lower<span class="op">=</span><span class="dv">0</span>, upper<span class="op">=</span><span class="dv">300</span>)</span>
<span id="cb15-83"><a href="#cb15-83"></a></span>
<span id="cb15-84"><a href="#cb15-84"></a>    <span class="co"># credit_score</span></span>
<span id="cb15-85"><a href="#cb15-85"></a>    <span class="cf">if</span> source_type <span class="op">==</span> <span class="st">'accepted'</span>:</span>
<span id="cb15-86"><a href="#cb15-86"></a>        <span class="co"># Use FICO if available, else fallback</span></span>
<span id="cb15-87"><a href="#cb15-87"></a>        <span class="cf">if</span> <span class="st">'fico_range_low'</span> <span class="kw">in</span> df_proc.columns <span class="kw">and</span> <span class="st">'fico_range_high'</span> <span class="kw">in</span> df_proc.columns:</span>
<span id="cb15-88"><a href="#cb15-88"></a>            df_proc[<span class="st">'credit_score'</span>] <span class="op">=</span> (df_proc[<span class="st">'fico_range_low'</span>] <span class="op">+</span> df_proc[<span class="st">'fico_range_high'</span>]) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb15-89"><a href="#cb15-89"></a>        <span class="cf">else</span>:</span>
<span id="cb15-90"><a href="#cb15-90"></a>            df_proc[<span class="st">'credit_score'</span>] <span class="op">=</span> np.nan</span>
<span id="cb15-91"><a href="#cb15-91"></a>        df_proc[<span class="st">'credit_score'</span>] <span class="op">=</span> df_proc[<span class="st">'credit_score'</span>].fillna(fillMissing_credit_score).clip(lower<span class="op">=</span><span class="dv">300</span>, upper<span class="op">=</span><span class="dv">850</span>)</span>
<span id="cb15-92"><a href="#cb15-92"></a>    <span class="cf">else</span>:</span>
<span id="cb15-93"><a href="#cb15-93"></a>        df_proc[<span class="st">'credit_score'</span>] <span class="op">=</span> pd.to_numeric(df_proc.get(<span class="st">'credit_score'</span>), errors<span class="op">=</span><span class="st">'coerce'</span>).fillna(fillMissing_credit_score).clip(lower<span class="op">=</span><span class="dv">300</span>, upper<span class="op">=</span><span class="dv">850</span>)</span>
<span id="cb15-94"><a href="#cb15-94"></a></span>
<span id="cb15-95"><a href="#cb15-95"></a>    <span class="co"># Target column for accepted</span></span>
<span id="cb15-96"><a href="#cb15-96"></a>    <span class="cf">if</span> source_type <span class="op">==</span> <span class="st">'accepted'</span>:</span>
<span id="cb15-97"><a href="#cb15-97"></a>        <span class="cf">if</span> target_col <span class="kw">not</span> <span class="kw">in</span> df_proc.columns:</span>
<span id="cb15-98"><a href="#cb15-98"></a>            <span class="bu">print</span>(<span class="ss">f"Warning: Target column '</span><span class="sc">{</span>target_col<span class="sc">}</span><span class="ss">' not found in accepted data. Adding placeholder."</span>)</span>
<span id="cb15-99"><a href="#cb15-99"></a>            df_proc[target_col] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb15-100"><a href="#cb15-100"></a>        cols_to_keep <span class="op">=</span> [col <span class="cf">for</span> col <span class="kw">in</span> common_feature_names <span class="cf">if</span> col <span class="kw">in</span> df_proc.columns] <span class="op">+</span> [target_col]</span>
<span id="cb15-101"><a href="#cb15-101"></a>    <span class="cf">else</span>:</span>
<span id="cb15-102"><a href="#cb15-102"></a>        cols_to_keep <span class="op">=</span> [col <span class="cf">for</span> col <span class="kw">in</span> common_feature_names <span class="cf">if</span> col <span class="kw">in</span> df_proc.columns]</span>
<span id="cb15-103"><a href="#cb15-103"></a></span>
<span id="cb15-104"><a href="#cb15-104"></a>    <span class="cf">return</span> df_proc[cols_to_keep].copy()</span>
<span id="cb15-105"><a href="#cb15-105"></a></span>
<span id="cb15-106"><a href="#cb15-106"></a><span class="co"># --- Apply the function to the training data ---</span></span>
<span id="cb15-107"><a href="#cb15-107"></a><span class="bu">print</span>(<span class="st">"Processing training data using the defined function..."</span>)</span>
<span id="cb15-108"><a href="#cb15-108"></a></span>
<span id="cb15-109"><a href="#cb15-109"></a>df_accepted_train_common <span class="op">=</span> process_lending_data(</span>
<span id="cb15-110"><a href="#cb15-110"></a>    df<span class="op">=</span>df_accepted_train, </span>
<span id="cb15-111"><a href="#cb15-111"></a>    source_type<span class="op">=</span><span class="st">'accepted'</span>,</span>
<span id="cb15-112"><a href="#cb15-112"></a>    fillMissing_loanamnt<span class="op">=</span>rejected_train_median_loan_amnt,</span>
<span id="cb15-113"><a href="#cb15-113"></a>    fillMissing_emp_length<span class="op">=</span>rejected_train_median_emp_length, </span>
<span id="cb15-114"><a href="#cb15-114"></a>    fillMissing_dti<span class="op">=</span>rejected_train_p90_dti,</span>
<span id="cb15-115"><a href="#cb15-115"></a>    fillMissing_credit_score<span class="op">=</span>rejected_train_p10_credit_score</span>
<span id="cb15-116"><a href="#cb15-116"></a>)</span>
<span id="cb15-117"><a href="#cb15-117"></a></span>
<span id="cb15-118"><a href="#cb15-118"></a>df_rejected_train_common <span class="op">=</span> process_lending_data(</span>
<span id="cb15-119"><a href="#cb15-119"></a>    df<span class="op">=</span>df_rejected_train, </span>
<span id="cb15-120"><a href="#cb15-120"></a>    source_type<span class="op">=</span><span class="st">'rejected'</span>,</span>
<span id="cb15-121"><a href="#cb15-121"></a>    fillMissing_loanamnt<span class="op">=</span>rejected_train_median_loan_amnt,</span>
<span id="cb15-122"><a href="#cb15-122"></a>    fillMissing_emp_length<span class="op">=</span>rejected_train_median_emp_length, </span>
<span id="cb15-123"><a href="#cb15-123"></a>    fillMissing_dti<span class="op">=</span>rejected_train_p90_dti,</span>
<span id="cb15-124"><a href="#cb15-124"></a>    fillMissing_credit_score<span class="op">=</span>rejected_train_p10_credit_score</span>
<span id="cb15-125"><a href="#cb15-125"></a>)</span>
<span id="cb15-126"><a href="#cb15-126"></a></span>
<span id="cb15-127"><a href="#cb15-127"></a><span class="bu">print</span>(<span class="ss">f"Accepted training data processed. Shape: </span><span class="sc">{</span>df_accepted_train_common<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-128"><a href="#cb15-128"></a><span class="co"># Verify common features are present</span></span>
<span id="cb15-129"><a href="#cb15-129"></a>missing_acc <span class="op">=</span> [f <span class="cf">for</span> f <span class="kw">in</span> common_feature_names <span class="cf">if</span> f <span class="kw">not</span> <span class="kw">in</span> df_accepted_train_common.columns]</span>
<span id="cb15-130"><a href="#cb15-130"></a><span class="cf">if</span> missing_acc: <span class="bu">print</span>(<span class="ss">f"  Warning: Missing common features after processing accepted: </span><span class="sc">{</span>missing_acc<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-131"><a href="#cb15-131"></a></span>
<span id="cb15-132"><a href="#cb15-132"></a></span>
<span id="cb15-133"><a href="#cb15-133"></a><span class="bu">print</span>(<span class="ss">f"Rejected training data processed. Shape: </span><span class="sc">{</span>df_rejected_train_common<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-134"><a href="#cb15-134"></a><span class="co"># Verify common features are present</span></span>
<span id="cb15-135"><a href="#cb15-135"></a>missing_rej <span class="op">=</span> [f <span class="cf">for</span> f <span class="kw">in</span> common_feature_names <span class="cf">if</span> f <span class="kw">not</span> <span class="kw">in</span> df_rejected_train_common.columns]</span>
<span id="cb15-136"><a href="#cb15-136"></a><span class="cf">if</span> missing_rej: <span class="bu">print</span>(<span class="ss">f"  Warning: Missing common features after processing rejected: </span><span class="sc">{</span>missing_rej<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-137"><a href="#cb15-137"></a></span>
<span id="cb15-138"><a href="#cb15-138"></a></span>
<span id="cb15-139"><a href="#cb15-139"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Common features expected:"</span>)</span>
<span id="cb15-140"><a href="#cb15-140"></a><span class="bu">print</span>(common_feature_names)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Processing training data using the defined function...
Accepted training data processed. Shape: (1356420, 6)
Rejected training data processed. Shape: (300000, 5)

Common features expected:
['loan_amnt', 'emp_length', 'addr_state', 'dti', 'credit_score']</code></pre>
</div>
</div>
<p>Check dtypes</p>
<div class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a><span class="co"># Convert .info() output to DataFrame for accepted training data</span></span>
<span id="cb17-2"><a href="#cb17-2"></a>accepted_info <span class="op">=</span> pd.DataFrame({</span>
<span id="cb17-3"><a href="#cb17-3"></a>    <span class="st">"column"</span>: df_accepted_train_common.columns,</span>
<span id="cb17-4"><a href="#cb17-4"></a>    <span class="st">"dtype"</span>: [df_accepted_train_common[col].dtype <span class="cf">for</span> col <span class="kw">in</span> df_accepted_train_common.columns],</span>
<span id="cb17-5"><a href="#cb17-5"></a>    <span class="st">"null_count"</span>: [df_accepted_train_common[col].isnull().<span class="bu">sum</span>() <span class="cf">for</span> col <span class="kw">in</span> df_accepted_train_common.columns],</span>
<span id="cb17-6"><a href="#cb17-6"></a>    <span class="st">"non_null_count"</span>: [df_accepted_train_common[col].notnull().<span class="bu">sum</span>() <span class="cf">for</span> col <span class="kw">in</span> df_accepted_train_common.columns],</span>
<span id="cb17-7"><a href="#cb17-7"></a>    <span class="st">"unique_count"</span>: [df_accepted_train_common[col].nunique() <span class="cf">for</span> col <span class="kw">in</span> df_accepted_train_common.columns]</span>
<span id="cb17-8"><a href="#cb17-8"></a>})</span>
<span id="cb17-9"><a href="#cb17-9"></a>display(accepted_info)</span>
<span id="cb17-10"><a href="#cb17-10"></a></span>
<span id="cb17-11"><a href="#cb17-11"></a><span class="co"># Convert .info() output to DataFrame for rejected training data</span></span>
<span id="cb17-12"><a href="#cb17-12"></a>rejected_info <span class="op">=</span> pd.DataFrame({</span>
<span id="cb17-13"><a href="#cb17-13"></a>    <span class="st">"column"</span>: df_rejected_train_common.columns,</span>
<span id="cb17-14"><a href="#cb17-14"></a>    <span class="st">"dtype"</span>: [df_rejected_train_common[col].dtype <span class="cf">for</span> col <span class="kw">in</span> df_rejected_train_common.columns],</span>
<span id="cb17-15"><a href="#cb17-15"></a>    <span class="st">"null_count"</span>: [df_rejected_train_common[col].isnull().<span class="bu">sum</span>() <span class="cf">for</span> col <span class="kw">in</span> df_rejected_train_common.columns],</span>
<span id="cb17-16"><a href="#cb17-16"></a>    <span class="st">"non_null_count"</span>: [df_rejected_train_common[col].notnull().<span class="bu">sum</span>() <span class="cf">for</span> col <span class="kw">in</span> df_rejected_train_common.columns],</span>
<span id="cb17-17"><a href="#cb17-17"></a>    <span class="st">"unique_count"</span>: [df_rejected_train_common[col].nunique() <span class="cf">for</span> col <span class="kw">in</span> df_rejected_train_common.columns]</span>
<span id="cb17-18"><a href="#cb17-18"></a>})</span>
<span id="cb17-19"><a href="#cb17-19"></a></span>
<span id="cb17-20"><a href="#cb17-20"></a>display(rejected_info)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-train-dtypes" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="8">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-train-dtypes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Data types in each data frame
</figcaption>
<div aria-describedby="tbl-train-dtypes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<div>


<table class="dataframe do-not-create-environment cell caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">column</th>
<th data-quarto-table-cell-role="th">dtype</th>
<th data-quarto-table-cell-role="th">null_count</th>
<th data-quarto-table-cell-role="th">non_null_count</th>
<th data-quarto-table-cell-role="th">unique_count</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>loan_amnt</td>
<td>float64</td>
<td>0</td>
<td>1356420</td>
<td>1567</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>emp_length</td>
<td>int64</td>
<td>0</td>
<td>1356420</td>
<td>11</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>addr_state</td>
<td>object</td>
<td>17</td>
<td>1356403</td>
<td>51</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>dti</td>
<td>float32</td>
<td>0</td>
<td>1356420</td>
<td>9131</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>credit_score</td>
<td>float64</td>
<td>0</td>
<td>1356420</td>
<td>48</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>default_flag</td>
<td>int64</td>
<td>0</td>
<td>1356420</td>
<td>2</td>
</tr>
</tbody>
</table>

</div>
</div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe do-not-create-environment cell caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">column</th>
<th data-quarto-table-cell-role="th">dtype</th>
<th data-quarto-table-cell-role="th">null_count</th>
<th data-quarto-table-cell-role="th">non_null_count</th>
<th data-quarto-table-cell-role="th">unique_count</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>loan_amnt</td>
<td>float64</td>
<td>0</td>
<td>300000</td>
<td>1571</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>emp_length</td>
<td>int64</td>
<td>0</td>
<td>300000</td>
<td>11</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>addr_state</td>
<td>object</td>
<td>0</td>
<td>300000</td>
<td>51</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>dti</td>
<td>float32</td>
<td>0</td>
<td>300000</td>
<td>13079</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>credit_score</td>
<td>float64</td>
<td>0</td>
<td>300000</td>
<td>461</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</figure>
</div>
</div>
</section>
<section id="compare-data-distributions" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="compare-data-distributions"><span class="header-section-number">4.2</span> Compare Data Distributions</h2>
<p>Compare data distributions between <code>df_accepted_train_common</code> and <code>df_rejected_train_common</code> using <code>ProfileReport</code>.</p>
<div id="eda-compare-common" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a><span class="co"># Check if dataframes and ProfileReport exist</span></span>
<span id="cb18-2"><a href="#cb18-2"></a><span class="cf">if</span> <span class="st">'df_accepted_train_common'</span> <span class="kw">in</span> <span class="bu">locals</span>() <span class="kw">and</span> df_accepted_train_common <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> <span class="op">\</span></span>
<span id="cb18-3"><a href="#cb18-3"></a>    <span class="st">'df_rejected_train_common'</span> <span class="kw">in</span> <span class="bu">locals</span>() <span class="kw">and</span> df_rejected_train_common <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> <span class="op">\</span></span>
<span id="cb18-4"><a href="#cb18-4"></a>    <span class="st">'ProfileReport'</span> <span class="kw">in</span> <span class="bu">locals</span>() <span class="kw">and</span> ProfileReport <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb18-5"><a href="#cb18-5"></a></span>
<span id="cb18-6"><a href="#cb18-6"></a>     <span class="bu">print</span>(<span class="st">"Generating comparison report for common features (this might take a while)..."</span>)</span>
<span id="cb18-7"><a href="#cb18-7"></a></span>
<span id="cb18-8"><a href="#cb18-8"></a>     <span class="co"># Define sampling fraction for faster report generation</span></span>
<span id="cb18-9"><a href="#cb18-9"></a>     p_frac_compare <span class="op">=</span> <span class="fl">0.1</span> <span class="co"># Sample 10% for comparison</span></span>
<span id="cb18-10"><a href="#cb18-10"></a></span>
<span id="cb18-11"><a href="#cb18-11"></a>     <span class="cf">try</span>:</span>
<span id="cb18-12"><a href="#cb18-12"></a>          <span class="co"># Profile for Accepted Common Features</span></span>
<span id="cb18-13"><a href="#cb18-13"></a>          accepted_common_profile <span class="op">=</span> ProfileReport(</span>
<span id="cb18-14"><a href="#cb18-14"></a>                df_accepted_train_common.sample(frac<span class="op">=</span>p_frac_compare, random_state<span class="op">=</span><span class="dv">2025</span>),</span>
<span id="cb18-15"><a href="#cb18-15"></a>                title<span class="op">=</span><span class="st">"Accepted Train"</span>,</span>
<span id="cb18-16"><a href="#cb18-16"></a>                progress_bar<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb18-17"><a href="#cb18-17"></a>                duplicates<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb18-18"><a href="#cb18-18"></a>                interactions<span class="op">=</span><span class="va">None</span></span>
<span id="cb18-19"><a href="#cb18-19"></a>          )</span>
<span id="cb18-20"><a href="#cb18-20"></a></span>
<span id="cb18-21"><a href="#cb18-21"></a>          <span class="co"># Profile for Rejected Common Features</span></span>
<span id="cb18-22"><a href="#cb18-22"></a>          rejected_common_profile <span class="op">=</span> ProfileReport(</span>
<span id="cb18-23"><a href="#cb18-23"></a>                df_rejected_train_common.sample(frac<span class="op">=</span>p_frac_compare, random_state<span class="op">=</span><span class="dv">2025</span>),</span>
<span id="cb18-24"><a href="#cb18-24"></a>                title<span class="op">=</span><span class="st">"Rejected Train"</span>,</span>
<span id="cb18-25"><a href="#cb18-25"></a>                progress_bar<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb18-26"><a href="#cb18-26"></a>                duplicates<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb18-27"><a href="#cb18-27"></a>                interactions<span class="op">=</span><span class="va">None</span></span>
<span id="cb18-28"><a href="#cb18-28"></a>          )</span>
<span id="cb18-29"><a href="#cb18-29"></a></span>
<span id="cb18-30"><a href="#cb18-30"></a>          <span class="co"># Compare the two profiles</span></span>
<span id="cb18-31"><a href="#cb18-31"></a>          comparison_report <span class="op">=</span> accepted_common_profile.compare(rejected_common_profile)</span>
<span id="cb18-32"><a href="#cb18-32"></a></span>
<span id="cb18-33"><a href="#cb18-33"></a>          <span class="co"># Save the comparison report</span></span>
<span id="cb18-34"><a href="#cb18-34"></a>          comparison_report_path <span class="op">=</span> <span class="st">"Lab02_eda_report_compare_common_features.html"</span></span>
<span id="cb18-35"><a href="#cb18-35"></a>          comparison_report.to_file(comparison_report_path)</span>
<span id="cb18-36"><a href="#cb18-36"></a>          <span class="bu">print</span>(<span class="ss">f"Comparison report saved to: </span><span class="sc">{</span>comparison_report_path<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-37"><a href="#cb18-37"></a></span>
<span id="cb18-38"><a href="#cb18-38"></a>     <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb18-39"><a href="#cb18-39"></a>          <span class="bu">print</span>(<span class="ss">f"Error generating comparison report: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-40"><a href="#cb18-40"></a></span>
<span id="cb18-41"><a href="#cb18-41"></a><span class="cf">else</span>:</span>
<span id="cb18-42"><a href="#cb18-42"></a>     <span class="bu">print</span>(<span class="st">"Skipping comparison report generation: Required dataframes or ProfileReport not available."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Generating comparison report for common features (this might take a while)...</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Comparison report saved to: Lab02_eda_report_compare_common_features.html</code></pre>
</div>
</div>
</section>
<section id="assess-feature-monotonicity-accepted-data" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="assess-feature-monotonicity-accepted-data"><span class="header-section-number">4.3</span> Assess Feature Monotonicity (Accepted Data)</h2>
<p><strong>Concept:</strong> Before building complex models, its essential to understand the fundamental relationship between key features and the target variable (probability of default). We expect certain features to have a monotonic relationship with risk  meaning, as the feature value increases, the risk should consistently increase or consistently decrease.</p>
<ul>
<li><strong>Example:</strong> We expect higher Debt-to-Income (<code>dti</code>) ratios to correspond to higher default risk (monotonic increasing). We expect higher credit scores (<code>credit_score</code>) to correspond to lower default risk (monotonic decreasing).</li>
</ul>
<p><strong>Method:</strong> We use the <code>binned_prob_plot</code> function on the <em>accepted training data</em> (<code>df_accepted_train_common</code>) to visualize the average default rate across bins of each feature. * For <strong>continuous features</strong>, the plot shows the trend across quantiles, and we calculate the <strong>Spearman rank correlation</strong> between the bin rank and the default rate (or log-odds). A correlation near +1 (increasing) or -1 (decreasing) suggests strong monotonicity. * For <strong>categorical features</strong>, the plot shows the default rate per category, and we calculate <strong>Mutual Information</strong> to measure if the feature provides information about the target, indicating varying risk levels across categories.</p>
<p><strong>Goal:</strong> Visualize the relationship between selected common features (<code>loan_amnt</code>, <code>dti</code>, <code>credit_score</code>, <code>emp_length</code>) and <code>default_flag</code> using the accepted training data. Check if the observed trends align with business intuition.</p>
<div class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb21-2"><a href="#cb21-2"></a></span>
<span id="cb21-3"><a href="#cb21-3"></a></span>
<span id="cb21-4"><a href="#cb21-4"></a><span class="cf">if</span> <span class="st">'df_accepted_train_common'</span> <span class="kw">in</span> <span class="bu">locals</span>() <span class="kw">and</span> df_accepted_train_common <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb21-5"><a href="#cb21-5"></a>    <span class="bu">print</span>(<span class="st">"--- Assessing Monotonicity on Accepted Training Data ---"</span>)</span>
<span id="cb21-6"><a href="#cb21-6"></a>    features_to_plot <span class="op">=</span> [<span class="st">'loan_amnt'</span>, <span class="st">'dti'</span>, <span class="st">'credit_score'</span>, <span class="st">'emp_length'</span>]</span>
<span id="cb21-7"><a href="#cb21-7"></a>    monotonicity_results_accepted <span class="op">=</span> []</span>
<span id="cb21-8"><a href="#cb21-8"></a>    </span>
<span id="cb21-9"><a href="#cb21-9"></a>    <span class="cf">for</span> feature <span class="kw">in</span> features_to_plot:</span>
<span id="cb21-10"><a href="#cb21-10"></a>        <span class="cf">if</span> feature <span class="kw">in</span> df_accepted_train_common.columns:</span>
<span id="cb21-11"><a href="#cb21-11"></a>            <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Plotting for feature: </span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-12"><a href="#cb21-12"></a>            </span>
<span id="cb21-13"><a href="#cb21-13"></a>            output <span class="op">=</span> binned_prob_plot(</span>
<span id="cb21-14"><a href="#cb21-14"></a>                data<span class="op">=</span>df_accepted_train_common, </span>
<span id="cb21-15"><a href="#cb21-15"></a>                feature<span class="op">=</span>feature, </span>
<span id="cb21-16"><a href="#cb21-16"></a>                target_binary<span class="op">=</span><span class="st">'default_flag'</span>,</span>
<span id="cb21-17"><a href="#cb21-17"></a>                cont_feat_flag<span class="op">=</span><span class="va">True</span></span>
<span id="cb21-18"><a href="#cb21-18"></a>            )</span>
<span id="cb21-19"><a href="#cb21-19"></a>            monotonicity_results_accepted.append(output)</span>
<span id="cb21-20"><a href="#cb21-20"></a>            <span class="bu">print</span>(<span class="ss">f"  Measure (</span><span class="sc">{</span>output[<span class="st">'measure_name'</span>]<span class="sc">}</span><span class="ss">): </span><span class="sc">{</span>output[<span class="st">'measure_value'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb21-21"><a href="#cb21-21"></a>            <span class="cf">if</span> output[<span class="st">'p_value'</span>] <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb21-22"><a href="#cb21-22"></a>                 <span class="bu">print</span>(<span class="ss">f"  P-value: </span><span class="sc">{</span>output[<span class="st">'p_value'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb21-23"><a href="#cb21-23"></a>        <span class="cf">else</span>:</span>
<span id="cb21-24"><a href="#cb21-24"></a>            <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Skipping plot for feature '</span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">' as it's not in df_accepted_train_common."</span>)</span>
<span id="cb21-25"><a href="#cb21-25"></a>            </span>
<span id="cb21-26"><a href="#cb21-26"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">--- Monotonicity Assessment Summary (Accepted Data) ---"</span>)</span>
<span id="cb21-27"><a href="#cb21-27"></a>    <span class="cf">for</span> res <span class="kw">in</span> monotonicity_results_accepted:</span>
<span id="cb21-28"><a href="#cb21-28"></a>        pval_str <span class="op">=</span> <span class="ss">f", p-value: </span><span class="sc">{</span>res[<span class="st">'p_value'</span>]<span class="sc">:.4f}</span><span class="ss">"</span> <span class="cf">if</span> res[<span class="st">'p_value'</span>] <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> <span class="st">""</span></span>
<span id="cb21-29"><a href="#cb21-29"></a>        <span class="bu">print</span>(<span class="ss">f"Feature: </span><span class="sc">{</span>res[<span class="st">'feature'</span>]<span class="sc">}</span><span class="ss">, Measure: </span><span class="sc">{</span>res[<span class="st">'measure_name'</span>]<span class="sc">}</span><span class="ss">, Value: </span><span class="sc">{</span>res[<span class="st">'measure_value'</span>]<span class="sc">:.4f}{</span>pval_str<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-30"><a href="#cb21-30"></a></span>
<span id="cb21-31"><a href="#cb21-31"></a><span class="cf">else</span>:</span>
<span id="cb21-32"><a href="#cb21-32"></a>    <span class="bu">print</span>(<span class="st">"Skipping monotonicity check as df_accepted_train_common is not available."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>--- Assessing Monotonicity on Accepted Training Data ---

Plotting for feature: loan_amnt</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>  Measure (spearman_corr): 0.7576
  P-value: 0.0111

Plotting for feature: dti</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>  Measure (spearman_corr): 1.0000
  P-value: 0.0000

Plotting for feature: credit_score</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>  Measure (spearman_corr): -1.0000
  P-value: 0.0000

Plotting for feature: emp_length</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>  Measure (spearman_corr): -0.7576
  P-value: 0.0111

--- Monotonicity Assessment Summary (Accepted Data) ---
Feature: loan_amnt, Measure: spearman_corr, Value: 0.7576, p-value: 0.0111
Feature: dti, Measure: spearman_corr, Value: 1.0000, p-value: 0.0000
Feature: credit_score, Measure: spearman_corr, Value: -1.0000, p-value: 0.0000
Feature: emp_length, Measure: spearman_corr, Value: -0.7576, p-value: 0.0111</code></pre>
</div>
<div id="fig-check-monotonicity-accepted" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="10">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-check-monotonicity-accepted-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<div id="fig-check-monotonicity-accepted-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-check-monotonicity-accepted-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Lab%2002_files/figure-html/fig-check-monotonicity-accepted-output-2.png" data-ref-parent="fig-check-monotonicity-accepted" width="1141" height="565" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-check-monotonicity-accepted-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Binned probability plots for the accept population.
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div id="fig-check-monotonicity-accepted-2" class="quarto-float quarto-figure quarto-figure-center anchored" height="565" width="1141">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-check-monotonicity-accepted-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Lab%2002_files/figure-html/fig-check-monotonicity-accepted-output-4.png" id="fig-check-monotonicity-accepted-2" data-ref-parent="fig-check-monotonicity-accepted" width="1141" height="565" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-check-monotonicity-accepted-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b)
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div id="fig-check-monotonicity-accepted-3" class="quarto-float quarto-figure quarto-figure-center anchored" height="565" width="1141">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-check-monotonicity-accepted-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Lab%2002_files/figure-html/fig-check-monotonicity-accepted-output-6.png" id="fig-check-monotonicity-accepted-3" data-ref-parent="fig-check-monotonicity-accepted" width="1141" height="565" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-check-monotonicity-accepted-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c)
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div id="fig-check-monotonicity-accepted-4" class="quarto-float quarto-figure quarto-figure-center anchored" height="565" width="1141">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-check-monotonicity-accepted-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Lab%2002_files/figure-html/fig-check-monotonicity-accepted-output-8.png" id="fig-check-monotonicity-accepted-4" data-ref-parent="fig-check-monotonicity-accepted" width="1141" height="565" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-check-monotonicity-accepted-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(d)
</figcaption>
</figure>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-check-monotonicity-accepted-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1
</figcaption>
</figure>
</div>
</div>
<p><strong>Interpretation:</strong> Based on the binned probability plots:</p>
<ul>
<li><strong>loan_amnt:</strong> Higher loan amounts are associated with higher default rates, indicating a positive correlation.</li>
<li><strong>dti:</strong> Higher Debt-to-Income ratios correlate with higher default rates, confirming the expected monotonic relationship.</li>
<li><strong>credit_score:</strong> Higher credit scores are associated with lower default rates, indicating a negative correlation.</li>
<li><strong>emp_length:</strong> Higher employment lengths are associated with lower default rates, which is expected.</li>
</ul>
<p><strong>Categorical Feature Check (State):</strong></p>
<p>For high-cardinality categorical features like <code>addr_state</code>, visualizing the binned probability might be less informative due to the large number of categories. However, we can still calculate the Mutual Information (MI) score to quantify the relationship between the state and the default flag without generating the plot. A higher MI suggests the state provides more information about the likelihood of default.</p>
<div id="check-monotonicity-state" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb27-2"><a href="#cb27-2"></a></span>
<span id="cb27-3"><a href="#cb27-3"></a></span>
<span id="cb27-4"><a href="#cb27-4"></a><span class="cf">if</span> <span class="st">'df_accepted_train_common'</span> <span class="kw">in</span> <span class="bu">locals</span>() <span class="kw">and</span> df_accepted_train_common <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb27-5"><a href="#cb27-5"></a>    feature_state <span class="op">=</span> <span class="st">'addr_state'</span></span>
<span id="cb27-6"><a href="#cb27-6"></a>    <span class="cf">if</span> feature_state <span class="kw">in</span> df_accepted_train_common.columns:</span>
<span id="cb27-7"><a href="#cb27-7"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">--- Calculating Mutual Information for </span><span class="sc">{</span>feature_state<span class="sc">}</span><span class="ss"> (Accepted Data) ---"</span>)</span>
<span id="cb27-8"><a href="#cb27-8"></a>        </span>
<span id="cb27-9"><a href="#cb27-9"></a>        <span class="co"># Calculate MI without plotting</span></span>
<span id="cb27-10"><a href="#cb27-10"></a>        state_mi_output <span class="op">=</span> binned_prob_plot(</span>
<span id="cb27-11"><a href="#cb27-11"></a>            data<span class="op">=</span>df_accepted_train_common, </span>
<span id="cb27-12"><a href="#cb27-12"></a>            feature<span class="op">=</span>feature_state, </span>
<span id="cb27-13"><a href="#cb27-13"></a>            target_binary<span class="op">=</span><span class="st">'default_flag'</span>,</span>
<span id="cb27-14"><a href="#cb27-14"></a>            cont_feat_flag<span class="op">=</span><span class="va">False</span>, <span class="co"># Explicitly categorical</span></span>
<span id="cb27-15"><a href="#cb27-15"></a>            show_plot<span class="op">=</span><span class="va">False</span> <span class="co"># Do not generate the plot</span></span>
<span id="cb27-16"><a href="#cb27-16"></a>        )</span>
<span id="cb27-17"><a href="#cb27-17"></a>        </span>
<span id="cb27-18"><a href="#cb27-18"></a>        <span class="bu">print</span>(<span class="ss">f"Feature: </span><span class="sc">{</span>state_mi_output[<span class="st">'feature'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb27-19"><a href="#cb27-19"></a>        <span class="bu">print</span>(<span class="ss">f"  Measure: </span><span class="sc">{</span>state_mi_output[<span class="st">'measure_name'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb27-20"><a href="#cb27-20"></a>        <span class="bu">print</span>(<span class="ss">f"  Value: </span><span class="sc">{</span>state_mi_output[<span class="st">'measure_value'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb27-21"><a href="#cb27-21"></a>        <span class="co"># Store result if needed</span></span>
<span id="cb27-22"><a href="#cb27-22"></a>        <span class="co"># monotonicity_results_accepted.append(state_mi_output) </span></span>
<span id="cb27-23"><a href="#cb27-23"></a>    <span class="cf">else</span>:</span>
<span id="cb27-24"><a href="#cb27-24"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Skipping MI calculation for feature '</span><span class="sc">{</span>feature_state<span class="sc">}</span><span class="ss">' as it's not in df_accepted_train_common."</span>)</span>
<span id="cb27-25"><a href="#cb27-25"></a><span class="cf">else</span>:</span>
<span id="cb27-26"><a href="#cb27-26"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Skipping MI calculation for 'addr_state' as df_accepted_train_common is not available."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
--- Calculating Mutual Information for addr_state (Accepted Data) ---
Feature: addr_state
  Measure: mutual_info
  Value: 0.0008</code></pre>
</div>
</div>
<p>Mutual Information for <code>addr_state</code> is calculated, but the plot is not generated due to the high cardinality of the feature. The MI value is very close to zero.</p>
</section>
<section id="define-modeling-features" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="define-modeling-features"><span class="header-section-number">4.4</span> Define Modeling Features</h2>
<p>In subsequent sections, we will use only the following features: <code>loan_amnt</code>, <code>dti</code>, <code>credit_score</code>, and <code>emp_length</code> for modeling. The <code>addr_state</code> feature will be excluded from the modeling process due to low mutual information.</p>
<div id="define-modeling-features" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1"></a>modeling_features <span class="op">=</span> [<span class="st">'loan_amnt'</span>, <span class="st">'dti'</span>, <span class="st">'credit_score'</span>, <span class="st">'emp_length'</span>]</span>
<span id="cb29-2"><a href="#cb29-2"></a><span class="bu">print</span>(<span class="ss">f"Features selected for modeling: </span><span class="sc">{</span>modeling_features<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Features selected for modeling: ['loan_amnt', 'dti', 'credit_score', 'emp_length']</code></pre>
</div>
</div>
</section>
</section>
<section id="fuzzy-augmentation" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Fuzzy Augmentation</h1>
<p><strong>Concept:</strong> Models trained only on accepted applicants (KGB - Known Good/Bad) suffer from selection bias because they dont learn from the rejected population, which often represents higher risk. Reject Inference (RI) techniques address this by incorporating information from rejects.</p>
<p><strong>Fuzzy Augmentation:</strong></p>
<ol type="1">
<li>Train a model (e.g., Logistic Regression) on the <em>accepted training data</em> (<code>df_accepted_train_common</code>) to predict the probability of default (PD), <code>PD = P(default=1 | features)</code>.</li>
<li>Apply this model to the <em>rejected</em> applicants to get their predicted <code>PD</code>.</li>
<li>Create two copies of the rejected applicant data:
<ol type="1">
<li><strong>Copy 1 (Assumed Bad):</strong> Assign <code>default_flag = 1</code> and <code>sample_weight = PD</code>.</li>
<li><strong>Copy 2 (Assumed Good):</strong> Assign <code>default_flag = 0</code> and <code>sample_weight = 1 - PD</code>.</li>
</ol></li>
<li>Combine the original accepted data (with <code>sample_weight = 1</code>) and the two weighted sets of rejected data to create the augmented Through-the-Door (TTD) dataset. This approach assigns fractional counts to both default and non-default outcomes for each rejected applicant based on their predicted risk.</li>
</ol>
<p><strong>Goal:</strong> Implement fuzzy augmentation to create the TTD training dataset.</p>
<section id="train-initial-model-for-weighting" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="train-initial-model-for-weighting"><span class="header-section-number">5.1</span> Train Initial Model for Weighting</h2>
<p>Train a Logistic Regression model on the <em>accepted training data</em> (<code>df_accepted_train_common</code>) using a subset of the common features. This model estimates the PD needed for weighting.</p>
<div id="ri-train-logit" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1"></a><span class="co"># Using only features selected for the main model for consistency in RI</span></span>
<span id="cb31-2"><a href="#cb31-2"></a>ri_features <span class="op">=</span> modeling_features </span>
<span id="cb31-3"><a href="#cb31-3"></a></span>
<span id="cb31-4"><a href="#cb31-4"></a><span class="bu">print</span>(<span class="ss">f"Using RI features (same as modeling features): </span><span class="sc">{</span>ri_features<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb31-5"><a href="#cb31-5"></a></span>
<span id="cb31-6"><a href="#cb31-6"></a><span class="co"># Prepare data for Logistic Regression - use .copy() to avoid SettingWithCopyWarning</span></span>
<span id="cb31-7"><a href="#cb31-7"></a>X_acc_ri <span class="op">=</span> df_accepted_train_common[ri_features].copy()</span>
<span id="cb31-8"><a href="#cb31-8"></a>y_acc_ri <span class="op">=</span> df_accepted_train_common[<span class="st">'default_flag'</span>] </span>
<span id="cb31-9"><a href="#cb31-9"></a></span>
<span id="cb31-10"><a href="#cb31-10"></a><span class="co"># Basic Preprocessing Pipeline for Logistic Regression</span></span>
<span id="cb31-11"><a href="#cb31-11"></a><span class="co"># All ri_features are numeric now</span></span>
<span id="cb31-12"><a href="#cb31-12"></a>numeric_features_ri <span class="op">=</span> ri_features <span class="co"># Use the selected features</span></span>
<span id="cb31-13"><a href="#cb31-13"></a></span>
<span id="cb31-14"><a href="#cb31-14"></a><span class="co"># Define preprocessing steps</span></span>
<span id="cb31-15"><a href="#cb31-15"></a>numeric_transformer_ri <span class="op">=</span> Pipeline(steps<span class="op">=</span>[</span>
<span id="cb31-16"><a href="#cb31-16"></a>    (<span class="st">'scaler'</span>, StandardScaler())</span>
<span id="cb31-17"><a href="#cb31-17"></a>])</span>
<span id="cb31-18"><a href="#cb31-18"></a></span>
<span id="cb31-19"><a href="#cb31-19"></a><span class="co"># Create the preprocessor - Apply the numeric transformer to the numeric features</span></span>
<span id="cb31-20"><a href="#cb31-20"></a>preprocessor_ri <span class="op">=</span> ColumnTransformer(</span>
<span id="cb31-21"><a href="#cb31-21"></a>    transformers<span class="op">=</span>[</span>
<span id="cb31-22"><a href="#cb31-22"></a>        (<span class="st">'num'</span>, numeric_transformer_ri, numeric_features_ri)</span>
<span id="cb31-23"><a href="#cb31-23"></a>    ],</span>
<span id="cb31-24"><a href="#cb31-24"></a>    remainder<span class="op">=</span><span class="st">'passthrough'</span> <span class="co"># Keep other columns if any (though there shouldn't be here)</span></span>
<span id="cb31-25"><a href="#cb31-25"></a>)</span>
<span id="cb31-26"><a href="#cb31-26"></a></span>
<span id="cb31-27"><a href="#cb31-27"></a></span>
<span id="cb31-28"><a href="#cb31-28"></a><span class="co"># Define LogisticRegressionCV with cross-validation parameters</span></span>
<span id="cb31-29"><a href="#cb31-29"></a>logreg_cv <span class="op">=</span> LogisticRegressionCV(</span>
<span id="cb31-30"><a href="#cb31-30"></a>    Cs<span class="op">=</span><span class="dv">10</span>,  <span class="co"># Try 10 C values on a logarithmic scale</span></span>
<span id="cb31-31"><a href="#cb31-31"></a>    cv<span class="op">=</span><span class="dv">5</span>,   <span class="co"># Use 5-fold cross-validation</span></span>
<span id="cb31-32"><a href="#cb31-32"></a>    penalty<span class="op">=</span><span class="st">'l2'</span>, <span class="co"># Use L2 regularization</span></span>
<span id="cb31-33"><a href="#cb31-33"></a>    scoring<span class="op">=</span><span class="st">'roc_auc'</span>, <span class="co"># Optimize for AUC</span></span>
<span id="cb31-34"><a href="#cb31-34"></a>    random_state<span class="op">=</span><span class="dv">2025</span>,</span>
<span id="cb31-35"><a href="#cb31-35"></a>    max_iter<span class="op">=</span><span class="dv">1000</span>, <span class="co"># Increase max iterations for convergence</span></span>
<span id="cb31-36"><a href="#cb31-36"></a>    solver<span class="op">=</span><span class="st">'liblinear'</span> <span class="co"># Suitable solver for this problem size and penalty</span></span>
<span id="cb31-37"><a href="#cb31-37"></a>)</span>
<span id="cb31-38"><a href="#cb31-38"></a></span>
<span id="cb31-39"><a href="#cb31-39"></a><span class="co"># Create the full pipeline with Logistic Regression CV</span></span>
<span id="cb31-40"><a href="#cb31-40"></a>ri_model <span class="op">=</span> Pipeline(steps<span class="op">=</span>[(<span class="st">'preprocessor'</span>, preprocessor_ri),</span>
<span id="cb31-41"><a href="#cb31-41"></a>                (<span class="st">'classifier'</span>, logreg_cv)]) <span class="co"># Use LogisticRegressionCV</span></span>
<span id="cb31-42"><a href="#cb31-42"></a></span>
<span id="cb31-43"><a href="#cb31-43"></a>global_set_seed(random_seed) <span class="co"># Set global seed</span></span>
<span id="cb31-44"><a href="#cb31-44"></a></span>
<span id="cb31-45"><a href="#cb31-45"></a><span class="bu">print</span>(<span class="st">"Training Reject Inference Logistic Regression model..."</span>)</span>
<span id="cb31-46"><a href="#cb31-46"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb31-47"><a href="#cb31-47"></a>ri_model.fit(X_acc_ri, y_acc_ri)</span>
<span id="cb31-48"><a href="#cb31-48"></a>end_time <span class="op">=</span> time.time()</span>
<span id="cb31-49"><a href="#cb31-49"></a><span class="bu">print</span>(<span class="ss">f"RI model training completed in </span><span class="sc">{</span>end_time <span class="op">-</span> start_time<span class="sc">:.2f}</span><span class="ss"> seconds."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Using RI features (same as modeling features): ['loan_amnt', 'dti', 'credit_score', 'emp_length']
Training Reject Inference Logistic Regression model...
RI model training completed in 29.31 seconds.</code></pre>
</div>
</div>
</section>
<section id="check-ri-model-coefficients" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="check-ri-model-coefficients"><span class="header-section-number">5.2</span> Check RI Model Coefficients</h2>
<p><strong>Goal:</strong> After training the RI model, we need to check the coefficients to ensure they align with our expectations based on the monotonicity checks. This helps validate that the model is capturing the expected relationships between features and default risk.</p>
<p>Check the <code>ri_model</code> (also known as the KGB model) to ensure the coefficients are reasonable.</p>
<div class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">--- RI Model Coefficients ---"</span>)</span>
<span id="cb33-2"><a href="#cb33-2"></a><span class="co"># Extract coefficients and intercept from the Logistic Regression model</span></span>
<span id="cb33-3"><a href="#cb33-3"></a><span class="cf">if</span> <span class="bu">hasattr</span>(ri_model.named_steps[<span class="st">'classifier'</span>], <span class="st">'coef_'</span>) <span class="kw">and</span> <span class="op">\</span></span>
<span id="cb33-4"><a href="#cb33-4"></a>    <span class="bu">hasattr</span>(ri_model.named_steps[<span class="st">'classifier'</span>], <span class="st">'intercept_'</span>):</span>
<span id="cb33-5"><a href="#cb33-5"></a>    </span>
<span id="cb33-6"><a href="#cb33-6"></a>    coeffs <span class="op">=</span> ri_model.named_steps[<span class="st">'classifier'</span>].coef_[<span class="dv">0</span>] <span class="co"># Get the coefficients</span></span>
<span id="cb33-7"><a href="#cb33-7"></a>    intercept <span class="op">=</span> ri_model.named_steps[<span class="st">'classifier'</span>].intercept_[<span class="dv">0</span>] <span class="co"># Get the intercept</span></span>
<span id="cb33-8"><a href="#cb33-8"></a>    </span>
<span id="cb33-9"><a href="#cb33-9"></a>    <span class="co"># Get feature names from the preprocessor step if possible</span></span>
<span id="cb33-10"><a href="#cb33-10"></a>    <span class="cf">try</span>:</span>
<span id="cb33-11"><a href="#cb33-11"></a>        <span class="co"># Access the fitted ColumnTransformer to get output feature names</span></span>
<span id="cb33-12"><a href="#cb33-12"></a>        preprocessor <span class="op">=</span> ri_model.named_steps[<span class="st">'preprocessor'</span>]</span>
<span id="cb33-13"><a href="#cb33-13"></a>        <span class="co"># Get feature names after transformation (e.g., scaled numeric features)</span></span>
<span id="cb33-14"><a href="#cb33-14"></a>        <span class="co"># Note: This relies on the structure of the preprocessor</span></span>
<span id="cb33-15"><a href="#cb33-15"></a>        feature_names <span class="op">=</span> preprocessor.get_feature_names_out()</span>
<span id="cb33-16"><a href="#cb33-16"></a>    <span class="cf">except</span> <span class="pp">Exception</span>:</span>
<span id="cb33-17"><a href="#cb33-17"></a>        <span class="co"># Fallback to original feature names if getting transformed names fails</span></span>
<span id="cb33-18"><a href="#cb33-18"></a>        <span class="bu">print</span>(<span class="st">"Warning: Could not get transformed feature names. Using original RI feature names."</span>)</span>
<span id="cb33-19"><a href="#cb33-19"></a>        feature_names <span class="op">=</span> ri_features <span class="co"># Use the original input feature names</span></span>
<span id="cb33-20"><a href="#cb33-20"></a></span>
<span id="cb33-21"><a href="#cb33-21"></a>    <span class="co"># Create DataFrame for coefficients</span></span>
<span id="cb33-22"><a href="#cb33-22"></a>    coeff_df <span class="op">=</span> pd.DataFrame({<span class="st">'Feature'</span>: feature_names, <span class="st">'Coefficient'</span>: coeffs})</span>
<span id="cb33-23"><a href="#cb33-23"></a>    </span>
<span id="cb33-24"><a href="#cb33-24"></a>    <span class="co"># Add the intercept as a separate row</span></span>
<span id="cb33-25"><a href="#cb33-25"></a>    intercept_df <span class="op">=</span> pd.DataFrame({<span class="st">'Feature'</span>: [<span class="st">'Intercept'</span>], <span class="st">'Coefficient'</span>: [intercept]})</span>
<span id="cb33-26"><a href="#cb33-26"></a>    </span>
<span id="cb33-27"><a href="#cb33-27"></a>    <span class="co"># Combine coefficients and intercept</span></span>
<span id="cb33-28"><a href="#cb33-28"></a>    full_coeff_df <span class="op">=</span> pd.concat([intercept_df, coeff_df], ignore_index<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb33-29"><a href="#cb33-29"></a>    </span>
<span id="cb33-30"><a href="#cb33-30"></a>    <span class="co"># Sort by absolute coefficient value might be more informative, but sorting by value is fine</span></span>
<span id="cb33-31"><a href="#cb33-31"></a>    <span class="co"># full_coeff_df.sort_values(by='Coefficient', ascending=False, inplace=True) </span></span>
<span id="cb33-32"><a href="#cb33-32"></a>    </span>
<span id="cb33-33"><a href="#cb33-33"></a>    display(full_coeff_df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
--- RI Model Coefficients ---</code></pre>
</div>
<div id="tbl-ri-model-coefficients" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="14">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-ri-model-coefficients-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: Reject Inference Model Coefficients
</figcaption>
<div aria-describedby="tbl-ri-model-coefficients-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<div>


<table class="dataframe do-not-create-environment cell caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Feature</th>
<th data-quarto-table-cell-role="th">Coefficient</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>Intercept</td>
<td>-1.850129</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>num__loan_amnt</td>
<td>0.103190</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>num__dti</td>
<td>0.091577</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>num__credit_score</td>
<td>-0.383273</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>num__emp_length</td>
<td>-0.047428</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</figure>
</div>
</div>
<p><strong>Interpretation:</strong> The coefficients of the RI model should align with our expectations based on the monotonicity checks. For example, we expect a positive coefficient for <code>dti</code> (higher DTI leads to higher default risk) and a negative coefficient for <code>credit_score</code> (higher credit score leads to lower default risk).</p>
</section>
<section id="calculate-weights-and-create-ttd-dataset" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="calculate-weights-and-create-ttd-dataset"><span class="header-section-number">5.3</span> Calculate Weights and Create TTD Dataset</h2>
<p>Apply the trained <code>ri_model</code> to the <em>rejected training data</em> (<code>df_rejected_train_common</code>), calculate fuzzy weights, assign <code>default_flag=1</code>, and combine with <code>df_accepted_train_common</code> to create <code>df_ttd_train</code>.</p>
<div class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1"></a><span class="bu">print</span>(<span class="st">"Applying RI model to rejected training data and calculating weights..."</span>)</span>
<span id="cb35-2"><a href="#cb35-2"></a></span>
<span id="cb35-3"><a href="#cb35-3"></a><span class="co"># Create df_ttd_train with the source column</span></span>
<span id="cb35-4"><a href="#cb35-4"></a>df_ttd_train <span class="op">=</span> create_TTD_data( <span class="co"># Renamed variable</span></span>
<span id="cb35-5"><a href="#cb35-5"></a>    ri_model<span class="op">=</span>ri_model,</span>
<span id="cb35-6"><a href="#cb35-6"></a>    df_rejected<span class="op">=</span>df_rejected_train_common,</span>
<span id="cb35-7"><a href="#cb35-7"></a>    df_accepted<span class="op">=</span>df_accepted_train_common,</span>
<span id="cb35-8"><a href="#cb35-8"></a>    ri_features<span class="op">=</span>ri_features,</span>
<span id="cb35-9"><a href="#cb35-9"></a>    modeling_features<span class="op">=</span>modeling_features,</span>
<span id="cb35-10"><a href="#cb35-10"></a>    target_col<span class="op">=</span>target_col</span>
<span id="cb35-11"><a href="#cb35-11"></a>)</span>
<span id="cb35-12"><a href="#cb35-12"></a></span>
<span id="cb35-13"><a href="#cb35-13"></a><span class="co"># Calculate and display summary statistics by source using helper function</span></span>
<span id="cb35-14"><a href="#cb35-14"></a>summary_default_rates <span class="op">=</span> summarize_ttd_by_source(</span>
<span id="cb35-15"><a href="#cb35-15"></a>    df_ttd<span class="op">=</span>df_ttd_train,</span>
<span id="cb35-16"><a href="#cb35-16"></a>    target_col<span class="op">=</span>target_col,</span>
<span id="cb35-17"><a href="#cb35-17"></a>    weight_col<span class="op">=</span><span class="st">'sample_weight'</span>,</span>
<span id="cb35-18"><a href="#cb35-18"></a>    source_col<span class="op">=</span><span class="st">'source'</span></span>
<span id="cb35-19"><a href="#cb35-19"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Applying RI model to rejected training data and calculating weights...
Applying RI model to rejected data and calculating weights...
TTD dataset created. Shape: (1956420, 7)
Sum of weights in TTD dataset: 1656420.00
Source distribution:
source
Accepted    0.693317
Rejected    0.306683
Name: proportion, dtype: float64

--- Summarizing TTD Data by 'source' ---
Summary of Default Rates by Source:</code></pre>
</div>
<div id="tbl-ri-apply-weights" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="15">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-ri-apply-weights-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: Weighted default rates for TTD Train Data
</figcaption>
<div aria-describedby="tbl-ri-apply-weights-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<div>


<table class="dataframe do-not-create-environment cell caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">source</th>
<th data-quarto-table-cell-role="th">row_count</th>
<th data-quarto-table-cell-role="th">unweighted_default_rate</th>
<th data-quarto-table-cell-role="th">sum_weights</th>
<th data-quarto-table-cell-role="th">weighted_default_rate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>Accepted</td>
<td>1356420</td>
<td>0.128645</td>
<td>1356420.0</td>
<td>0.128645</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>Rejected</td>
<td>600000</td>
<td>0.500000</td>
<td>300000.0</td>
<td>0.457948</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</figure>
</div>
</div>
<p>For rejected applicants in the TTD dataset, the <code>default_flag</code> and <code>sample_weight</code> are not observed outcomes. Instead, they are assigned based on the RI models predicted probability of default (PD). Each rejected applicant is represented twice: once as an assumed default (<code>default_flag=1</code>, weighted by PD) and once as an assumed non-default (<code>default_flag=0</code>, weighted by 1-PD). This approach reflects the models belief about their likely outcome, rather than an actual observed default status.</p>
<div class="cell" data-execution_count="16">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1"></a>df_sample <span class="op">=</span> df_ttd_train.sample(<span class="dv">10</span>, random_state<span class="op">=</span><span class="dv">2025</span>)</span>
<span id="cb37-2"><a href="#cb37-2"></a></span>
<span id="cb37-3"><a href="#cb37-3"></a>display(df_sample)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-ttd-sample" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="16">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-ttd-sample-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4: Sample of Augmented TTD Training Data (Fuzzy Augmentation)
</figcaption>
<div aria-describedby="tbl-ttd-sample-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<div>


<table class="dataframe do-not-create-environment cell caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">loan_amnt</th>
<th data-quarto-table-cell-role="th">dti</th>
<th data-quarto-table-cell-role="th">credit_score</th>
<th data-quarto-table-cell-role="th">emp_length</th>
<th data-quarto-table-cell-role="th">default_flag</th>
<th data-quarto-table-cell-role="th">sample_weight</th>
<th data-quarto-table-cell-role="th">source</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">312851</td>
<td>15000.0</td>
<td>23.620001</td>
<td>682.0</td>
<td>0</td>
<td>1</td>
<td>1.000000</td>
<td>Accepted</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">661783</td>
<td>30000.0</td>
<td>26.700001</td>
<td>662.0</td>
<td>8</td>
<td>0</td>
<td>1.000000</td>
<td>Accepted</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">867013</td>
<td>14000.0</td>
<td>24.100000</td>
<td>662.0</td>
<td>8</td>
<td>1</td>
<td>1.000000</td>
<td>Accepted</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">90266</td>
<td>11500.0</td>
<td>15.890000</td>
<td>727.0</td>
<td>4</td>
<td>0</td>
<td>1.000000</td>
<td>Accepted</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">776829</td>
<td>22000.0</td>
<td>27.010000</td>
<td>682.0</td>
<td>4</td>
<td>0</td>
<td>1.000000</td>
<td>Accepted</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">738926</td>
<td>10000.0</td>
<td>20.110001</td>
<td>662.0</td>
<td>1</td>
<td>0</td>
<td>1.000000</td>
<td>Accepted</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">838145</td>
<td>17000.0</td>
<td>14.190000</td>
<td>667.0</td>
<td>8</td>
<td>0</td>
<td>1.000000</td>
<td>Accepted</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1408504</td>
<td>1000.0</td>
<td>22.299999</td>
<td>572.0</td>
<td>0</td>
<td>1</td>
<td>0.396747</td>
<td>Rejected</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">1505540</td>
<td>9500.0</td>
<td>21.629999</td>
<td>541.0</td>
<td>0</td>
<td>1</td>
<td>0.507679</td>
<td>Rejected</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1191956</td>
<td>3000.0</td>
<td>3.600000</td>
<td>787.0</td>
<td>0</td>
<td>0</td>
<td>1.000000</td>
<td>Accepted</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</figure>
</div>
</div>
</section>
</section>
<section id="building-the-ttd-scorecard-model-initial" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Building the TTD Scorecard Model (Initial)</h1>
<p><strong>Goal:</strong> Train a preliminary scorecard model using AutoGluon on the augmented TTD training dataset (<code>df_ttd_train</code>). This model will incorporate the <code>sample_weight</code> calculated during reject inference but will <em>not</em> yet have monotonic constraints applied.</p>
<section id="configure-and-train-autogluon-model" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="configure-and-train-autogluon-model"><span class="header-section-number">6.1</span> Configure and Train AutoGluon Model</h2>
<p>Set up AutoGluon to train on the TTD data, specifying the label (<code>default_flag</code>), sample weight column, and excluding complex models like Neural Networks to favor interpretability.</p>
<p>Sample weights are used only during the fit() process to influence how the model learns from the training data. Once the model is trained, predictions are made based solely on the input features the model learned from. You only need to provide the feature columns in the DataFrame passed to <code>predict()</code> or <code>predict_proba()</code>.</p>
<div class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1"></a>label <span class="op">=</span> <span class="st">'default_flag'</span></span>
<span id="cb38-2"><a href="#cb38-2"></a>weight_col <span class="op">=</span> <span class="st">'sample_weight'</span></span>
<span id="cb38-3"><a href="#cb38-3"></a>        </span>
<span id="cb38-4"><a href="#cb38-4"></a><span class="co"># --- AutoGluon Configuration ---</span></span>
<span id="cb38-5"><a href="#cb38-5"></a>model_folder_ttd_initial <span class="op">=</span> <span class="st">'Lab02_ag_models_TTD_Initial'</span></span>
<span id="cb38-6"><a href="#cb38-6"></a></span>
<span id="cb38-7"><a href="#cb38-7"></a><span class="co"># Hyperparameters: Limit tree depth for simplicity and interpretability</span></span>
<span id="cb38-8"><a href="#cb38-8"></a>custom_hyperparameters <span class="op">=</span> {</span>
<span id="cb38-9"><a href="#cb38-9"></a>    <span class="st">'GBM'</span>: {<span class="st">'num_boost_round'</span>: <span class="dv">10000</span>, <span class="st">'num_leaves'</span>: <span class="dv">4</span>},</span>
<span id="cb38-10"><a href="#cb38-10"></a>    <span class="st">'CAT'</span>: {<span class="st">'iterations'</span>: <span class="dv">10000</span>, <span class="st">'depth'</span>: <span class="dv">2</span>}</span>
<span id="cb38-11"><a href="#cb38-11"></a>}</span>
<span id="cb38-12"><a href="#cb38-12"></a>excluded_model_types <span class="op">=</span> [<span class="st">'NN_TORCH'</span>, <span class="st">'FASTAI'</span>, <span class="st">'KNN'</span>] </span>
<span id="cb38-13"><a href="#cb38-13"></a></span>
<span id="cb38-14"><a href="#cb38-14"></a><span class="co"># Arguments for TabularPredictor initialization (passed via wrapper)</span></span>
<span id="cb38-15"><a href="#cb38-15"></a>predictor_args <span class="op">=</span> {</span>
<span id="cb38-16"><a href="#cb38-16"></a>    <span class="st">'problem_type'</span>: <span class="st">'binary'</span>,</span>
<span id="cb38-17"><a href="#cb38-17"></a>    <span class="st">'eval_metric'</span>: <span class="st">'roc_auc'</span>, </span>
<span id="cb38-18"><a href="#cb38-18"></a>    <span class="st">'path'</span>: model_folder_ttd_initial,</span>
<span id="cb38-19"><a href="#cb38-19"></a>    <span class="st">'sample_weight'</span>: weight_col         <span class="co"># key parameter for Fuzz Augmentation</span></span>
<span id="cb38-20"><a href="#cb38-20"></a>}</span>
<span id="cb38-21"><a href="#cb38-21"></a></span>
<span id="cb38-22"><a href="#cb38-22"></a><span class="co"># Arguments for TabularPredictor.fit (passed via wrapper)</span></span>
<span id="cb38-23"><a href="#cb38-23"></a>fit_args <span class="op">=</span> {</span>
<span id="cb38-24"><a href="#cb38-24"></a>    <span class="st">'presets'</span>: {</span>
<span id="cb38-25"><a href="#cb38-25"></a>        <span class="st">'holdout_frac'</span>: <span class="fl">0.2</span>,</span>
<span id="cb38-26"><a href="#cb38-26"></a>        <span class="st">'excluded_model_types'</span>: excluded_model_types,</span>
<span id="cb38-27"><a href="#cb38-27"></a>        <span class="st">'hyperparameters'</span>: custom_hyperparameters, </span>
<span id="cb38-28"><a href="#cb38-28"></a>        <span class="st">'time_limit'</span>: <span class="dv">300</span></span>
<span id="cb38-29"><a href="#cb38-29"></a>    }</span>
<span id="cb38-30"><a href="#cb38-30"></a>}</span>
<span id="cb38-31"><a href="#cb38-31"></a></span>
<span id="cb38-32"><a href="#cb38-32"></a>global_set_seed(random_seed) <span class="co"># Set global seed</span></span>
<span id="cb38-33"><a href="#cb38-33"></a></span>
<span id="cb38-34"><a href="#cb38-34"></a><span class="co"># Train model using the helper function</span></span>
<span id="cb38-35"><a href="#cb38-35"></a>ag_model_initial_wrapped <span class="op">=</span> train_autogluon_model(</span>
<span id="cb38-36"><a href="#cb38-36"></a>    df_train<span class="op">=</span>df_ttd_train,</span>
<span id="cb38-37"><a href="#cb38-37"></a>    label<span class="op">=</span>label,</span>
<span id="cb38-38"><a href="#cb38-38"></a>    weight_col<span class="op">=</span>weight_col,</span>
<span id="cb38-39"><a href="#cb38-39"></a>    modeling_features<span class="op">=</span>modeling_features, <span class="co"># Use derived features list</span></span>
<span id="cb38-40"><a href="#cb38-40"></a>    model_folder<span class="op">=</span>model_folder_ttd_initial,</span>
<span id="cb38-41"><a href="#cb38-41"></a>    predictor_args<span class="op">=</span>predictor_args,</span>
<span id="cb38-42"><a href="#cb38-42"></a>    fit_args<span class="op">=</span>fit_args</span>
<span id="cb38-43"><a href="#cb38-43"></a>)</span>
<span id="cb38-44"><a href="#cb38-44"></a></span>
<span id="cb38-45"><a href="#cb38-45"></a><span class="co"># Access the underlying predictor for leaderboard etc. (optional, as helper prints it)</span></span>
<span id="cb38-46"><a href="#cb38-46"></a>ag_predictor_initial <span class="op">=</span> ag_model_initial_wrapped.predictor</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>--- Training AutoGluon Model in: Lab02_ag_models_TTD_Initial ---
Removed existing AutoGluon folder: Lab02_ag_models_TTD_Initial
Using features: ['loan_amnt', 'dti', 'credit_score', 'emp_length']
Training data shape: (1956420, 7)
Sum of weights in training data: 1656420.00</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>AutoGluon training completed in 12.80 seconds.

AutoGluon Leaderboard:</code></pre>
</div>
<div id="tbl-ttd-modeling-initial" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="17">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-ttd-modeling-initial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;5: Leaderboard for initial TTD Model
</figcaption>
<div aria-describedby="tbl-ttd-modeling-initial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<div>


<table class="dataframe do-not-create-environment cell caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">model</th>
<th data-quarto-table-cell-role="th">score_val</th>
<th data-quarto-table-cell-role="th">eval_metric</th>
<th data-quarto-table-cell-role="th">pred_time_val</th>
<th data-quarto-table-cell-role="th">fit_time</th>
<th data-quarto-table-cell-role="th">pred_time_val_marginal</th>
<th data-quarto-table-cell-role="th">fit_time_marginal</th>
<th data-quarto-table-cell-role="th">stack_level</th>
<th data-quarto-table-cell-role="th">can_infer</th>
<th data-quarto-table-cell-role="th">fit_order</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>LightGBM</td>
<td>0.735284</td>
<td>roc_auc</td>
<td>0.084064</td>
<td>3.676083</td>
<td>0.084064</td>
<td>3.676083</td>
<td>1</td>
<td>True</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>WeightedEnsemble_L2</td>
<td>0.735284</td>
<td>roc_auc</td>
<td>0.122645</td>
<td>5.699791</td>
<td>0.038581</td>
<td>2.023708</td>
<td>2</td>
<td>True</td>
<td>3</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>CatBoost</td>
<td>0.733252</td>
<td>roc_auc</td>
<td>0.012412</td>
<td>5.221535</td>
<td>0.012412</td>
<td>5.221535</td>
<td>1</td>
<td>True</td>
<td>2</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</figure>
</div>
</div>
</section>
</section>
<section id="initial-model-diagnostics-pdpice" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Initial Model Diagnostics (PDP/ICE)</h1>
<p><strong>Goal:</strong> Analyze the behavior of the <em>initial, unconstrained</em> TTD model. Use Partial Dependence Plots (PDP) and Individual Conditional Expectation (ICE) plots to understand how the models predictions change on average (PDP) and for individual instances (ICE) as key feature values vary. This helps identify if the model learned relationships that contradict business logic (e.g., non-monotonic trends).</p>
<div class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb41-2"><a href="#cb41-2"></a></span>
<span id="cb41-3"><a href="#cb41-3"></a><span class="co"># Get all features from ag_model.predictor</span></span>
<span id="cb41-4"><a href="#cb41-4"></a>all_features <span class="op">=</span> ag_model_initial_wrapped.predictor.features()</span>
<span id="cb41-5"><a href="#cb41-5"></a></span>
<span id="cb41-6"><a href="#cb41-6"></a><span class="co"># Get categorical features from ag_model.predictor's feature metadata</span></span>
<span id="cb41-7"><a href="#cb41-7"></a>categorical_features <span class="op">=</span> ag_model_initial_wrapped.predictor.feature_metadata.get_features(valid_raw_types<span class="op">=</span>[<span class="st">'category'</span>])</span>
<span id="cb41-8"><a href="#cb41-8"></a></span>
<span id="cb41-9"><a href="#cb41-9"></a><span class="co"># Get numeric features from ag_model.predictor's feature metadata</span></span>
<span id="cb41-10"><a href="#cb41-10"></a>numeric_features <span class="op">=</span> ag_model_initial_wrapped.predictor.feature_metadata.get_features(valid_raw_types<span class="op">=</span>[<span class="st">'int'</span>, <span class="st">'float'</span>, <span class="st">'int64'</span>, <span class="st">'float64'</span>, <span class="st">'int32'</span>, <span class="st">'float32'</span>])</span>
<span id="cb41-11"><a href="#cb41-11"></a></span>
<span id="cb41-12"><a href="#cb41-12"></a></span>
<span id="cb41-13"><a href="#cb41-13"></a>show_pdp(wrappedAGModel <span class="op">=</span> ag_model_initial_wrapped,</span>
<span id="cb41-14"><a href="#cb41-14"></a>        list_features <span class="op">=</span> all_features, </span>
<span id="cb41-15"><a href="#cb41-15"></a>        list_categ_features <span class="op">=</span> categorical_features <span class="cf">if</span> categorical_features <span class="cf">else</span> <span class="va">None</span>,</span>
<span id="cb41-16"><a href="#cb41-16"></a>        df <span class="op">=</span> df_ttd_train.drop(columns<span class="op">=</span>[label, weight_col]),</span>
<span id="cb41-17"><a href="#cb41-17"></a>        show_ice<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb41-18"><a href="#cb41-18"></a>        sampSize<span class="op">=</span><span class="dv">25_000</span></span>
<span id="cb41-19"><a href="#cb41-19"></a>        )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-pdp-ice-initial" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="18">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pdp-ice-initial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<div id="fig-pdp-ice-initial-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-pdp-ice-initial-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Lab%2002_files/figure-html/fig-pdp-ice-initial-output-1.png" data-ref-parent="fig-pdp-ice-initial" width="687" height="400" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-pdp-ice-initial-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) PDP/ICE plots for the initial TTD model
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div id="fig-pdp-ice-initial-2" class="quarto-float quarto-figure quarto-figure-center anchored" height="399" width="684">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-pdp-ice-initial-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Lab%2002_files/figure-html/fig-pdp-ice-initial-output-2.png" id="fig-pdp-ice-initial-2" data-ref-parent="fig-pdp-ice-initial" width="684" height="399" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-pdp-ice-initial-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b)
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div id="fig-pdp-ice-initial-3" class="quarto-float quarto-figure quarto-figure-center anchored" height="400" width="684">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-pdp-ice-initial-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Lab%2002_files/figure-html/fig-pdp-ice-initial-output-3.png" id="fig-pdp-ice-initial-3" data-ref-parent="fig-pdp-ice-initial" width="684" height="400" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-pdp-ice-initial-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c)
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div id="fig-pdp-ice-initial-4" class="quarto-float quarto-figure quarto-figure-center anchored" height="400" width="684">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-pdp-ice-initial-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Lab%2002_files/figure-html/fig-pdp-ice-initial-output-4.png" id="fig-pdp-ice-initial-4" data-ref-parent="fig-pdp-ice-initial" width="684" height="400" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-pdp-ice-initial-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(d)
</figcaption>
</figure>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-pdp-ice-initial-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2
</figcaption>
</figure>
</div>
</div>
<p><strong>Interpretation:</strong></p>
<p>Examine the PDP (red dashed line) and ICE (thin blue lines) for each feature:</p>
<ul>
<li>Does <code>loan_amnt</code> consistently increase the predicted probability?</li>
<li>Does <code>dti</code> consistently increase the predicted probability?</li>
<li>Does <code>credit_score</code> consistently decrease the predicted probability?</li>
</ul>
<p>If any of these plots show non-monotonic behavior (e.g., the average trend goes up then down, or vice-versa), it violates our business intuition and suggests that applying monotonic constraints is necessary. The ICE lines show if this behavior is consistent across all samples or if theres significant heterogeneity.</p>
</section>
<section id="applying-monotonic-constraints" class="level1" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> Applying Monotonic Constraints</h1>
<p><strong>Concept:</strong> Monotonic constraints force the model to learn relationships that align with business expectations. We specify whether a feature should have a non-decreasing (+) or non-increasing (-) relationship with the target probability. AutoGluon passes these constraints to underlying models that support them (like LightGBM, XGBoost).</p>
<p><strong>Goal:</strong> Define monotonic constraints based on business logic (e.g., <code>dti</code> increases risk, <code>credit_score</code> decreases risk) and retrain the AutoGluon model on the TTD data with these constraints enforced.</p>
<section id="define-constraints-and-retrain-model" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="define-constraints-and-retrain-model"><span class="header-section-number">8.1</span> Define Constraints and Retrain Model</h2>
<p>Specify the desired monotonic relationship for key features using AutoGluons <code>feature_metadata</code> and retrain the model.</p>
<div class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1"></a><span class="bu">print</span>(<span class="st">"Training constrained AutoGluon model with monotonic constraints via hyperparameters..."</span>)</span>
<span id="cb42-2"><a href="#cb42-2"></a><span class="co"># Define monotonic constraints for boosting models</span></span>
<span id="cb42-3"><a href="#cb42-3"></a></span>
<span id="cb42-4"><a href="#cb42-4"></a></span>
<span id="cb42-5"><a href="#cb42-5"></a>monotone_constraints_dict <span class="op">=</span> {<span class="st">'loan_amnt'</span>: <span class="dv">1</span>, <span class="st">'dti'</span>: <span class="dv">1</span>, <span class="st">'credit_score'</span>: <span class="op">-</span><span class="dv">1</span>, <span class="st">'emp_length'</span>: <span class="op">-</span><span class="dv">1</span>}</span>
<span id="cb42-6"><a href="#cb42-6"></a></span>
<span id="cb42-7"><a href="#cb42-7"></a>monotone_constraints_ordered <span class="op">=</span> [monotone_constraints_dict.get(f, <span class="dv">0</span>) <span class="cf">for</span> f <span class="kw">in</span> all_features] <span class="co"># Default to 0 if feature not in dict</span></span>
<span id="cb42-8"><a href="#cb42-8"></a></span>
<span id="cb42-9"><a href="#cb42-9"></a><span class="co"># Update hyperparameters to include monotonic constraints</span></span>
<span id="cb42-10"><a href="#cb42-10"></a>custom_hyperparameters_constrained <span class="op">=</span> {</span>
<span id="cb42-11"><a href="#cb42-11"></a>    <span class="st">'GBM'</span>: {<span class="op">**</span>custom_hyperparameters[<span class="st">'GBM'</span>], <span class="st">'monotone_constraints'</span>: monotone_constraints_ordered},</span>
<span id="cb42-12"><a href="#cb42-12"></a>    <span class="st">'CAT'</span>: {<span class="op">**</span>custom_hyperparameters[<span class="st">'CAT'</span>], <span class="st">'monotone_constraints'</span>: monotone_constraints_dict} <span class="co"># CatBoost uses dict</span></span>
<span id="cb42-13"><a href="#cb42-13"></a>}</span>
<span id="cb42-14"><a href="#cb42-14"></a></span>
<span id="cb42-15"><a href="#cb42-15"></a><span class="co"># Clean up previous model folder</span></span>
<span id="cb42-16"><a href="#cb42-16"></a>model_folder_ttd_constrained <span class="op">=</span> <span class="st">'Lab02_ag_models_TTD_Constrained'</span></span>
<span id="cb42-17"><a href="#cb42-17"></a></span>
<span id="cb42-18"><a href="#cb42-18"></a><span class="co"># Prepare predictor and fit arguments (reuse from initial, update path and hyperparameters)</span></span>
<span id="cb42-19"><a href="#cb42-19"></a>predictor_args_constrained <span class="op">=</span> predictor_args.copy()</span>
<span id="cb42-20"><a href="#cb42-20"></a>predictor_args_constrained[<span class="st">'path'</span>] <span class="op">=</span> model_folder_ttd_constrained</span>
<span id="cb42-21"><a href="#cb42-21"></a></span>
<span id="cb42-22"><a href="#cb42-22"></a>fit_args_constrained <span class="op">=</span> {</span>
<span id="cb42-23"><a href="#cb42-23"></a>    <span class="st">'presets'</span>: {</span>
<span id="cb42-24"><a href="#cb42-24"></a>        <span class="st">'hyperparameters'</span>: custom_hyperparameters_constrained,</span>
<span id="cb42-25"><a href="#cb42-25"></a>        <span class="st">'excluded_model_types'</span>: excluded_model_types,</span>
<span id="cb42-26"><a href="#cb42-26"></a>        <span class="st">'holdout_frac'</span>: <span class="fl">0.2</span>,</span>
<span id="cb42-27"><a href="#cb42-27"></a>        <span class="st">'time_limit'</span>: <span class="dv">300</span></span>
<span id="cb42-28"><a href="#cb42-28"></a>    }</span>
<span id="cb42-29"><a href="#cb42-29"></a>}</span>
<span id="cb42-30"><a href="#cb42-30"></a></span>
<span id="cb42-31"><a href="#cb42-31"></a>global_set_seed(random_seed) <span class="co"># Set global seed</span></span>
<span id="cb42-32"><a href="#cb42-32"></a></span>
<span id="cb42-33"><a href="#cb42-33"></a><span class="co"># Train constrained model using the helper function</span></span>
<span id="cb42-34"><a href="#cb42-34"></a>ag_model_constrained_wrapped <span class="op">=</span> train_autogluon_model(</span>
<span id="cb42-35"><a href="#cb42-35"></a>    df_train<span class="op">=</span>df_ttd_train,</span>
<span id="cb42-36"><a href="#cb42-36"></a>    label<span class="op">=</span>label,</span>
<span id="cb42-37"><a href="#cb42-37"></a>    weight_col<span class="op">=</span>weight_col,</span>
<span id="cb42-38"><a href="#cb42-38"></a>    modeling_features<span class="op">=</span>modeling_features, <span class="co"># Use features from initial model</span></span>
<span id="cb42-39"><a href="#cb42-39"></a>    model_folder<span class="op">=</span>model_folder_ttd_constrained,</span>
<span id="cb42-40"><a href="#cb42-40"></a>    predictor_args<span class="op">=</span>predictor_args_constrained,</span>
<span id="cb42-41"><a href="#cb42-41"></a>    fit_args<span class="op">=</span>fit_args_constrained</span>
<span id="cb42-42"><a href="#cb42-42"></a>)</span>
<span id="cb42-43"><a href="#cb42-43"></a></span>
<span id="cb42-44"><a href="#cb42-44"></a><span class="co"># Access the underlying predictor (optional, as helper prints leaderboard)</span></span>
<span id="cb42-45"><a href="#cb42-45"></a>ag_predictor_constrained <span class="op">=</span> ag_model_constrained_wrapped.predictor</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Training constrained AutoGluon model with monotonic constraints via hyperparameters...
--- Training AutoGluon Model in: Lab02_ag_models_TTD_Constrained ---
Removed existing AutoGluon folder: Lab02_ag_models_TTD_Constrained
Using features: ['loan_amnt', 'dti', 'credit_score', 'emp_length']
Training data shape: (1956420, 7)
Sum of weights in training data: 1656420.00</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>AutoGluon training completed in 10.64 seconds.

AutoGluon Leaderboard:</code></pre>
</div>
<div id="tbl-monotonic-constraints-setup-train" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="19">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-monotonic-constraints-setup-train-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;6: Leaderboard for constrained TTD Model
</figcaption>
<div aria-describedby="tbl-monotonic-constraints-setup-train-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<div>


<table class="dataframe do-not-create-environment cell caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">model</th>
<th data-quarto-table-cell-role="th">score_val</th>
<th data-quarto-table-cell-role="th">eval_metric</th>
<th data-quarto-table-cell-role="th">pred_time_val</th>
<th data-quarto-table-cell-role="th">fit_time</th>
<th data-quarto-table-cell-role="th">pred_time_val_marginal</th>
<th data-quarto-table-cell-role="th">fit_time_marginal</th>
<th data-quarto-table-cell-role="th">stack_level</th>
<th data-quarto-table-cell-role="th">can_infer</th>
<th data-quarto-table-cell-role="th">fit_order</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>LightGBM</td>
<td>0.734591</td>
<td>roc_auc</td>
<td>0.072582</td>
<td>1.508358</td>
<td>0.072582</td>
<td>1.508358</td>
<td>1</td>
<td>True</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>WeightedEnsemble_L2</td>
<td>0.734591</td>
<td>roc_auc</td>
<td>0.117359</td>
<td>3.385536</td>
<td>0.044777</td>
<td>1.877178</td>
<td>2</td>
<td>True</td>
<td>3</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>CatBoost</td>
<td>0.733427</td>
<td>roc_auc</td>
<td>0.014516</td>
<td>5.382039</td>
<td>0.014516</td>
<td>5.382039</td>
<td>1</td>
<td>True</td>
<td>2</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</figure>
</div>
</div>
</section>
</section>
<section id="verifying-constraints-pdpice" class="level1" data-number="9">
<h1 data-number="9"><span class="header-section-number">9</span> Verifying Constraints (PDP/ICE)</h1>
<p><strong>Goal:</strong> Re-run the PDP/ICE plots on the <em>constrained</em> model to visually confirm that the specified monotonic relationships for <code>loan_amnt</code>, <code>dti</code>, and <code>credit_score</code> are now enforced.</p>
<div class="cell" data-execution_count="20">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1"></a><span class="bu">print</span>(<span class="st">"Generating PDP/ICE plots for the constrained model to verify monotonic behavior..."</span>)</span>
<span id="cb45-2"><a href="#cb45-2"></a><span class="co"># Extract feature lists from the constrained predictor</span></span>
<span id="cb45-3"><a href="#cb45-3"></a>all_features_constrained <span class="op">=</span> ag_model_constrained_wrapped.predictor.features()</span>
<span id="cb45-4"><a href="#cb45-4"></a>categorical_features_constrained <span class="op">=</span> ag_model_constrained_wrapped.predictor.feature_metadata.get_features(valid_raw_types<span class="op">=</span>[<span class="st">'category'</span>])</span>
<span id="cb45-5"><a href="#cb45-5"></a><span class="co"># Prepare DataFrame for plotting (drop label and weight)</span></span>
<span id="cb45-6"><a href="#cb45-6"></a>plot_df_constrained <span class="op">=</span> df_ttd_train.drop(columns<span class="op">=</span>[label, weight_col])</span>
<span id="cb45-7"><a href="#cb45-7"></a><span class="co"># Display PDP/ICE using the helper</span></span>
<span id="cb45-8"><a href="#cb45-8"></a>show_pdp(</span>
<span id="cb45-9"><a href="#cb45-9"></a>    wrappedAGModel<span class="op">=</span>ag_model_constrained_wrapped,</span>
<span id="cb45-10"><a href="#cb45-10"></a>    list_features<span class="op">=</span>all_features_constrained,</span>
<span id="cb45-11"><a href="#cb45-11"></a>    list_categ_features<span class="op">=</span>categorical_features_constrained <span class="cf">if</span> categorical_features_constrained <span class="cf">else</span> <span class="va">None</span>,</span>
<span id="cb45-12"><a href="#cb45-12"></a>    df<span class="op">=</span>plot_df_constrained,</span>
<span id="cb45-13"><a href="#cb45-13"></a>    show_ice<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb45-14"><a href="#cb45-14"></a>    sampSize<span class="op">=</span><span class="dv">25_000</span></span>
<span id="cb45-15"><a href="#cb45-15"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Generating PDP/ICE plots for the constrained model to verify monotonic behavior...</code></pre>
</div>
<div id="fig-pdp-ice-constrained" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="20">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pdp-ice-constrained-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<div id="fig-pdp-ice-constrained-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-pdp-ice-constrained-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Lab%2002_files/figure-html/fig-pdp-ice-constrained-output-2.png" data-ref-parent="fig-pdp-ice-constrained" width="687" height="400" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-pdp-ice-constrained-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) PDP/ICE plots for the constrained TTD model
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div id="fig-pdp-ice-constrained-2" class="quarto-float quarto-figure quarto-figure-center anchored" height="399" width="684">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-pdp-ice-constrained-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Lab%2002_files/figure-html/fig-pdp-ice-constrained-output-3.png" id="fig-pdp-ice-constrained-2" data-ref-parent="fig-pdp-ice-constrained" width="684" height="399" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-pdp-ice-constrained-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b)
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div id="fig-pdp-ice-constrained-3" class="quarto-float quarto-figure quarto-figure-center anchored" height="400" width="684">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-pdp-ice-constrained-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Lab%2002_files/figure-html/fig-pdp-ice-constrained-output-4.png" id="fig-pdp-ice-constrained-3" data-ref-parent="fig-pdp-ice-constrained" width="684" height="400" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-pdp-ice-constrained-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(c)
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div id="fig-pdp-ice-constrained-4" class="quarto-float quarto-figure quarto-figure-center anchored" height="400" width="684">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-pdp-ice-constrained-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Lab%2002_files/figure-html/fig-pdp-ice-constrained-output-5.png" id="fig-pdp-ice-constrained-4" data-ref-parent="fig-pdp-ice-constrained" width="684" height="400" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-pdp-ice-constrained-4-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(d)
</figcaption>
</figure>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-pdp-ice-constrained-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3
</figcaption>
</figure>
</div>
</div>
<p><strong>Interpretation:</strong> Carefully examine the PDP curves (red dashed lines) for <code>loan_amnt</code>, <code>dti</code>, and <code>credit_score</code>. They should now exhibit the enforced monotonic behavior: non-decreasing for <code>loan_amnt</code> and <code>dti</code>, and non-increasing for <code>credit_score</code>. If the curves are flat or follow the expected trend without reversals, the constraints have been successfully applied.</p>
</section>
<section id="evaluation-scoring-the-test-set" class="level1" data-number="10">
<h1 data-number="10"><span class="header-section-number">10</span> Evaluation &amp; Scoring the Test Set</h1>
<p><strong>Goal:</strong> Evaluate the performance of the final <em>constrained</em> model on the held-out TTD test set. Convert predicted PDs into a 3-digit scores and analyze the results using a KS table.</p>
<section id="prepare-test-set-and-evaluate" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="prepare-test-set-and-evaluate"><span class="header-section-number">10.1</span> Prepare Test Set and Evaluate</h2>
<p><strong>Goal:</strong> Apply the same feature processing steps used for the training data to the <em>accepted</em> and <em>rejected</em> test sets. For the rejected applicants, use the <code>ri_model</code> to estimate their reject inference weights. Combine the accepted and rejected test sets, ensuring that the <code>sample_weight</code> is set to 1 for accepted applicants and the rejected inference weights are used for rejected applicants.</p>
<div class="cell" data-execution_count="21">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1"></a><span class="bu">print</span>(<span class="st">"--- Preparing TTD Test Set ---"</span>)</span>
<span id="cb47-2"><a href="#cb47-2"></a></span>
<span id="cb47-3"><a href="#cb47-3"></a><span class="co"># 1. Process Accepted Test Data</span></span>
<span id="cb47-4"><a href="#cb47-4"></a><span class="bu">print</span>(<span class="st">"Processing accepted test data..."</span>)</span>
<span id="cb47-5"><a href="#cb47-5"></a>df_accepted_test_common <span class="op">=</span> process_lending_data(</span>
<span id="cb47-6"><a href="#cb47-6"></a>    df<span class="op">=</span>df_accepted_test,</span>
<span id="cb47-7"><a href="#cb47-7"></a>    source_type<span class="op">=</span><span class="st">'accepted'</span>,</span>
<span id="cb47-8"><a href="#cb47-8"></a>    fillMissing_loanamnt<span class="op">=</span>rejected_train_median_loan_amnt, <span class="co"># Use values derived from training</span></span>
<span id="cb47-9"><a href="#cb47-9"></a>    fillMissing_emp_length<span class="op">=</span>rejected_train_median_emp_length,</span>
<span id="cb47-10"><a href="#cb47-10"></a>    fillMissing_dti<span class="op">=</span>rejected_train_p90_dti,</span>
<span id="cb47-11"><a href="#cb47-11"></a>    fillMissing_credit_score<span class="op">=</span>rejected_train_p10_credit_score</span>
<span id="cb47-12"><a href="#cb47-12"></a>)</span>
<span id="cb47-13"><a href="#cb47-13"></a></span>
<span id="cb47-14"><a href="#cb47-14"></a><span class="co"># 2. Process Rejected Test Data</span></span>
<span id="cb47-15"><a href="#cb47-15"></a><span class="bu">print</span>(<span class="st">"Processing rejected test data..."</span>)</span>
<span id="cb47-16"><a href="#cb47-16"></a>df_rejected_test_common <span class="op">=</span> process_lending_data(</span>
<span id="cb47-17"><a href="#cb47-17"></a>    df<span class="op">=</span>df_rejected_test,</span>
<span id="cb47-18"><a href="#cb47-18"></a>    source_type<span class="op">=</span><span class="st">'rejected'</span>,</span>
<span id="cb47-19"><a href="#cb47-19"></a>    fillMissing_loanamnt<span class="op">=</span>rejected_train_median_loan_amnt, <span class="co"># Use values derived from training</span></span>
<span id="cb47-20"><a href="#cb47-20"></a>    fillMissing_emp_length<span class="op">=</span>rejected_train_median_emp_length,</span>
<span id="cb47-21"><a href="#cb47-21"></a>    fillMissing_dti<span class="op">=</span>rejected_train_p90_dti,</span>
<span id="cb47-22"><a href="#cb47-22"></a>    fillMissing_credit_score<span class="op">=</span>rejected_train_p10_credit_score</span>
<span id="cb47-23"><a href="#cb47-23"></a>)</span>
<span id="cb47-24"><a href="#cb47-24"></a></span>
<span id="cb47-25"><a href="#cb47-25"></a><span class="co"># 3. Create TTD Test Set using Fuzzy Augmentation</span></span>
<span id="cb47-26"><a href="#cb47-26"></a><span class="bu">print</span>(<span class="st">"Creating TTD test set using RI model and Fuzzy Augmentation..."</span>)</span>
<span id="cb47-27"><a href="#cb47-27"></a>df_ttd_test_full <span class="op">=</span> create_TTD_data(</span>
<span id="cb47-28"><a href="#cb47-28"></a>    ri_model<span class="op">=</span>ri_model,</span>
<span id="cb47-29"><a href="#cb47-29"></a>    df_rejected<span class="op">=</span>df_rejected_test_common,</span>
<span id="cb47-30"><a href="#cb47-30"></a>    df_accepted<span class="op">=</span>df_accepted_test_common,</span>
<span id="cb47-31"><a href="#cb47-31"></a>    ri_features<span class="op">=</span>ri_features, <span class="co"># Features used by ri_model</span></span>
<span id="cb47-32"><a href="#cb47-32"></a>    modeling_features<span class="op">=</span>modeling_features, <span class="co"># Features to keep in the final TTD set</span></span>
<span id="cb47-33"><a href="#cb47-33"></a>    target_col<span class="op">=</span>target_col</span>
<span id="cb47-34"><a href="#cb47-34"></a>)</span>
<span id="cb47-35"><a href="#cb47-35"></a></span>
<span id="cb47-36"><a href="#cb47-36"></a><span class="bu">print</span>(<span class="ss">f"TTD test set created. Shape: </span><span class="sc">{</span>df_ttd_test_full<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb47-37"><a href="#cb47-37"></a></span>
<span id="cb47-38"><a href="#cb47-38"></a><span class="co"># 4. Summarize the TTD Test Set by Source using helper function</span></span>
<span id="cb47-39"><a href="#cb47-39"></a>summary_default_rates_test <span class="op">=</span> summarize_ttd_by_source(</span>
<span id="cb47-40"><a href="#cb47-40"></a>    df_ttd<span class="op">=</span>df_ttd_test_full,</span>
<span id="cb47-41"><a href="#cb47-41"></a>    target_col<span class="op">=</span>target_col,</span>
<span id="cb47-42"><a href="#cb47-42"></a>    weight_col<span class="op">=</span><span class="st">'sample_weight'</span>,</span>
<span id="cb47-43"><a href="#cb47-43"></a>    source_col<span class="op">=</span><span class="st">'source'</span></span>
<span id="cb47-44"><a href="#cb47-44"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>--- Preparing TTD Test Set ---
Processing accepted test data...
Processing rejected test data...
Creating TTD test set using RI model and Fuzzy Augmentation...
Applying RI model to rejected data and calculating weights...
TTD dataset created. Shape: (652141, 7)
Sum of weights in TTD dataset: 552141.00
Source distribution:
source
Accepted    0.693318
Rejected    0.306682
Name: proportion, dtype: float64
TTD test set created. Shape: (652141, 7)

--- Summarizing TTD Data by 'source' ---
Summary of Default Rates by Source:</code></pre>
</div>
<div id="tbl-test-weighted-default-rates" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="21">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-test-weighted-default-rates-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;7: Weighted default rates for TTD Test Data
</figcaption>
<div aria-describedby="tbl-test-weighted-default-rates-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<div>


<table class="dataframe do-not-create-environment cell caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">source</th>
<th data-quarto-table-cell-role="th">row_count</th>
<th data-quarto-table-cell-role="th">unweighted_default_rate</th>
<th data-quarto-table-cell-role="th">sum_weights</th>
<th data-quarto-table-cell-role="th">weighted_default_rate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>Accepted</td>
<td>452141</td>
<td>0.128646</td>
<td>452141.0</td>
<td>0.128646</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>Rejected</td>
<td>200000</td>
<td>0.500000</td>
<td>100000.0</td>
<td>0.458358</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</figure>
</div>
</div>
<div id="weighted-auc" class="cell" data-execution_count="22">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1"></a><span class="co"># 5. Evaluate the Constrained Model on the TTD Test Set</span></span>
<span id="cb49-2"><a href="#cb49-2"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">--- Evaluating Constrained Model on TTD Test Set ---"</span>)</span>
<span id="cb49-3"><a href="#cb49-3"></a></span>
<span id="cb49-4"><a href="#cb49-4"></a><span class="co"># Prepare features for prediction - use the features the model was trained on</span></span>
<span id="cb49-5"><a href="#cb49-5"></a>X_test_ttd <span class="op">=</span> df_ttd_test_full[ag_model_constrained_wrapped.feature_names_] </span>
<span id="cb49-6"><a href="#cb49-6"></a>y_true_ttd <span class="op">=</span> df_ttd_test_full[<span class="st">'default_flag'</span>]</span>
<span id="cb49-7"><a href="#cb49-7"></a>weights_test_ttd <span class="op">=</span> df_ttd_test_full[<span class="st">'sample_weight'</span>]</span>
<span id="cb49-8"><a href="#cb49-8"></a></span>
<span id="cb49-9"><a href="#cb49-9"></a><span class="co"># Predict probabilities using the wrapped model</span></span>
<span id="cb49-10"><a href="#cb49-10"></a>pred_proba_ttd <span class="op">=</span> ag_model_constrained_wrapped.predict_proba(X_test_ttd)[:, <span class="dv">1</span>]</span>
<span id="cb49-11"><a href="#cb49-11"></a></span>
<span id="cb49-12"><a href="#cb49-12"></a><span class="co"># Create results DataFrame</span></span>
<span id="cb49-13"><a href="#cb49-13"></a>df_test_results <span class="op">=</span> df_ttd_test_full.copy()</span>
<span id="cb49-14"><a href="#cb49-14"></a>df_test_results[<span class="st">'pred_proba'</span>] <span class="op">=</span> pred_proba_ttd</span>
<span id="cb49-15"><a href="#cb49-15"></a></span>
<span id="cb49-16"><a href="#cb49-16"></a><span class="co"># Calculate weighted AUC for the entire TTD test set</span></span>
<span id="cb49-17"><a href="#cb49-17"></a>auc_ttd_weighted <span class="op">=</span> roc_auc_score(y_true_ttd, pred_proba_ttd, sample_weight<span class="op">=</span>weights_test_ttd)</span>
<span id="cb49-18"><a href="#cb49-18"></a><span class="bu">print</span>(<span class="ss">f"Weighted AUC on entire TTD Test Set: </span><span class="sc">{</span>auc_ttd_weighted<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb49-19"><a href="#cb49-19"></a></span>
<span id="cb49-20"><a href="#cb49-20"></a><span class="co"># Calculate unweighted AUC for the entire TTD test set</span></span>
<span id="cb49-21"><a href="#cb49-21"></a>auc_ttd_unweighted <span class="op">=</span> roc_auc_score(y_true_ttd, pred_proba_ttd)</span>
<span id="cb49-22"><a href="#cb49-22"></a><span class="bu">print</span>(<span class="ss">f"Unweighted AUC on entire TTD Test Set: </span><span class="sc">{</span>auc_ttd_unweighted<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
--- Evaluating Constrained Model on TTD Test Set ---
Weighted AUC on entire TTD Test Set: 0.7339
Unweighted AUC on entire TTD Test Set: 0.7349</code></pre>
</div>
</div>
</section>
<section id="convert-probability-pd-to-score" class="level2" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="convert-probability-pd-to-score"><span class="header-section-number">10.2</span> Convert Probability (PD) to Score</h2>
<p><strong>Concept:</strong> Convert the models predicted probability of default (PD) into a more intuitive 3-digit credit score (e.g., 300-850). Lower scores indicate higher risk.</p>
<p><strong>Formulas:</strong></p>
<p><span class="math inline">\(Score = BaseScore - Factor \cdot \ln(OddsBad)\)</span></p>
<p>where <span class="math inline">\(OddsBad = \frac{PD}{1 - PD}\)</span></p>
<p><span class="math inline">\(Factor = \frac{PDO}{\ln(2)}\)</span>.</p>
<div id="cell-fig-convert-pd-to-score" class="cell" data-execution_count="23">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1"></a>user_pdo <span class="op">=</span> <span class="dv">40</span></span>
<span id="cb51-2"><a href="#cb51-2"></a>user_basescore <span class="op">=</span> <span class="dv">680</span></span>
<span id="cb51-3"><a href="#cb51-3"></a></span>
<span id="cb51-4"><a href="#cb51-4"></a><span class="bu">print</span>(<span class="st">"Converting predicted probabilities to scores..."</span>)</span>
<span id="cb51-5"><a href="#cb51-5"></a><span class="co"># Use the defined helper function</span></span>
<span id="cb51-6"><a href="#cb51-6"></a>df_test_results[<span class="st">'score'</span>] <span class="op">=</span> calculate_score(</span>
<span id="cb51-7"><a href="#cb51-7"></a>    df_test_results[<span class="st">'pred_proba'</span>], </span>
<span id="cb51-8"><a href="#cb51-8"></a>    pdo<span class="op">=</span>user_pdo, </span>
<span id="cb51-9"><a href="#cb51-9"></a>    base_score<span class="op">=</span>user_basescore</span>
<span id="cb51-10"><a href="#cb51-10"></a>)</span>
<span id="cb51-11"><a href="#cb51-11"></a></span>
<span id="cb51-12"><a href="#cb51-12"></a><span class="bu">print</span>(<span class="st">"Scores calculated and added to test results."</span>)</span>
<span id="cb51-13"><a href="#cb51-13"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Score Distribution Summary:"</span>)</span>
<span id="cb51-14"><a href="#cb51-14"></a><span class="bu">print</span>(df_test_results[<span class="st">'score'</span>].describe())</span>
<span id="cb51-15"><a href="#cb51-15"></a></span>
<span id="cb51-16"><a href="#cb51-16"></a><span class="co"># Plot score distribution</span></span>
<span id="cb51-17"><a href="#cb51-17"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb51-18"><a href="#cb51-18"></a>sns.histplot(df_test_results[<span class="st">'score'</span>], bins<span class="op">=</span><span class="dv">50</span>, kde<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb51-19"><a href="#cb51-19"></a>plt.title(<span class="st">'Distribution of Calculated Scores (TTD Test Set)'</span>)</span>
<span id="cb51-20"><a href="#cb51-20"></a>plt.xlabel(<span class="st">'Score'</span>)</span>
<span id="cb51-21"><a href="#cb51-21"></a>plt.ylabel(<span class="st">'Frequency'</span>)</span>
<span id="cb51-22"><a href="#cb51-22"></a>plt.grid(axis<span class="op">=</span><span class="st">'y'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb51-23"><a href="#cb51-23"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Converting predicted probabilities to scores...
Scores calculated and added to test results.

Score Distribution Summary:
count    652141.000000
mean        761.681345
std          51.329953
min         600.000000
25%         729.000000
50%         778.000000
75%         797.000000
max         835.000000
Name: score, dtype: float64</code></pre>
</div>
<div class="cell-output cell-output-display">
<div id="fig-convert-pd-to-score" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-convert-pd-to-score-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Lab%2002_files/figure-html/fig-convert-pd-to-score-output-2.png" width="867" height="473" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-convert-pd-to-score-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Distribution of Calculated Scores (TTD Test Set)
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="build-ks-table" class="level2" data-number="10.3">
<h2 data-number="10.3" class="anchored" data-anchor-id="build-ks-table"><span class="header-section-number">10.3</span> Build KS Table</h2>
<p><strong>Concept:</strong> In credit scorecards, the Kolmogorov-Smirnov (KS) statistic measures how well the scorecard separates goods (non-defaults) from bads (defaults). It quantifies the maximum difference between the cumulative distribution functions of the scores for the good and bad populations across different score ranges (bins). A higher KS value indicates better separation power of the scorecard.</p>
<p>A model with high KS would generate high PD predictions for defaulted loans and low PD predictions for non-defaulted loans. Ideally, the PD predictions from the two target classes should not overlap. While perfect separation is rarely achieved in practice, a high KS signifies the model is closer to this ideal.</p>
<div class="cell" data-execution_count="24">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1"></a><span class="bu">print</span>(<span class="st">"Generating KS table using scores and sample weights..."</span>)</span>
<span id="cb53-2"><a href="#cb53-2"></a>ks_results_table <span class="op">=</span> ks_table(</span>
<span id="cb53-3"><a href="#cb53-3"></a>    data<span class="op">=</span>df_test_results,</span>
<span id="cb53-4"><a href="#cb53-4"></a>    y_true_col<span class="op">=</span><span class="st">'default_flag'</span>,</span>
<span id="cb53-5"><a href="#cb53-5"></a>    y_pred_col<span class="op">=</span><span class="st">'score'</span>,</span>
<span id="cb53-6"><a href="#cb53-6"></a>    n_bins<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb53-7"><a href="#cb53-7"></a>    is_score<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb53-8"><a href="#cb53-8"></a>    sample_weight_col<span class="op">=</span><span class="st">'sample_weight'</span></span>
<span id="cb53-9"><a href="#cb53-9"></a>)</span>
<span id="cb53-10"><a href="#cb53-10"></a>display(ks_results_table)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Generating KS table using scores and sample weights...
KS Statistic (Max KS): 34.7001
  Occurs in bin with score range: [765.0000 - 770.0000]
  Average score in this bin: 767.3271</code></pre>
</div>
<div id="tbl-build-ks-table" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="24">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-build-ks-table-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;8: KS Table for Constrained Model on TTD Test Set
</figcaption>
<div aria-describedby="tbl-build-ks-table-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<div>


<table class="dataframe do-not-create-environment cell caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">min_score</th>
<th data-quarto-table-cell-role="th">max_score</th>
<th data-quarto-table-cell-role="th">avg_score</th>
<th data-quarto-table-cell-role="th">count</th>
<th data-quarto-table-cell-role="th">bads</th>
<th data-quarto-table-cell-role="th">goods</th>
<th data-quarto-table-cell-role="th">bad_rate</th>
<th data-quarto-table-cell-role="th">cum_bads_pct</th>
<th data-quarto-table-cell-role="th">cum_goods_pct</th>
<th data-quarto-table-cell-role="th">ks</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>600</td>
<td>673</td>
<td>658.136531</td>
<td>16315.055037</td>
<td>10647.233302</td>
<td>5667.821735</td>
<td>0.652602</td>
<td>10.237546</td>
<td>1.264746</td>
<td>8.972801e+00</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>673</td>
<td>680</td>
<td>676.834238</td>
<td>16590.577015</td>
<td>10645.195248</td>
<td>5945.381766</td>
<td>0.641641</td>
<td>20.473133</td>
<td>2.591428</td>
<td>1.788171e+01</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>680</td>
<td>685</td>
<td>682.584660</td>
<td>15921.508600</td>
<td>7320.385827</td>
<td>8601.122773</td>
<td>0.459780</td>
<td>27.511843</td>
<td>4.510725</td>
<td>2.300112e+01</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>685</td>
<td>685</td>
<td>685.000000</td>
<td>16325.381303</td>
<td>7476.039896</td>
<td>8849.341407</td>
<td>0.457940</td>
<td>34.700218</td>
<td>6.485411</td>
<td>2.821481e+01</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>685</td>
<td>729</td>
<td>701.876468</td>
<td>16368.166721</td>
<td>6077.182292</td>
<td>10290.984430</td>
<td>0.371281</td>
<td>40.543561</td>
<td>8.781792</td>
<td>3.176177e+01</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>729</td>
<td>756</td>
<td>746.633545</td>
<td>24588.311324</td>
<td>6303.525704</td>
<td>18284.785620</td>
<td>0.256363</td>
<td>46.604539</td>
<td>12.861950</td>
<td>3.374259e+01</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>756</td>
<td>765</td>
<td>760.707241</td>
<td>30620.000000</td>
<td>6548.463954</td>
<td>24071.536046</td>
<td>0.213862</td>
<td>52.901029</td>
<td>18.233392</td>
<td>3.466764e+01</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>765</td>
<td>770</td>
<td>767.327108</td>
<td>31709.000000</td>
<td>6000.155091</td>
<td>25708.844909</td>
<td>0.189226</td>
<td>58.670309</td>
<td>23.970191</td>
<td>3.470012e+01</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">8</td>
<td>770</td>
<td>773</td>
<td>771.828319</td>
<td>32033.000000</td>
<td>5502.815389</td>
<td>26530.184611</td>
<td>0.171786</td>
<td>63.961385</td>
<td>29.890267</td>
<td>3.407112e+01</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">9</td>
<td>773</td>
<td>778</td>
<td>775.741528</td>
<td>31700.000000</td>
<td>5021.498933</td>
<td>26678.501067</td>
<td>0.158407</td>
<td>68.789666</td>
<td>35.843440</td>
<td>3.294623e+01</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">10</td>
<td>778</td>
<td>781</td>
<td>779.639311</td>
<td>32002.000000</td>
<td>4718.074400</td>
<td>27283.925600</td>
<td>0.147431</td>
<td>73.326197</td>
<td>41.931711</td>
<td>3.139449e+01</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">11</td>
<td>781</td>
<td>785</td>
<td>782.908026</td>
<td>31930.000000</td>
<td>4133.676427</td>
<td>27796.323573</td>
<td>0.129461</td>
<td>77.300817</td>
<td>48.134320</td>
<td>2.916650e+01</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">12</td>
<td>785</td>
<td>788</td>
<td>786.474959</td>
<td>32046.000000</td>
<td>4076.962849</td>
<td>27969.037151</td>
<td>0.127222</td>
<td>81.220905</td>
<td>54.375470</td>
<td>2.684544e+01</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">13</td>
<td>788</td>
<td>792</td>
<td>789.919680</td>
<td>32146.000000</td>
<td>3767.363955</td>
<td>28378.636045</td>
<td>0.117195</td>
<td>84.843308</td>
<td>60.708019</td>
<td>2.413529e+01</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">14</td>
<td>792</td>
<td>797</td>
<td>794.678167</td>
<td>32011.000000</td>
<td>3512.404412</td>
<td>28498.595588</td>
<td>0.109725</td>
<td>88.220561</td>
<td>67.067337</td>
<td>2.115322e+01</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">15</td>
<td>797</td>
<td>803</td>
<td>799.794645</td>
<td>32025.000000</td>
<td>3278.462576</td>
<td>28746.537424</td>
<td>0.102372</td>
<td>91.372874</td>
<td>73.481982</td>
<td>1.789089e+01</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">16</td>
<td>803</td>
<td>811</td>
<td>806.612016</td>
<td>31992.000000</td>
<td>2861.198181</td>
<td>29130.801819</td>
<td>0.089435</td>
<td>94.123978</td>
<td>79.982373</td>
<td>1.414161e+01</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">17</td>
<td>811</td>
<td>818</td>
<td>814.107738</td>
<td>31727.000000</td>
<td>2467.100371</td>
<td>29259.899629</td>
<td>0.077760</td>
<td>96.496149</td>
<td>86.511572</td>
<td>9.984577e+00</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">18</td>
<td>818</td>
<td>827</td>
<td>822.466066</td>
<td>31986.000000</td>
<td>2088.373678</td>
<td>29897.626322</td>
<td>0.065290</td>
<td>98.504166</td>
<td>93.183077</td>
<td>5.321089e+00</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">19</td>
<td>827</td>
<td>835</td>
<td>830.359831</td>
<td>32105.000000</td>
<td>1555.694689</td>
<td>30549.305311</td>
<td>0.048456</td>
<td>100.000000</td>
<td>100.000000</td>
<td>4.263256e-14</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</figure>
</div>
</div>
<p>Plot cumulative bad rates and good rates to visualize the KS statistic.</p>
<div id="cell-fig-cumulative-rates" class="cell" data-execution_count="25">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1"></a><span class="bu">print</span>(<span class="st">"Plotting cumulative bad rates and good rates to visualize the KS statistic..."</span>)</span>
<span id="cb55-2"><a href="#cb55-2"></a></span>
<span id="cb55-3"><a href="#cb55-3"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb55-4"><a href="#cb55-4"></a>plt.plot(ks_results_table[<span class="st">'cum_bads_pct'</span>], label<span class="op">=</span><span class="st">'Cumulative Bad Rate (%)'</span>, color<span class="op">=</span><span class="st">'red'</span>, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb55-5"><a href="#cb55-5"></a>plt.plot(ks_results_table[<span class="st">'cum_goods_pct'</span>], label<span class="op">=</span><span class="st">'Cumulative Good Rate (%)'</span>, color<span class="op">=</span><span class="st">'blue'</span>, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb55-6"><a href="#cb55-6"></a>plt.fill_between(<span class="bu">range</span>(<span class="bu">len</span>(ks_results_table)), </span>
<span id="cb55-7"><a href="#cb55-7"></a>                    ks_results_table[<span class="st">'cum_bads_pct'</span>], </span>
<span id="cb55-8"><a href="#cb55-8"></a>                    ks_results_table[<span class="st">'cum_goods_pct'</span>], </span>
<span id="cb55-9"><a href="#cb55-9"></a>                    color<span class="op">=</span><span class="st">'gray'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, label<span class="op">=</span><span class="st">'KS Gap'</span>)</span>
<span id="cb55-10"><a href="#cb55-10"></a></span>
<span id="cb55-11"><a href="#cb55-11"></a><span class="co"># Highlight the maximum KS point</span></span>
<span id="cb55-12"><a href="#cb55-12"></a>max_ks_idx <span class="op">=</span> ks_results_table[<span class="st">'ks'</span>].idxmax()</span>
<span id="cb55-13"><a href="#cb55-13"></a>max_ks_value <span class="op">=</span> ks_results_table.loc[max_ks_idx, <span class="st">'ks'</span>]</span>
<span id="cb55-14"><a href="#cb55-14"></a>plt.axvline(x<span class="op">=</span>max_ks_idx, color<span class="op">=</span><span class="st">'green'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="ss">f'Max KS = </span><span class="sc">{</span>max_ks_value<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb55-15"><a href="#cb55-15"></a></span>
<span id="cb55-16"><a href="#cb55-16"></a><span class="co"># Set x-ticks to average score from each bin</span></span>
<span id="cb55-17"><a href="#cb55-17"></a>y_pred_col_used <span class="op">=</span> <span class="st">'score'</span> <span class="co"># The column used in ks_table call</span></span>
<span id="cb55-18"><a href="#cb55-18"></a>avg_score_col_name <span class="op">=</span> <span class="ss">f'avg_</span><span class="sc">{</span>y_pred_col_used<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb55-19"><a href="#cb55-19"></a></span>
<span id="cb55-20"><a href="#cb55-20"></a><span class="cf">if</span> avg_score_col_name <span class="kw">in</span> ks_results_table.columns:</span>
<span id="cb55-21"><a href="#cb55-21"></a>    avg_scores <span class="op">=</span> ks_results_table[avg_score_col_name]</span>
<span id="cb55-22"><a href="#cb55-22"></a>    plt.xticks(ticks<span class="op">=</span><span class="bu">range</span>(<span class="bu">len</span>(avg_scores)), labels<span class="op">=</span>[<span class="ss">f"</span><span class="sc">{</span>score<span class="sc">:.0f}</span><span class="ss">"</span> <span class="cf">for</span> score <span class="kw">in</span> avg_scores], rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb55-23"><a href="#cb55-23"></a>    plt.xlabel(<span class="ss">f'Average </span><span class="sc">{</span>y_pred_col_used<span class="sc">.</span>capitalize()<span class="sc">}</span><span class="ss"> (per Bin)'</span>) <span class="co"># Dynamic label</span></span>
<span id="cb55-24"><a href="#cb55-24"></a><span class="cf">else</span>:</span>
<span id="cb55-25"><a href="#cb55-25"></a>    <span class="bu">print</span>(<span class="ss">f"Warning: Column '</span><span class="sc">{</span>avg_score_col_name<span class="sc">}</span><span class="ss">' not found in KS table for x-axis labels."</span>)</span>
<span id="cb55-26"><a href="#cb55-26"></a>    plt.xlabel(<span class="st">'Bin Index'</span>) <span class="co"># Fallback label</span></span>
<span id="cb55-27"><a href="#cb55-27"></a></span>
<span id="cb55-28"><a href="#cb55-28"></a>plt.title(<span class="st">'Cumulative Bad and Good Rates with KS Statistic'</span>)</span>
<span id="cb55-29"><a href="#cb55-29"></a>plt.ylabel(<span class="st">'Cumulative Percentage (%)'</span>)</span>
<span id="cb55-30"><a href="#cb55-30"></a>plt.legend(loc<span class="op">=</span><span class="st">'best'</span>)</span>
<span id="cb55-31"><a href="#cb55-31"></a>plt.grid(alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb55-32"><a href="#cb55-32"></a>plt.tight_layout()</span>
<span id="cb55-33"><a href="#cb55-33"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Plotting cumulative bad rates and good rates to visualize the KS statistic...</code></pre>
</div>
<div class="cell-output cell-output-display">
<div id="fig-cumulative-rates" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-cumulative-rates-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Lab%2002_files/figure-html/fig-cumulative-rates-output-2.png" width="934" height="551" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-cumulative-rates-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: KS Plot: Cumulative Bad and Good Rates
</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>Interpretation:</strong> A KS &gt; 30 is often acceptable, &gt; 40 good, &gt; 50 excellent. These ranges are general guidelines often cited in the industry; the acceptable KS level depends heavily on the specific business context and application.</p>
</section>
</section>
<section id="decision-threshold-selection" class="level1" data-number="11">
<h1 data-number="11"><span class="header-section-number">11</span> Decision Threshold Selection</h1>
<section id="prepare-calibration-data-for-thresholding" class="level2" data-number="11.1">
<h2 data-number="11.1" class="anchored" data-anchor-id="prepare-calibration-data-for-thresholding"><span class="header-section-number">11.1</span> Prepare Calibration Data for Thresholding</h2>
<p>There are multiple methods to estimate the optimal decision threshold for binary classification. We explored some of those methods in previous lectures and labs:</p>
<ol type="1">
<li>Maximizing F1 score</li>
<li>Maximizing Profit</li>
</ol>
<p>In this lab, we will use the score associated with the KS statistic.</p>
<p>We will use the calibration data set to estimate the threshold.</p>
<p>First, we need to construct the TTD data for the calibration data.</p>
<div class="cell" data-execution_count="26">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1"></a>df_accepted_calib_common <span class="op">=</span> process_lending_data(</span>
<span id="cb57-2"><a href="#cb57-2"></a>    df<span class="op">=</span>df_accepted_calib,</span>
<span id="cb57-3"><a href="#cb57-3"></a>    source_type<span class="op">=</span><span class="st">'accepted'</span>,</span>
<span id="cb57-4"><a href="#cb57-4"></a>    fillMissing_loanamnt<span class="op">=</span>rejected_train_median_loan_amnt, <span class="co"># Use values derived from training</span></span>
<span id="cb57-5"><a href="#cb57-5"></a>    fillMissing_emp_length<span class="op">=</span>rejected_train_median_emp_length,</span>
<span id="cb57-6"><a href="#cb57-6"></a>    fillMissing_dti<span class="op">=</span>rejected_train_p90_dti,</span>
<span id="cb57-7"><a href="#cb57-7"></a>    fillMissing_credit_score<span class="op">=</span>rejected_train_p10_credit_score</span>
<span id="cb57-8"><a href="#cb57-8"></a>)</span>
<span id="cb57-9"><a href="#cb57-9"></a></span>
<span id="cb57-10"><a href="#cb57-10"></a>df_rejected_calib_common <span class="op">=</span> process_lending_data(</span>
<span id="cb57-11"><a href="#cb57-11"></a>    df<span class="op">=</span>df_rejected_calib,</span>
<span id="cb57-12"><a href="#cb57-12"></a>    source_type<span class="op">=</span><span class="st">'rejected'</span>,</span>
<span id="cb57-13"><a href="#cb57-13"></a>    fillMissing_loanamnt<span class="op">=</span>rejected_train_median_loan_amnt, <span class="co"># Use values derived from training</span></span>
<span id="cb57-14"><a href="#cb57-14"></a>    fillMissing_emp_length<span class="op">=</span>rejected_train_median_emp_length,</span>
<span id="cb57-15"><a href="#cb57-15"></a>    fillMissing_dti<span class="op">=</span>rejected_train_p90_dti,</span>
<span id="cb57-16"><a href="#cb57-16"></a>    fillMissing_credit_score<span class="op">=</span>rejected_train_p10_credit_score</span>
<span id="cb57-17"><a href="#cb57-17"></a>)</span>
<span id="cb57-18"><a href="#cb57-18"></a></span>
<span id="cb57-19"><a href="#cb57-19"></a><span class="co"># Create TTD Calibration Set using Fuzzy Augmentation</span></span>
<span id="cb57-20"><a href="#cb57-20"></a><span class="bu">print</span>(<span class="st">"Creating TTD calibration set using RI model and Fuzzy Augmentation..."</span>)</span>
<span id="cb57-21"><a href="#cb57-21"></a>df_ttd_calib_full <span class="op">=</span> create_TTD_data(</span>
<span id="cb57-22"><a href="#cb57-22"></a>    ri_model<span class="op">=</span>ri_model,</span>
<span id="cb57-23"><a href="#cb57-23"></a>    df_rejected<span class="op">=</span>df_rejected_calib_common,</span>
<span id="cb57-24"><a href="#cb57-24"></a>    df_accepted<span class="op">=</span>df_accepted_calib_common,</span>
<span id="cb57-25"><a href="#cb57-25"></a>    ri_features<span class="op">=</span>ri_features, <span class="co"># Features used by ri_model</span></span>
<span id="cb57-26"><a href="#cb57-26"></a>    modeling_features<span class="op">=</span>modeling_features, <span class="co"># Features to keep in the final TTD set</span></span>
<span id="cb57-27"><a href="#cb57-27"></a>    target_col<span class="op">=</span>target_col</span>
<span id="cb57-28"><a href="#cb57-28"></a>)</span>
<span id="cb57-29"><a href="#cb57-29"></a><span class="bu">print</span>(<span class="ss">f"TTD calibration set created. Shape: </span><span class="sc">{</span>df_ttd_calib_full<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb57-30"><a href="#cb57-30"></a><span class="co"># Summarize the TTD Calibration Set by Source using helper function</span></span>
<span id="cb57-31"><a href="#cb57-31"></a>summary_default_rates_calib <span class="op">=</span> summarize_ttd_by_source(</span>
<span id="cb57-32"><a href="#cb57-32"></a>    df_ttd<span class="op">=</span>df_ttd_calib_full,</span>
<span id="cb57-33"><a href="#cb57-33"></a>    target_col<span class="op">=</span>target_col,</span>
<span id="cb57-34"><a href="#cb57-34"></a>    weight_col<span class="op">=</span><span class="st">'sample_weight'</span>,</span>
<span id="cb57-35"><a href="#cb57-35"></a>    source_col<span class="op">=</span><span class="st">'source'</span></span>
<span id="cb57-36"><a href="#cb57-36"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Creating TTD calibration set using RI model and Fuzzy Augmentation...
Applying RI model to rejected data and calculating weights...
TTD dataset created. Shape: (652140, 7)
Sum of weights in TTD dataset: 552140.00
Source distribution:
source
Accepted    0.693317
Rejected    0.306683
Name: proportion, dtype: float64
TTD calibration set created. Shape: (652140, 7)

--- Summarizing TTD Data by 'source' ---
Summary of Default Rates by Source:</code></pre>
</div>
<div id="tbl-ttd-calibration" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="26">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-ttd-calibration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;9: Weighted default rates for TTD Calibration Data
</figcaption>
<div aria-describedby="tbl-ttd-calibration-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<div>


<table class="dataframe do-not-create-environment cell caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">source</th>
<th data-quarto-table-cell-role="th">row_count</th>
<th data-quarto-table-cell-role="th">unweighted_default_rate</th>
<th data-quarto-table-cell-role="th">sum_weights</th>
<th data-quarto-table-cell-role="th">weighted_default_rate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>Accepted</td>
<td>452140</td>
<td>0.128644</td>
<td>452140.0</td>
<td>0.128644</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>Rejected</td>
<td>200000</td>
<td>0.500000</td>
<td>100000.0</td>
<td>0.459152</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</figure>
</div>
</div>
</section>
<section id="determine-score-threshold-using-ks-table" class="level2" data-number="11.2">
<h2 data-number="11.2" class="anchored" data-anchor-id="determine-score-threshold-using-ks-table"><span class="header-section-number">11.2</span> Determine Score Threshold using KS Table</h2>
<p>Second, predict each applicants PD and convert to a score.</p>
<div id="calculate-scores-calib" class="cell" data-execution_count="27">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1"></a>df_ttd_calib_full[<span class="st">'pred_proba'</span>] <span class="op">=</span> ag_model_constrained_wrapped.predict_proba(</span>
<span id="cb59-2"><a href="#cb59-2"></a>    df_ttd_calib_full[ag_model_constrained_wrapped.feature_names_]</span>
<span id="cb59-3"><a href="#cb59-3"></a>)[:, <span class="dv">1</span>]</span>
<span id="cb59-4"><a href="#cb59-4"></a></span>
<span id="cb59-5"><a href="#cb59-5"></a>df_ttd_calib_full[<span class="st">'score'</span>] <span class="op">=</span> calculate_score(</span>
<span id="cb59-6"><a href="#cb59-6"></a>    df_ttd_calib_full[<span class="st">'pred_proba'</span>], </span>
<span id="cb59-7"><a href="#cb59-7"></a>    pdo<span class="op">=</span>user_pdo, </span>
<span id="cb59-8"><a href="#cb59-8"></a>    base_score<span class="op">=</span>user_basescore</span>
<span id="cb59-9"><a href="#cb59-9"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Finally, find the optimal threshold using the KS statistic.</p>
<div class="cell" data-execution_count="28">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1"></a>ks_calibration <span class="op">=</span> ks_table(</span>
<span id="cb60-2"><a href="#cb60-2"></a>    data<span class="op">=</span>df_ttd_calib_full,</span>
<span id="cb60-3"><a href="#cb60-3"></a>    y_true_col<span class="op">=</span><span class="st">'default_flag'</span>,</span>
<span id="cb60-4"><a href="#cb60-4"></a>    y_pred_col<span class="op">=</span><span class="st">'score'</span>,</span>
<span id="cb60-5"><a href="#cb60-5"></a>    n_bins<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb60-6"><a href="#cb60-6"></a>    is_score<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb60-7"><a href="#cb60-7"></a>    sample_weight_col<span class="op">=</span><span class="st">'sample_weight'</span></span>
<span id="cb60-8"><a href="#cb60-8"></a>)</span>
<span id="cb60-9"><a href="#cb60-9"></a></span>
<span id="cb60-10"><a href="#cb60-10"></a>display(ks_calibration)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>KS Statistic (Max KS): 34.4992
  Occurs in bin with score range: [765.0000 - 769.0000]
  Average score in this bin: 767.2254</code></pre>
</div>
<div id="tbl-ks-table-calib" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="28">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-ks-table-calib-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;10: KS Table for TTD Calibration Data
</figcaption>
<div aria-describedby="tbl-ks-table-calib-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<div>


<table class="dataframe do-not-create-environment cell caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">min_score</th>
<th data-quarto-table-cell-role="th">max_score</th>
<th data-quarto-table-cell-role="th">avg_score</th>
<th data-quarto-table-cell-role="th">count</th>
<th data-quarto-table-cell-role="th">bads</th>
<th data-quarto-table-cell-role="th">goods</th>
<th data-quarto-table-cell-role="th">bad_rate</th>
<th data-quarto-table-cell-role="th">cum_bads_pct</th>
<th data-quarto-table-cell-role="th">cum_goods_pct</th>
<th data-quarto-table-cell-role="th">ks</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>603</td>
<td>672</td>
<td>657.735762</td>
<td>16329.755492</td>
<td>10729.224945</td>
<td>5600.530547</td>
<td>0.657035</td>
<td>10.308608</td>
<td>1.249952</td>
<td>9.058657</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>672</td>
<td>680</td>
<td>676.687398</td>
<td>16566.830802</td>
<td>10594.445936</td>
<td>5972.384866</td>
<td>0.639497</td>
<td>20.487722</td>
<td>2.582896</td>
<td>17.904826</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>680</td>
<td>685</td>
<td>682.462079</td>
<td>15948.080755</td>
<td>7181.347606</td>
<td>8766.733150</td>
<td>0.450295</td>
<td>27.387540</td>
<td>4.539495</td>
<td>22.848045</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>685</td>
<td>685</td>
<td>685.000000</td>
<td>16293.509047</td>
<td>7767.658946</td>
<td>8525.850102</td>
<td>0.476733</td>
<td>34.850684</td>
<td>6.442332</td>
<td>28.408352</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>685</td>
<td>729</td>
<td>701.741252</td>
<td>16369.852938</td>
<td>5964.687482</td>
<td>10405.165455</td>
<td>0.364370</td>
<td>40.581539</td>
<td>8.764604</td>
<td>31.816934</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>729</td>
<td>756</td>
<td>746.484681</td>
<td>24485.970965</td>
<td>6194.294710</td>
<td>18291.676255</td>
<td>0.252973</td>
<td>46.532999</td>
<td>12.847023</td>
<td>33.685976</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>756</td>
<td>765</td>
<td>760.537952</td>
<td>30700.000000</td>
<td>6426.643379</td>
<td>24273.356621</td>
<td>0.209337</td>
<td>52.707700</td>
<td>18.264461</td>
<td>34.443239</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>765</td>
<td>769</td>
<td>767.225412</td>
<td>31826.000000</td>
<td>6046.530782</td>
<td>25779.469218</td>
<td>0.189987</td>
<td>58.517189</td>
<td>24.018039</td>
<td>34.499150</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">8</td>
<td>769</td>
<td>773</td>
<td>771.738338</td>
<td>31974.000000</td>
<td>5517.138438</td>
<td>26456.861562</td>
<td>0.172551</td>
<td>63.818040</td>
<td>29.922801</td>
<td>33.895239</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">9</td>
<td>773</td>
<td>778</td>
<td>775.610973</td>
<td>31718.000000</td>
<td>5012.559793</td>
<td>26705.440207</td>
<td>0.158035</td>
<td>68.634093</td>
<td>35.883042</td>
<td>32.751051</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">10</td>
<td>778</td>
<td>781</td>
<td>779.531849</td>
<td>31992.000000</td>
<td>4787.346858</td>
<td>27204.653142</td>
<td>0.149642</td>
<td>73.233762</td>
<td>41.954699</td>
<td>31.279063</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">11</td>
<td>781</td>
<td>785</td>
<td>782.771123</td>
<td>31930.000000</td>
<td>4123.017192</td>
<td>27806.982808</td>
<td>0.129127</td>
<td>77.195145</td>
<td>48.160787</td>
<td>29.034358</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">12</td>
<td>785</td>
<td>788</td>
<td>786.393504</td>
<td>32063.000000</td>
<td>4174.850924</td>
<td>27888.149076</td>
<td>0.130208</td>
<td>81.206330</td>
<td>54.384990</td>
<td>26.821340</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">13</td>
<td>788</td>
<td>792</td>
<td>789.821081</td>
<td>32081.000000</td>
<td>3686.513528</td>
<td>28394.486472</td>
<td>0.114913</td>
<td>84.748321</td>
<td>60.722200</td>
<td>24.026122</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">14</td>
<td>792</td>
<td>797</td>
<td>794.516730</td>
<td>31974.000000</td>
<td>3584.373350</td>
<td>28389.626650</td>
<td>0.112103</td>
<td>88.192177</td>
<td>67.058325</td>
<td>21.133852</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">15</td>
<td>797</td>
<td>803</td>
<td>799.625479</td>
<td>31995.000000</td>
<td>3230.779937</td>
<td>28764.220063</td>
<td>0.100978</td>
<td>91.296301</td>
<td>73.478053</td>
<td>17.818248</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">16</td>
<td>803</td>
<td>810</td>
<td>806.454964</td>
<td>32116.000000</td>
<td>2946.768176</td>
<td>29169.231824</td>
<td>0.091754</td>
<td>94.127547</td>
<td>79.988174</td>
<td>14.139373</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">17</td>
<td>810</td>
<td>818</td>
<td>814.012451</td>
<td>31690.000000</td>
<td>2484.635885</td>
<td>29205.364115</td>
<td>0.078404</td>
<td>96.514778</td>
<td>86.506359</td>
<td>10.008419</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">18</td>
<td>818</td>
<td>827</td>
<td>822.451253</td>
<td>31986.000000</td>
<td>2101.806890</td>
<td>29884.193110</td>
<td>0.065710</td>
<td>98.534188</td>
<td>93.176049</td>
<td>5.358140</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">19</td>
<td>827</td>
<td>835</td>
<td>830.360873</td>
<td>32101.000000</td>
<td>1525.620316</td>
<td>30575.379684</td>
<td>0.047526</td>
<td>100.000000</td>
<td>100.000000</td>
<td>0.000000</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</figure>
</div>
</div>
<p>Based on the KS table for the calibration set, the maximum KS occurs in the bin starting at score 767. We will select 767 as our decision threshold (approve if score &gt;= 767).</p>
<p>In order to simplify model predictions, we will convert the score of 767 into a probability and set the threshold inside AutoGluon.</p>
<div id="set-predict-threshold" class="cell" data-execution_count="29">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1"></a>score_threshold <span class="op">=</span> <span class="dv">767</span></span>
<span id="cb62-2"><a href="#cb62-2"></a></span>
<span id="cb62-3"><a href="#cb62-3"></a>PD_threshold <span class="op">=</span> score_to_probability(</span>
<span id="cb62-4"><a href="#cb62-4"></a>    score<span class="op">=</span>score_threshold, </span>
<span id="cb62-5"><a href="#cb62-5"></a>    pdo<span class="op">=</span>user_pdo, </span>
<span id="cb62-6"><a href="#cb62-6"></a>    base_score<span class="op">=</span>user_basescore</span>
<span id="cb62-7"><a href="#cb62-7"></a>)</span>
<span id="cb62-8"><a href="#cb62-8"></a></span>
<span id="cb62-9"><a href="#cb62-9"></a>ag_model_constrained_wrapped.predictor.set_decision_threshold(PD_threshold)</span>
<span id="cb62-10"><a href="#cb62-10"></a></span>
<span id="cb62-11"><a href="#cb62-11"></a>ag_model_constrained_wrapped.predictor.save() <span class="co"># Save the model with the new threshold</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="evaluate-model-decisions" class="level2" data-number="11.3">
<h2 data-number="11.3" class="anchored" data-anchor-id="evaluate-model-decisions"><span class="header-section-number">11.3</span> Evaluate Model Decisions</h2>
<p>Now lets evaluate the threshold using the test set in a way that matches how the model would be used in production.</p>
<p>During training and calibration, we used sample weights and cloned the rejected applicants to correct for selection bias (since we dont know their true outcomes). However, when scoring <strong>new</strong> applicants, we do not know their outcomes and do not use sample weights or cloning.</p>
<p>Therefore, to simulate production scoring, we will evaluate the model on the test set by setting all sample weights to 1 and including each applicant only once (no cloning of rejects).</p>
<p>Note that this evaluation differs from the weighted KS calculation in Section 8. There, we assessed the models ability to separate goods and bads according to the weighted distribution it was trained on. Here, we are simulating the practical application of the chosen score threshold (767) to new, individual applicants (without weighting or cloning) to see the resulting approval/rejection rates.</p>
<div class="cell" data-execution_count="30">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1"></a>df_ttd_test_noCloning <span class="op">=</span> create_TTD_data(</span>
<span id="cb63-2"><a href="#cb63-2"></a>    ri_model<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb63-3"><a href="#cb63-3"></a>    df_rejected<span class="op">=</span>df_rejected_test_common,</span>
<span id="cb63-4"><a href="#cb63-4"></a>    df_accepted<span class="op">=</span>df_accepted_test_common,</span>
<span id="cb63-5"><a href="#cb63-5"></a>    ri_features<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb63-6"><a href="#cb63-6"></a>    modeling_features<span class="op">=</span>modeling_features,</span>
<span id="cb63-7"><a href="#cb63-7"></a>    target_col<span class="op">=</span>target_col,</span>
<span id="cb63-8"><a href="#cb63-8"></a>    clone_rejected<span class="op">=</span><span class="va">False</span></span>
<span id="cb63-9"><a href="#cb63-9"></a>)</span>
<span id="cb63-10"><a href="#cb63-10"></a></span>
<span id="cb63-11"><a href="#cb63-11"></a>display(df_ttd_test_noCloning.sample(<span class="dv">100</span>).head())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>No RI model provided. Creating TTD data with uniform weights (sample_weight=1)...
TTD dataset created. Shape: (552141, 7)
Sum of weights in TTD dataset: 552141.00
Source distribution:
source
Accepted    0.818887
Rejected    0.181113
Name: proportion, dtype: float64</code></pre>
</div>
<div id="tbl-ttd-data-nocloning" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="30">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-ttd-data-nocloning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;11: Sample of TTD Test Data (No Cloning)
</figcaption>
<div aria-describedby="tbl-ttd-data-nocloning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<div>


<table class="dataframe do-not-create-environment cell caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">loan_amnt</th>
<th data-quarto-table-cell-role="th">dti</th>
<th data-quarto-table-cell-role="th">credit_score</th>
<th data-quarto-table-cell-role="th">emp_length</th>
<th data-quarto-table-cell-role="th">default_flag</th>
<th data-quarto-table-cell-role="th">sample_weight</th>
<th data-quarto-table-cell-role="th">source</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">177278</td>
<td>10000.0</td>
<td>60.799999</td>
<td>717.0</td>
<td>0</td>
<td>0</td>
<td>1.0</td>
<td>Accepted</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">307171</td>
<td>16000.0</td>
<td>18.570000</td>
<td>682.0</td>
<td>2</td>
<td>0</td>
<td>1.0</td>
<td>Accepted</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">524196</td>
<td>20000.0</td>
<td>10.960000</td>
<td>541.0</td>
<td>0</td>
<td>None</td>
<td>1.0</td>
<td>Rejected</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">472558</td>
<td>10000.0</td>
<td>21.030001</td>
<td>541.0</td>
<td>0</td>
<td>None</td>
<td>1.0</td>
<td>Rejected</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">19253</td>
<td>7000.0</td>
<td>11.130000</td>
<td>692.0</td>
<td>10</td>
<td>0</td>
<td>1.0</td>
<td>Accepted</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</figure>
</div>
</div>
<p>We can score the applicants in the test set and determine how many applicants are accepted or rejected by the TTD model.</p>
<div class="cell" data-execution_count="31">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1"></a>df_ttd_test_noCloning[<span class="st">'model_decision'</span>] <span class="op">=</span> ag_model_constrained_wrapped.predict(</span>
<span id="cb65-2"><a href="#cb65-2"></a>    df_ttd_test_noCloning[ag_model_constrained_wrapped.feature_names_]</span>
<span id="cb65-3"><a href="#cb65-3"></a>)</span>
<span id="cb65-4"><a href="#cb65-4"></a></span>
<span id="cb65-5"><a href="#cb65-5"></a>df_ttd_test_noCloning[<span class="st">'model_decision'</span>] <span class="op">=</span> df_ttd_test_noCloning[<span class="st">'model_decision'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="st">'Model_Reject'</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">1</span> <span class="cf">else</span> <span class="st">'Model_Approve'</span>)</span>
<span id="cb65-6"><a href="#cb65-6"></a></span>
<span id="cb65-7"><a href="#cb65-7"></a><span class="co"># Compare model decisions to actual outcomes</span></span>
<span id="cb65-8"><a href="#cb65-8"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">--- Comparing Model Decisions to Actual Outcomes ---"</span>)</span>
<span id="cb65-9"><a href="#cb65-9"></a></span>
<span id="cb65-10"><a href="#cb65-10"></a><span class="co"># Create a cross-tabulation of model decision vs actual outcome</span></span>
<span id="cb65-11"><a href="#cb65-11"></a>comparison <span class="op">=</span> pd.crosstab(</span>
<span id="cb65-12"><a href="#cb65-12"></a>    df_ttd_test_noCloning[<span class="st">'model_decision'</span>],</span>
<span id="cb65-13"><a href="#cb65-13"></a>    df_ttd_test_noCloning[<span class="st">'source'</span>],</span>
<span id="cb65-14"><a href="#cb65-14"></a>    margins<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb65-15"><a href="#cb65-15"></a>    margins_name<span class="op">=</span><span class="st">'Total'</span></span>
<span id="cb65-16"><a href="#cb65-16"></a>)</span>
<span id="cb65-17"><a href="#cb65-17"></a></span>
<span id="cb65-18"><a href="#cb65-18"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Count of model decisions vs. actual outcomes:"</span>)</span>
<span id="cb65-19"><a href="#cb65-19"></a>display(comparison)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
--- Comparing Model Decisions to Actual Outcomes ---

Count of model decisions vs. actual outcomes:</code></pre>
</div>
<div id="tbl-ttd-model-decisions" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="31">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-ttd-model-decisions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;12: Model Decisions on TTD Test Set (No Cloning)
</figcaption>
<div aria-describedby="tbl-ttd-model-decisions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<div>


<table class="dataframe do-not-create-environment cell caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">source</th>
<th data-quarto-table-cell-role="th">Accepted</th>
<th data-quarto-table-cell-role="th">Rejected</th>
<th data-quarto-table-cell-role="th">Total</th>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">model_decision</th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">Model_Approve</td>
<td>398733</td>
<td>8227</td>
<td>406960</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">Model_Reject</td>
<td>53408</td>
<td>91773</td>
<td>145181</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">Total</td>
<td>452141</td>
<td>100000</td>
<td>552141</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</figure>
</div>
</div>
</section>
</section>
<section id="conclusion" class="level1" data-number="12">
<h1 data-number="12"><span class="header-section-number">12</span> Conclusion</h1>
<p>In this lab, we built a credit scorecard using LendingClub data. We went through the entire process, from preparing the data to evaluating the final model and choosing a cutoff score. A key part was using data from rejected applications to make the model representative of the full applicant pool (this is called reject inference).</p>
<p><strong>Key steps and takeaways:</strong></p>
<ul>
<li><strong>Preparing Data:</strong> We loaded data for both approved and rejected loans, created useful features, and processed both datasets consistently.</li>
<li><strong>Checking Feature Trends:</strong> We verified that features like Debt-to-Income (DTI) and credit score behaved as expected (e.g., higher credit score means lower risk). This ensures the model makes business sense.</li>
<li><strong>Using Rejected Data (Reject Inference):</strong> We trained a simple model on approved loans to predict risk for rejected applicants. We then combined the approved and rejected data, assigning weights to the rejected applicants based on their predicted risk. This helps the main model learn from all applicants (Through-the-Door).</li>
<li><strong>Training the Scorecard Model:</strong> We used AutoGluon to build the main model. We trained it twice: once normally, and a second time forcing it to follow the expected feature trends (monotonic constraints).</li>
<li><strong>Understanding the Model:</strong> We used plots (PDP/ICE) to visualize how the model made predictions and to confirm it followed the trends we enforced.</li>
<li><strong>Checking Performance:</strong> We tested the final model on data it hadnt seen before, using metrics like AUC and the KS statistic. We also converted its risk predictions into easy-to-understand 3-digit scores.</li>
<li><strong>Choosing a Cutoff Score:</strong> We used the KS statistic on a separate calibration dataset to determine the minimum score needed for loan approval. We then checked how this cutoff performed on the test data.</li>
</ul>
<p><strong>Summary:</strong> This lab demonstrated a complete process for building a practical credit scorecard. By carefully preparing the data, checking feature relationships, including information from rejected applicants, and using tools like AutoGluon with constraints, we created a model that predicts risk accurately and aligns with business logic. These techniques are valuable for credit risk modeling and other areas where fairness and model understanding are crucial.</p>
<!-- -->

</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb67" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb67-1"><a href="#cb67-1"></a><span class="co">---</span></span>
<span id="cb67-2"><a href="#cb67-2"></a><span class="an">title:</span><span class="co"> "Lab 02: Credit Scorecard Development"</span></span>
<span id="cb67-3"><a href="#cb67-3"></a><span class="an">format:</span><span class="co"> </span></span>
<span id="cb67-4"><a href="#cb67-4"></a><span class="co">    html:</span></span>
<span id="cb67-5"><a href="#cb67-5"></a><span class="co">        toc: true</span></span>
<span id="cb67-6"><a href="#cb67-6"></a><span class="co">        code-fold: true</span></span>
<span id="cb67-7"><a href="#cb67-7"></a><span class="co">        code-tools: true</span></span>
<span id="cb67-8"><a href="#cb67-8"></a><span class="co">        code-line-numbers: true</span></span>
<span id="cb67-9"><a href="#cb67-9"></a><span class="co">        page-layout: full</span></span>
<span id="cb67-10"><a href="#cb67-10"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb67-11"><a href="#cb67-11"></a><span class="an">number-figures:</span><span class="co"> true</span></span>
<span id="cb67-12"><a href="#cb67-12"></a><span class="an">number-tables:</span><span class="co"> true</span></span>
<span id="cb67-13"><a href="#cb67-13"></a><span class="an">execute:</span></span>
<span id="cb67-14"><a href="#cb67-14"></a><span class="co">    warning: false</span></span>
<span id="cb67-15"><a href="#cb67-15"></a><span class="co">    message: false</span></span>
<span id="cb67-16"><a href="#cb67-16"></a><span class="co">---</span></span>
<span id="cb67-17"><a href="#cb67-17"></a></span>
<span id="cb67-18"><a href="#cb67-18"></a><span class="fu"># Introduction</span></span>
<span id="cb67-19"><a href="#cb67-19"></a></span>
<span id="cb67-20"><a href="#cb67-20"></a>The lab provides a hands-on guide to building and evaluating a credit scorecard using loan-level (accepts) and applicant-level (rejects) data from LendingClub (an unsecured lender). The primary goal is to develop a 3-digit score that ranks Through-the-Door (TTD) applicants based on their predicted credit risk. We will use Fuzzy Augmentation (a reject inference method) to incorporate data from rejected applicants, to reduce selection bias in the training data.</span>
<span id="cb67-21"><a href="#cb67-21"></a></span>
<span id="cb67-22"><a href="#cb67-22"></a>We will cover data preparation, assessing feature relationships, reject inference using fuzzy augmentation, training models with AutoGluon, diagnosing model behavior with ICE/PDP, applying monotonic constraints, evaluating performance using the KS statistic, and selecting decision thresholds.</span>
<span id="cb67-23"><a href="#cb67-23"></a></span>
<span id="cb67-24"><a href="#cb67-24"></a>**Learning Objectives:**</span>
<span id="cb67-25"><a href="#cb67-25"></a></span>
<span id="cb67-26"><a href="#cb67-26"></a><span class="ss">*   </span>Load and prepare accepted and rejected lending data.</span>
<span id="cb67-27"><a href="#cb67-27"></a><span class="ss">*   </span>Define a target variable for credit default.</span>
<span id="cb67-28"><a href="#cb67-28"></a><span class="ss">*   </span>Perform feature engineering and selection for scorecard modeling.</span>
<span id="cb67-29"><a href="#cb67-29"></a><span class="ss">*   </span>Assess feature monotonicity using binned probability plots.</span>
<span id="cb67-30"><a href="#cb67-30"></a><span class="ss">*   </span>Understand and implement reject inference (Fuzzy Augmentation).</span>
<span id="cb67-31"><a href="#cb67-31"></a><span class="ss">*   </span>Train weighted models using AutoGluon on the augmented TTD dataset.</span>
<span id="cb67-32"><a href="#cb67-32"></a><span class="ss">*   </span>Diagnose model behavior using ICE and PDP.</span>
<span id="cb67-33"><a href="#cb67-33"></a><span class="ss">*   </span>Apply monotonic constraints to align models with business logic.</span>
<span id="cb67-34"><a href="#cb67-34"></a><span class="ss">*   </span>Evaluate scorecard performance using the KS statistic.</span>
<span id="cb67-35"><a href="#cb67-35"></a><span class="ss">*   </span>Select an optimal decision threshold based on business objectives.</span>
<span id="cb67-36"><a href="#cb67-36"></a></span>
<span id="cb67-37"><a href="#cb67-37"></a><span class="fu"># Setup</span></span>
<span id="cb67-38"><a href="#cb67-38"></a></span>
<span id="cb67-39"><a href="#cb67-39"></a><span class="fu">## Import Python Libraries</span></span>
<span id="cb67-40"><a href="#cb67-40"></a></span>
<span id="cb67-43"><a href="#cb67-43"></a><span class="in">```{python}</span></span>
<span id="cb67-44"><a href="#cb67-44"></a><span class="co">#| label: setup-imports</span></span>
<span id="cb67-45"><a href="#cb67-45"></a><span class="co">#| message: false</span></span>
<span id="cb67-46"><a href="#cb67-46"></a></span>
<span id="cb67-47"><a href="#cb67-47"></a><span class="co"># System utilities</span></span>
<span id="cb67-48"><a href="#cb67-48"></a><span class="im">import</span> os</span>
<span id="cb67-49"><a href="#cb67-49"></a><span class="im">import</span> shutil</span>
<span id="cb67-50"><a href="#cb67-50"></a><span class="im">import</span> random</span>
<span id="cb67-51"><a href="#cb67-51"></a><span class="im">import</span> warnings</span>
<span id="cb67-52"><a href="#cb67-52"></a><span class="im">import</span> time</span>
<span id="cb67-53"><a href="#cb67-53"></a><span class="im">import</span> gc</span>
<span id="cb67-54"><a href="#cb67-54"></a><span class="im">import</span> psutil</span>
<span id="cb67-55"><a href="#cb67-55"></a></span>
<span id="cb67-56"><a href="#cb67-56"></a><span class="co"># Data manipulation and visualization</span></span>
<span id="cb67-57"><a href="#cb67-57"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb67-58"><a href="#cb67-58"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb67-59"><a href="#cb67-59"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb67-60"><a href="#cb67-60"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb67-61"><a href="#cb67-61"></a><span class="im">from</span> IPython.display <span class="im">import</span> display <span class="co"># Explicit import for display</span></span>
<span id="cb67-62"><a href="#cb67-62"></a><span class="im">from</span> scipy <span class="im">import</span> stats, special</span>
<span id="cb67-63"><a href="#cb67-63"></a><span class="im">from</span> sklearn.feature_selection <span class="im">import</span> mutual_info_classif</span>
<span id="cb67-64"><a href="#cb67-64"></a><span class="im">import</span> re</span>
<span id="cb67-65"><a href="#cb67-65"></a><span class="im">import</span> duckdb</span>
<span id="cb67-66"><a href="#cb67-66"></a></span>
<span id="cb67-67"><a href="#cb67-67"></a><span class="co"># Machine learning - scikit-learn</span></span>
<span id="cb67-68"><a href="#cb67-68"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb67-69"><a href="#cb67-69"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression, LogisticRegressionCV</span>
<span id="cb67-70"><a href="#cb67-70"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_auc_score, f1_score, confusion_matrix, classification_report</span>
<span id="cb67-71"><a href="#cb67-71"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> LabelEncoder, StandardScaler, OneHotEncoder</span>
<span id="cb67-72"><a href="#cb67-72"></a><span class="im">from</span> sklearn.impute <span class="im">import</span> SimpleImputer</span>
<span id="cb67-73"><a href="#cb67-73"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb67-74"><a href="#cb67-74"></a><span class="im">from</span> sklearn.compose <span class="im">import</span> ColumnTransformer</span>
<span id="cb67-75"><a href="#cb67-75"></a><span class="im">from</span> sklearn.inspection <span class="im">import</span> PartialDependenceDisplay</span>
<span id="cb67-76"><a href="#cb67-76"></a><span class="im">from</span> sklearn.base <span class="im">import</span> BaseEstimator, ClassifierMixin</span>
<span id="cb67-77"><a href="#cb67-77"></a><span class="im">from</span> sklearn.utils.validation <span class="im">import</span> check_is_fitted, check_X_y, check_array</span>
<span id="cb67-78"><a href="#cb67-78"></a><span class="im">from</span> sklearn <span class="im">import</span> set_config</span>
<span id="cb67-79"><a href="#cb67-79"></a></span>
<span id="cb67-80"><a href="#cb67-80"></a><span class="co"># Specialized ML libraries</span></span>
<span id="cb67-81"><a href="#cb67-81"></a><span class="im">from</span> autogluon.tabular <span class="im">import</span> TabularPredictor, TabularDataset</span>
<span id="cb67-82"><a href="#cb67-82"></a><span class="im">from</span> autogluon.common.features.feature_metadata <span class="im">import</span> FeatureMetadata <span class="co"># For monotonic constraints</span></span>
<span id="cb67-83"><a href="#cb67-83"></a><span class="im">import</span> shap</span>
<span id="cb67-84"><a href="#cb67-84"></a><span class="im">from</span> ydata_profiling <span class="im">import</span> ProfileReport</span>
<span id="cb67-85"><a href="#cb67-85"></a></span>
<span id="cb67-86"><a href="#cb67-86"></a><span class="co"># Counterfactual Explanations (optional, install if needed: pip install dice-ml)</span></span>
<span id="cb67-87"><a href="#cb67-87"></a><span class="cf">try</span>:</span>
<span id="cb67-88"><a href="#cb67-88"></a>    <span class="im">import</span> dice_ml</span>
<span id="cb67-89"><a href="#cb67-89"></a>    <span class="im">from</span> dice_ml.utils <span class="im">import</span> helpers <span class="co"># Helper functions for DICE</span></span>
<span id="cb67-90"><a href="#cb67-90"></a><span class="cf">except</span> <span class="pp">ImportError</span>:</span>
<span id="cb67-91"><a href="#cb67-91"></a>    <span class="bu">print</span>(<span class="st">"""</span></span>
<span id="cb67-92"><a href="#cb67-92"></a><span class="st">    </span></span>
<span id="cb67-93"><a href="#cb67-93"></a><span class="st">    dice-ml not found. You need to update your conda env by running:</span></span>
<span id="cb67-94"><a href="#cb67-94"></a><span class="st">    </span></span>
<span id="cb67-95"><a href="#cb67-95"></a><span class="st">    conda activate env_AutoGluon_202502</span></span>
<span id="cb67-96"><a href="#cb67-96"></a><span class="st">    conda install -c conda-forge dice-ml</span></span>
<span id="cb67-97"><a href="#cb67-97"></a><span class="st">    </span></span>
<span id="cb67-98"><a href="#cb67-98"></a><span class="st">    """</span>)</span>
<span id="cb67-99"><a href="#cb67-99"></a>    dice_ml <span class="op">=</span> <span class="va">None</span></span>
<span id="cb67-100"><a href="#cb67-100"></a></span>
<span id="cb67-101"><a href="#cb67-101"></a><span class="co"># Settings</span></span>
<span id="cb67-102"><a href="#cb67-102"></a>pd.set_option(<span class="st">'display.max_columns'</span>, <span class="dv">50</span>)</span>
<span id="cb67-103"><a href="#cb67-103"></a>pd.set_option(<span class="st">'display.max_rows'</span>, <span class="dv">100</span>)</span>
<span id="cb67-104"><a href="#cb67-104"></a>warnings.filterwarnings(<span class="st">'ignore'</span>, category<span class="op">=</span><span class="pp">FutureWarning</span>) <span class="co"># Suppress specific FutureWarnings</span></span>
<span id="cb67-105"><a href="#cb67-105"></a>set_config(transform_output<span class="op">=</span><span class="st">"pandas"</span>) <span class="co"># Set sklearn output to pandas</span></span>
<span id="cb67-106"><a href="#cb67-106"></a></span>
<span id="cb67-107"><a href="#cb67-107"></a><span class="bu">print</span>(<span class="st">"Libraries imported successfully."</span>)</span>
<span id="cb67-108"><a href="#cb67-108"></a><span class="in">```</span></span>
<span id="cb67-109"><a href="#cb67-109"></a></span>
<span id="cb67-110"><a href="#cb67-110"></a><span class="fu">## Helper Functions and Classes</span></span>
<span id="cb67-111"><a href="#cb67-111"></a></span>
<span id="cb67-112"><a href="#cb67-112"></a>Define helper functions for plotting, scoring, data transformation, and a wrapper for AutoGluon compatibility with scikit-learn.</span>
<span id="cb67-113"><a href="#cb67-113"></a></span>
<span id="cb67-116"><a href="#cb67-116"></a><span class="in">```{python}</span></span>
<span id="cb67-117"><a href="#cb67-117"></a><span class="co"># | label: helper-funcs-classes</span></span>
<span id="cb67-118"><a href="#cb67-118"></a></span>
<span id="cb67-119"><a href="#cb67-119"></a></span>
<span id="cb67-120"><a href="#cb67-120"></a><span class="kw">def</span> binned_prob_plot(</span>
<span id="cb67-121"><a href="#cb67-121"></a>    data: pd.DataFrame,</span>
<span id="cb67-122"><a href="#cb67-122"></a>    feature: <span class="bu">str</span>,</span>
<span id="cb67-123"><a href="#cb67-123"></a>    target_binary: <span class="bu">str</span>,</span>
<span id="cb67-124"><a href="#cb67-124"></a>    cont_feat_flag: <span class="bu">bool</span> <span class="op">|</span> <span class="va">None</span> <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb67-125"><a href="#cb67-125"></a>    transform_log_odds: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb67-126"><a href="#cb67-126"></a>    num_bins: <span class="bu">int</span> <span class="op">=</span> <span class="dv">10</span>,</span>
<span id="cb67-127"><a href="#cb67-127"></a>    show_plot: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb67-128"><a href="#cb67-128"></a>):</span>
<span id="cb67-129"><a href="#cb67-129"></a>    <span class="co">"""</span></span>
<span id="cb67-130"><a href="#cb67-130"></a><span class="co">    Plots the average binary target against either bins of a feature or categories of the feature.</span></span>
<span id="cb67-131"><a href="#cb67-131"></a><span class="co">    If show_plot=False, skips plotting and only returns Spearman correlation (for continuous feature)</span></span>
<span id="cb67-132"><a href="#cb67-132"></a><span class="co">    or mutual information (for categorical feature).</span></span>
<span id="cb67-133"><a href="#cb67-133"></a></span>
<span id="cb67-134"><a href="#cb67-134"></a><span class="co">    Parameters:</span></span>
<span id="cb67-135"><a href="#cb67-135"></a><span class="co">        data (DataFrame): The DataFrame containing the data.</span></span>
<span id="cb67-136"><a href="#cb67-136"></a><span class="co">        feature (str): The name of the feature to be binned or used as is if categorical.</span></span>
<span id="cb67-137"><a href="#cb67-137"></a><span class="co">        target_binary (str): The name of the binary target variable.</span></span>
<span id="cb67-138"><a href="#cb67-138"></a><span class="co">        cont_feat_flag (bool): True if the feature is continuous, False if it's categorical.</span></span>
<span id="cb67-139"><a href="#cb67-139"></a><span class="co">                    The function will try to infer the feature type if not provided by user.</span></span>
<span id="cb67-140"><a href="#cb67-140"></a><span class="co">        transform_log_odds (bool): If True, transforms probabilities into log odds.</span></span>
<span id="cb67-141"><a href="#cb67-141"></a><span class="co">        num_bins (int): Number of bins for discretization if the feature is continuous.</span></span>
<span id="cb67-142"><a href="#cb67-142"></a><span class="co">        show_plot (bool): If True, plot the figure.</span></span>
<span id="cb67-143"><a href="#cb67-143"></a></span>
<span id="cb67-144"><a href="#cb67-144"></a><span class="co">    Returns:</span></span>
<span id="cb67-145"><a href="#cb67-145"></a><span class="co">        dict: {</span></span>
<span id="cb67-146"><a href="#cb67-146"></a><span class="co">            'feature': feature,</span></span>
<span id="cb67-147"><a href="#cb67-147"></a><span class="co">            'measure_name': string ("spearman_corr" if continuous; "mutual_info" if categorical),</span></span>
<span id="cb67-148"><a href="#cb67-148"></a><span class="co">            "measure_value": float,</span></span>
<span id="cb67-149"><a href="#cb67-149"></a><span class="co">            'p_value': float (or None for MI)</span></span>
<span id="cb67-150"><a href="#cb67-150"></a><span class="co">        }</span></span>
<span id="cb67-151"><a href="#cb67-151"></a><span class="co">    """</span></span>
<span id="cb67-152"><a href="#cb67-152"></a>    <span class="co"># Work on a copy to avoid modifying original</span></span>
<span id="cb67-153"><a href="#cb67-153"></a>    df <span class="op">=</span> data.copy()</span>
<span id="cb67-154"><a href="#cb67-154"></a>    <span class="co"># Infer cont_feat_flag if not provided: sample up to 100 obs, if &gt;60 unique values =&gt; continuous</span></span>
<span id="cb67-155"><a href="#cb67-155"></a>    <span class="cf">if</span> cont_feat_flag <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb67-156"><a href="#cb67-156"></a>        tmp <span class="op">=</span> df[feature].dropna()</span>
<span id="cb67-157"><a href="#cb67-157"></a>        <span class="co"># Ensure sample size does not exceed available data</span></span>
<span id="cb67-158"><a href="#cb67-158"></a>        sample_size <span class="op">=</span> <span class="bu">min</span>(<span class="dv">100</span>, <span class="bu">len</span>(tmp))</span>
<span id="cb67-159"><a href="#cb67-159"></a>        <span class="cf">if</span> sample_size <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb67-160"><a href="#cb67-160"></a>            tmp <span class="op">=</span> tmp.sample(sample_size, replace<span class="op">=</span><span class="va">False</span>, random_state<span class="op">=</span><span class="dv">2025</span>)</span>
<span id="cb67-161"><a href="#cb67-161"></a>            cont_feat_flag <span class="op">=</span> tmp.nunique() <span class="op">&gt;</span> <span class="bu">min</span>(<span class="dv">60</span>, sample_size <span class="op">*</span> <span class="fl">0.5</span>) <span class="co"># Adjust threshold based on sample size</span></span>
<span id="cb67-162"><a href="#cb67-162"></a>        <span class="cf">else</span>:</span>
<span id="cb67-163"><a href="#cb67-163"></a>            cont_feat_flag <span class="op">=</span> <span class="va">False</span> <span class="co"># Default to categorical if no data</span></span>
<span id="cb67-164"><a href="#cb67-164"></a>        <span class="bu">print</span>(</span>
<span id="cb67-165"><a href="#cb67-165"></a>            <span class="ss">f"Feature </span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss"> is inferred as </span><span class="sc">{</span><span class="st">'continuous'</span> <span class="cf">if</span> cont_feat_flag <span class="cf">else</span> <span class="st">'categorical'</span><span class="sc">}</span><span class="ss">."</span></span>
<span id="cb67-166"><a href="#cb67-166"></a>        )</span>
<span id="cb67-167"><a href="#cb67-167"></a></span>
<span id="cb67-168"><a href="#cb67-168"></a>    <span class="co"># Bin or categorize</span></span>
<span id="cb67-169"><a href="#cb67-169"></a>    <span class="cf">if</span> cont_feat_flag:</span>
<span id="cb67-170"><a href="#cb67-170"></a>        <span class="co"># Use rank(method='first') to handle non-unique bin edges better</span></span>
<span id="cb67-171"><a href="#cb67-171"></a>        <span class="cf">try</span>:</span>
<span id="cb67-172"><a href="#cb67-172"></a>            df[<span class="st">"bin_label"</span>] <span class="op">=</span> pd.qcut(</span>
<span id="cb67-173"><a href="#cb67-173"></a>                df[feature].rank(method<span class="op">=</span><span class="st">'first'</span>), <span class="co"># Rank first</span></span>
<span id="cb67-174"><a href="#cb67-174"></a>                q<span class="op">=</span>num_bins,</span>
<span id="cb67-175"><a href="#cb67-175"></a>                duplicates<span class="op">=</span><span class="st">"drop"</span>,</span>
<span id="cb67-176"><a href="#cb67-176"></a>                labels<span class="op">=</span>[<span class="bu">str</span>(i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, num_bins <span class="op">+</span> <span class="dv">1</span>)],</span>
<span id="cb67-177"><a href="#cb67-177"></a>            )</span>
<span id="cb67-178"><a href="#cb67-178"></a>        <span class="cf">except</span> <span class="pp">ValueError</span> <span class="im">as</span> e:</span>
<span id="cb67-179"><a href="#cb67-179"></a>             <span class="co"># Fallback if qcut still fails (e.g., too few unique values)</span></span>
<span id="cb67-180"><a href="#cb67-180"></a>            <span class="bu">print</span>(<span class="ss">f"Warning: pd.qcut failed for </span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">). Using fewer bins or manual ranking."</span>)</span>
<span id="cb67-181"><a href="#cb67-181"></a>            ranks <span class="op">=</span> df[feature].rank(method<span class="op">=</span><span class="st">'first'</span>)</span>
<span id="cb67-182"><a href="#cb67-182"></a>            bin_size <span class="op">=</span> <span class="bu">max</span>(<span class="dv">1</span>, <span class="bu">len</span>(df) <span class="op">//</span> num_bins)</span>
<span id="cb67-183"><a href="#cb67-183"></a>            df[<span class="st">'bin_label'</span>] <span class="op">=</span> ((ranks <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> bin_size).clip(upper<span class="op">=</span>num_bins <span class="op">-</span> <span class="dv">1</span>).astype(<span class="bu">str</span>)</span>
<span id="cb67-184"><a href="#cb67-184"></a>    <span class="cf">else</span>:</span>
<span id="cb67-185"><a href="#cb67-185"></a>        df[<span class="st">"bin_label"</span>] <span class="op">=</span> df[feature].astype(<span class="st">"category"</span>)</span>
<span id="cb67-186"><a href="#cb67-186"></a>        <span class="co"># Convert original feature to category codes for MI calculation later</span></span>
<span id="cb67-187"><a href="#cb67-187"></a>        df[feature <span class="op">+</span> <span class="st">"_codes"</span>] <span class="op">=</span> df[feature].astype(<span class="st">"category"</span>).cat.codes</span>
<span id="cb67-188"><a href="#cb67-188"></a></span>
<span id="cb67-189"><a href="#cb67-189"></a>    <span class="co"># Group and compute mean &amp; count</span></span>
<span id="cb67-190"><a href="#cb67-190"></a>    grouped <span class="op">=</span> df.groupby(<span class="st">"bin_label"</span>, observed<span class="op">=</span><span class="va">False</span>).agg( <span class="co"># Use observed=False for category</span></span>
<span id="cb67-191"><a href="#cb67-191"></a>        <span class="op">**</span>{  <span class="co"># **{} unpacks the dict</span></span>
<span id="cb67-192"><a href="#cb67-192"></a>            <span class="st">"average_"</span> <span class="op">+</span> target_binary: (target_binary, <span class="st">"mean"</span>),  <span class="co"># proba</span></span>
<span id="cb67-193"><a href="#cb67-193"></a>            <span class="st">"count"</span>: (target_binary, <span class="st">"count"</span>),  <span class="co"># row count</span></span>
<span id="cb67-194"><a href="#cb67-194"></a>        }</span>
<span id="cb67-195"><a href="#cb67-195"></a>    )</span>
<span id="cb67-196"><a href="#cb67-196"></a></span>
<span id="cb67-197"><a href="#cb67-197"></a>    <span class="co"># Log-odds transform if requested</span></span>
<span id="cb67-198"><a href="#cb67-198"></a>    <span class="cf">if</span> transform_log_odds:</span>
<span id="cb67-199"><a href="#cb67-199"></a>        eps <span class="op">=</span> <span class="fl">1e-6</span></span>
<span id="cb67-200"><a href="#cb67-200"></a>        grouped[<span class="st">"transform_avg_prob"</span>] <span class="op">=</span> special.logit(</span>
<span id="cb67-201"><a href="#cb67-201"></a>            np.clip(grouped[<span class="st">"average_"</span> <span class="op">+</span> target_binary], eps, <span class="dv">1</span> <span class="op">-</span> eps)</span>
<span id="cb67-202"><a href="#cb67-202"></a>        )</span>
<span id="cb67-203"><a href="#cb67-203"></a></span>
<span id="cb67-204"><a href="#cb67-204"></a>    <span class="co"># Compute Spearman for continuous or Mutual Information for categorical</span></span>
<span id="cb67-205"><a href="#cb67-205"></a>    measure_name <span class="op">=</span> <span class="va">None</span></span>
<span id="cb67-206"><a href="#cb67-206"></a>    measure_value <span class="op">=</span> <span class="va">None</span></span>
<span id="cb67-207"><a href="#cb67-207"></a>    p_value <span class="op">=</span> <span class="va">None</span></span>
<span id="cb67-208"><a href="#cb67-208"></a></span>
<span id="cb67-209"><a href="#cb67-209"></a>    <span class="cf">if</span> cont_feat_flag:</span>
<span id="cb67-210"><a href="#cb67-210"></a>        measure_name <span class="op">=</span> <span class="st">"spearman_corr"</span></span>
<span id="cb67-211"><a href="#cb67-211"></a>        y <span class="op">=</span> <span class="st">"transform_avg_prob"</span> <span class="cf">if</span> transform_log_odds <span class="cf">else</span> <span class="st">"average_"</span> <span class="op">+</span> target_binary</span>
<span id="cb67-212"><a href="#cb67-212"></a>        <span class="co"># Ensure grouped index is numeric for correlation</span></span>
<span id="cb67-213"><a href="#cb67-213"></a>        grouped_idx_numeric <span class="op">=</span> pd.to_numeric(grouped.index, errors<span class="op">=</span><span class="st">'coerce'</span>).fillna(<span class="dv">0</span>)</span>
<span id="cb67-214"><a href="#cb67-214"></a>        <span class="cf">if</span> <span class="bu">len</span>(grouped) <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb67-215"><a href="#cb67-215"></a>            measure_value, p_value <span class="op">=</span> stats.spearmanr(grouped_idx_numeric, grouped[y])</span>
<span id="cb67-216"><a href="#cb67-216"></a>        <span class="cf">else</span>:</span>
<span id="cb67-217"><a href="#cb67-217"></a>            measure_value, p_value <span class="op">=</span> np.nan, np.nan <span class="co"># Cannot compute correlation with one group</span></span>
<span id="cb67-218"><a href="#cb67-218"></a>    <span class="cf">else</span>:</span>
<span id="cb67-219"><a href="#cb67-219"></a>        measure_name <span class="op">=</span> <span class="st">"mutual_info"</span></span>
<span id="cb67-220"><a href="#cb67-220"></a>        <span class="co"># Compute mutual information using the category codes</span></span>
<span id="cb67-221"><a href="#cb67-221"></a>        <span class="co"># Ensure no NaNs in target or feature codes</span></span>
<span id="cb67-222"><a href="#cb67-222"></a>        df_mi <span class="op">=</span> df[[feature <span class="op">+</span> <span class="st">"_codes"</span>, target_binary]].dropna()</span>
<span id="cb67-223"><a href="#cb67-223"></a>        <span class="cf">if</span> <span class="kw">not</span> df_mi.empty:</span>
<span id="cb67-224"><a href="#cb67-224"></a>            measure_value <span class="op">=</span> mutual_info_classif(df_mi[[feature <span class="op">+</span> <span class="st">"_codes"</span>]], df_mi[target_binary], discrete_features<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]</span>
<span id="cb67-225"><a href="#cb67-225"></a>        <span class="cf">else</span>:</span>
<span id="cb67-226"><a href="#cb67-226"></a>            measure_value <span class="op">=</span> np.nan</span>
<span id="cb67-227"><a href="#cb67-227"></a>        p_value <span class="op">=</span> <span class="va">None</span> <span class="co"># MI doesn't have a standard p-value like correlation</span></span>
<span id="cb67-228"><a href="#cb67-228"></a></span>
<span id="cb67-229"><a href="#cb67-229"></a>    <span class="co"># Plotting</span></span>
<span id="cb67-230"><a href="#cb67-230"></a>    <span class="cf">if</span> show_plot:</span>
<span id="cb67-231"><a href="#cb67-231"></a>        y_col <span class="op">=</span> (</span>
<span id="cb67-232"><a href="#cb67-232"></a>            <span class="st">"transform_avg_prob"</span> <span class="cf">if</span> transform_log_odds <span class="cf">else</span> <span class="st">"average_"</span> <span class="op">+</span> target_binary</span>
<span id="cb67-233"><a href="#cb67-233"></a>        )</span>
<span id="cb67-234"><a href="#cb67-234"></a></span>
<span id="cb67-235"><a href="#cb67-235"></a>        fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb67-236"><a href="#cb67-236"></a>        <span class="co"># Use numeric index for plotting if continuous, otherwise use category labels</span></span>
<span id="cb67-237"><a href="#cb67-237"></a>        plot_x <span class="op">=</span> <span class="bu">range</span>(<span class="bu">len</span>(grouped)) <span class="cf">if</span> cont_feat_flag <span class="cf">else</span> grouped.index</span>
<span id="cb67-238"><a href="#cb67-238"></a>        ax.plot(</span>
<span id="cb67-239"><a href="#cb67-239"></a>            plot_x,</span>
<span id="cb67-240"><a href="#cb67-240"></a>            grouped[y_col],</span>
<span id="cb67-241"><a href="#cb67-241"></a>            marker<span class="op">=</span><span class="st">"o"</span>,</span>
<span id="cb67-242"><a href="#cb67-242"></a>            linestyle<span class="op">=</span><span class="st">"-"</span>,</span>
<span id="cb67-243"><a href="#cb67-243"></a>            label<span class="op">=</span><span class="st">"Log Odds"</span> <span class="cf">if</span> transform_log_odds <span class="cf">else</span> <span class="st">"Probability"</span>,</span>
<span id="cb67-244"><a href="#cb67-244"></a>        )</span>
<span id="cb67-245"><a href="#cb67-245"></a>        ax.set_xlabel(feature, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb67-246"><a href="#cb67-246"></a>        ax.set_ylabel(<span class="st">"Log Odds"</span> <span class="cf">if</span> transform_log_odds <span class="cf">else</span> <span class="st">"Probability"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb67-247"><a href="#cb67-247"></a>        ax.tick_params(axis<span class="op">=</span><span class="st">"both"</span>, labelsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb67-248"><a href="#cb67-248"></a></span>
<span id="cb67-249"><a href="#cb67-249"></a>        ax2 <span class="op">=</span> ax.twinx()</span>
<span id="cb67-250"><a href="#cb67-250"></a>        ax2.bar(</span>
<span id="cb67-251"><a href="#cb67-251"></a>            plot_x,</span>
<span id="cb67-252"><a href="#cb67-252"></a>            grouped[<span class="st">"count"</span>],</span>
<span id="cb67-253"><a href="#cb67-253"></a>            alpha<span class="op">=</span><span class="fl">0.25</span>,</span>
<span id="cb67-254"><a href="#cb67-254"></a>            color<span class="op">=</span><span class="st">"gray"</span>,</span>
<span id="cb67-255"><a href="#cb67-255"></a>            align<span class="op">=</span><span class="st">"center"</span>,</span>
<span id="cb67-256"><a href="#cb67-256"></a>            label<span class="op">=</span><span class="st">"Counts"</span>,</span>
<span id="cb67-257"><a href="#cb67-257"></a>        )</span>
<span id="cb67-258"><a href="#cb67-258"></a>        ax2.set_ylabel(<span class="st">"Counts"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb67-259"><a href="#cb67-259"></a>        ax2.tick_params(axis<span class="op">=</span><span class="st">"y"</span>, labelsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb67-260"><a href="#cb67-260"></a>        <span class="co"># Adjust secondary axis limits to 0 and 10x its current maximum</span></span>
<span id="cb67-261"><a href="#cb67-261"></a>        y_max <span class="op">=</span> ax2.get_ylim()[<span class="dv">1</span>]</span>
<span id="cb67-262"><a href="#cb67-262"></a>        ax2.set_ylim(<span class="dv">0</span>, y_max <span class="op">*</span> <span class="dv">10</span>)</span>
<span id="cb67-263"><a href="#cb67-263"></a></span>
<span id="cb67-264"><a href="#cb67-264"></a>        <span class="co"># Set x-ticks and labels</span></span>
<span id="cb67-265"><a href="#cb67-265"></a>        ax.set_xticks(plot_x)</span>
<span id="cb67-266"><a href="#cb67-266"></a>        ax.set_xticklabels(grouped.index, rotation<span class="op">=</span><span class="dv">45</span>, ha<span class="op">=</span><span class="st">"right"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb67-267"><a href="#cb67-267"></a></span>
<span id="cb67-268"><a href="#cb67-268"></a>        <span class="co"># Legend</span></span>
<span id="cb67-269"><a href="#cb67-269"></a>        h1, l1 <span class="op">=</span> ax.get_legend_handles_labels()</span>
<span id="cb67-270"><a href="#cb67-270"></a>        h2, l2 <span class="op">=</span> ax2.get_legend_handles_labels()</span>
<span id="cb67-271"><a href="#cb67-271"></a>        ax.legend(h1 <span class="op">+</span> h2, l1 <span class="op">+</span> l2, loc<span class="op">=</span><span class="st">"upper right"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb67-272"><a href="#cb67-272"></a></span>
<span id="cb67-273"><a href="#cb67-273"></a>        title_suffix <span class="op">=</span> <span class="ss">f" (Spearman: </span><span class="sc">{</span>measure_value<span class="sc">:.3f}</span><span class="ss">)"</span> <span class="cf">if</span> cont_feat_flag <span class="kw">and</span> measure_value <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> <span class="ss">f" (MI: </span><span class="sc">{</span>measure_value<span class="sc">:.3f}</span><span class="ss">)"</span> <span class="cf">if</span> <span class="kw">not</span> cont_feat_flag <span class="kw">and</span> measure_value <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> <span class="st">""</span></span>
<span id="cb67-274"><a href="#cb67-274"></a>        ax.set_title(<span class="ss">f"Binned Probability Plot for </span><span class="sc">{</span>feature<span class="sc">}{</span>title_suffix<span class="sc">}</span><span class="ss">"</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb67-275"><a href="#cb67-275"></a>        ax.grid(alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb67-276"><a href="#cb67-276"></a>        plt.tight_layout()</span>
<span id="cb67-277"><a href="#cb67-277"></a>        plt.show() <span class="co"># Removed: Let Quarto handle plot display</span></span>
<span id="cb67-278"><a href="#cb67-278"></a></span>
<span id="cb67-279"><a href="#cb67-279"></a>    <span class="cf">return</span> {</span>
<span id="cb67-280"><a href="#cb67-280"></a>        <span class="st">"feature"</span>: feature,</span>
<span id="cb67-281"><a href="#cb67-281"></a>        <span class="st">"measure_name"</span>: measure_name,</span>
<span id="cb67-282"><a href="#cb67-282"></a>        <span class="st">"measure_value"</span>: measure_value,</span>
<span id="cb67-283"><a href="#cb67-283"></a>        <span class="st">"p_value"</span>: p_value,</span>
<span id="cb67-284"><a href="#cb67-284"></a>        <span class="st">"log_odds"</span>: transform_log_odds,</span>
<span id="cb67-285"><a href="#cb67-285"></a>    }</span>
<span id="cb67-286"><a href="#cb67-286"></a></span>
<span id="cb67-287"><a href="#cb67-287"></a></span>
<span id="cb67-288"><a href="#cb67-288"></a><span class="kw">def</span> global_set_seed(seed_value<span class="op">=</span><span class="dv">2025</span>):</span>
<span id="cb67-289"><a href="#cb67-289"></a>    <span class="co">"""Sets random seeds for reproducibility."""</span></span>
<span id="cb67-290"><a href="#cb67-290"></a>    random.seed(seed_value)</span>
<span id="cb67-291"><a href="#cb67-291"></a>    np.random.seed(seed_value)</span>
<span id="cb67-292"><a href="#cb67-292"></a></span>
<span id="cb67-293"><a href="#cb67-293"></a><span class="kw">def</span> remove_ag_folder(mdl_folder: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb67-294"><a href="#cb67-294"></a>    <span class="co">"""Removes the AutoGluon model folder if it exists."""</span></span>
<span id="cb67-295"><a href="#cb67-295"></a>    <span class="cf">if</span> os.path.exists(mdl_folder):</span>
<span id="cb67-296"><a href="#cb67-296"></a>        shutil.rmtree(mdl_folder)</span>
<span id="cb67-297"><a href="#cb67-297"></a>        <span class="bu">print</span>(<span class="ss">f"Removed existing AutoGluon folder: </span><span class="sc">{</span>mdl_folder<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-298"><a href="#cb67-298"></a></span>
<span id="cb67-299"><a href="#cb67-299"></a><span class="kw">class</span> AutoGluonSklearnWrapper(BaseEstimator, ClassifierMixin):</span>
<span id="cb67-300"><a href="#cb67-300"></a>    <span class="co">"""</span></span>
<span id="cb67-301"><a href="#cb67-301"></a><span class="co">    Scikit-learn compatible wrapper for AutoGluon TabularPredictor</span></span>
<span id="cb67-302"><a href="#cb67-302"></a><span class="co">    </span></span>
<span id="cb67-303"><a href="#cb67-303"></a><span class="co">    Inherits from scikit-learn's BaseEstimator and ClassifierMixin to provide</span></span>
<span id="cb67-304"><a href="#cb67-304"></a><span class="co">    full compatibility with scikit-learn tools like PartialDependenceDisplay().</span></span>
<span id="cb67-305"><a href="#cb67-305"></a><span class="co">    </span></span>
<span id="cb67-306"><a href="#cb67-306"></a><span class="co">    Parameters</span></span>
<span id="cb67-307"><a href="#cb67-307"></a><span class="co">    ----------</span></span>
<span id="cb67-308"><a href="#cb67-308"></a><span class="co">    label : str</span></span>
<span id="cb67-309"><a href="#cb67-309"></a><span class="co">        Name of the target column</span></span>
<span id="cb67-310"><a href="#cb67-310"></a></span>
<span id="cb67-311"><a href="#cb67-311"></a><span class="co">    **predictor_args : dict</span></span>
<span id="cb67-312"><a href="#cb67-312"></a><span class="co">        Additional arguments passed to TabularPredictor()</span></span>
<span id="cb67-313"><a href="#cb67-313"></a><span class="co">        (e.g., problem_type, eval_metric, path)</span></span>
<span id="cb67-314"><a href="#cb67-314"></a></span>
<span id="cb67-315"><a href="#cb67-315"></a><span class="co">    **fit_args : dict</span></span>
<span id="cb67-316"><a href="#cb67-316"></a><span class="co">        Additional arguments passed to TabularPredictor.fit() method</span></span>
<span id="cb67-317"><a href="#cb67-317"></a><span class="co">        (e.g., holdout_frac, presets, time_limit, excluded_model_types)</span></span>
<span id="cb67-318"><a href="#cb67-318"></a></span>
<span id="cb67-319"><a href="#cb67-319"></a></span>
<span id="cb67-320"><a href="#cb67-320"></a><span class="co">    Attributes</span></span>
<span id="cb67-321"><a href="#cb67-321"></a><span class="co">    ----------</span></span>
<span id="cb67-322"><a href="#cb67-322"></a><span class="co">    predictor : TabularPredictor</span></span>
<span id="cb67-323"><a href="#cb67-323"></a><span class="co">        The trained AutoGluon predictor</span></span>
<span id="cb67-324"><a href="#cb67-324"></a></span>
<span id="cb67-325"><a href="#cb67-325"></a><span class="co">    classes_ : ndarray</span></span>
<span id="cb67-326"><a href="#cb67-326"></a><span class="co">        Class labels (for classification tasks)</span></span>
<span id="cb67-327"><a href="#cb67-327"></a></span>
<span id="cb67-328"><a href="#cb67-328"></a><span class="co">    n_features_in_ : int</span></span>
<span id="cb67-329"><a href="#cb67-329"></a><span class="co">        Number of features seen during fit</span></span>
<span id="cb67-330"><a href="#cb67-330"></a></span>
<span id="cb67-331"><a href="#cb67-331"></a><span class="co">    feature_names_ : list</span></span>
<span id="cb67-332"><a href="#cb67-332"></a><span class="co">        Feature names inferred during fitting</span></span>
<span id="cb67-333"><a href="#cb67-333"></a></span>
<span id="cb67-334"><a href="#cb67-334"></a><span class="co">    is_fitted_ : bool</span></span>
<span id="cb67-335"><a href="#cb67-335"></a><span class="co">        Whether the estimator has been fitted</span></span>
<span id="cb67-336"><a href="#cb67-336"></a><span class="co">    """</span></span>
<span id="cb67-337"><a href="#cb67-337"></a>    </span>
<span id="cb67-338"><a href="#cb67-338"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, label, predictor_args<span class="op">=</span><span class="va">None</span>, fit_args<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb67-339"><a href="#cb67-339"></a>        <span class="va">self</span>.label <span class="op">=</span> label</span>
<span id="cb67-340"><a href="#cb67-340"></a>        <span class="va">self</span>.predictor_args <span class="op">=</span> predictor_args <span class="cf">if</span> predictor_args <span class="cf">else</span> {}</span>
<span id="cb67-341"><a href="#cb67-341"></a>        <span class="va">self</span>.fit_args <span class="op">=</span> fit_args <span class="cf">if</span> fit_args <span class="cf">else</span> {}</span>
<span id="cb67-342"><a href="#cb67-342"></a>        <span class="va">self</span>.predictor <span class="op">=</span> <span class="va">None</span></span>
<span id="cb67-343"><a href="#cb67-343"></a>        <span class="va">self</span>.classes_ <span class="op">=</span> <span class="va">None</span></span>
<span id="cb67-344"><a href="#cb67-344"></a>        <span class="va">self</span>.n_features_in_ <span class="op">=</span> <span class="va">None</span></span>
<span id="cb67-345"><a href="#cb67-345"></a>        <span class="va">self</span>.feature_names_ <span class="op">=</span> <span class="va">None</span></span>
<span id="cb67-346"><a href="#cb67-346"></a>        <span class="va">self</span>.is_fitted_ <span class="op">=</span> <span class="va">False</span></span>
<span id="cb67-347"><a href="#cb67-347"></a></span>
<span id="cb67-348"><a href="#cb67-348"></a>    <span class="kw">def</span> __sklearn_is_fitted__(<span class="va">self</span>):</span>
<span id="cb67-349"><a href="#cb67-349"></a>        <span class="co">"""Official scikit-learn API for checking fitted status"""</span></span>
<span id="cb67-350"><a href="#cb67-350"></a>        <span class="cf">return</span> <span class="va">self</span>.is_fitted_</span>
<span id="cb67-351"><a href="#cb67-351"></a></span>
<span id="cb67-352"><a href="#cb67-352"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X, y, sample_weight<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb67-353"><a href="#cb67-353"></a>        <span class="co">"""</span></span>
<span id="cb67-354"><a href="#cb67-354"></a><span class="co">        Fit AutoGluon model using scikit-learn interface.</span></span>
<span id="cb67-355"><a href="#cb67-355"></a><span class="co">        If sample_weight is provided, it is added as a column to X for AutoGluon.</span></span>
<span id="cb67-356"><a href="#cb67-356"></a><span class="co">        """</span></span>
<span id="cb67-357"><a href="#cb67-357"></a>        <span class="va">self</span>._check_feature_names(X, reset<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb67-358"><a href="#cb67-358"></a>        <span class="va">self</span>._check_n_features(X, reset<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb67-359"><a href="#cb67-359"></a></span>
<span id="cb67-360"><a href="#cb67-360"></a>        <span class="co"># Convert to DataFrame with preserved feature names</span></span>
<span id="cb67-361"><a href="#cb67-361"></a>        train_data <span class="op">=</span> pd.DataFrame(X, columns<span class="op">=</span><span class="va">self</span>.feature_names_)</span>
<span id="cb67-362"><a href="#cb67-362"></a>        train_data[<span class="va">self</span>.label] <span class="op">=</span> y</span>
<span id="cb67-363"><a href="#cb67-363"></a></span>
<span id="cb67-364"><a href="#cb67-364"></a>        <span class="co"># If sample_weight is provided, add it as a column (name must match predictor_args['sample_weight'])</span></span>
<span id="cb67-365"><a href="#cb67-365"></a>        weight_col_name <span class="op">=</span> <span class="va">self</span>.predictor_args.get(<span class="st">'sample_weight'</span>, <span class="va">None</span>)</span>
<span id="cb67-366"><a href="#cb67-366"></a>        <span class="cf">if</span> sample_weight <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb67-367"><a href="#cb67-367"></a>            <span class="cf">if</span> weight_col_name:</span>
<span id="cb67-368"><a href="#cb67-368"></a>                train_data[weight_col_name] <span class="op">=</span> sample_weight</span>
<span id="cb67-369"><a href="#cb67-369"></a>            <span class="cf">else</span>:</span>
<span id="cb67-370"><a href="#cb67-370"></a>                <span class="bu">print</span>(<span class="st">"Warning: sample_weight provided to fit, but 'sample_weight' key not found in predictor_args. Weights will be ignored by AutoGluon."</span>)</span>
<span id="cb67-371"><a href="#cb67-371"></a></span>
<span id="cb67-372"><a href="#cb67-372"></a>        train_data <span class="op">=</span> TabularDataset(train_data)</span>
<span id="cb67-373"><a href="#cb67-373"></a></span>
<span id="cb67-374"><a href="#cb67-374"></a>        <span class="co"># Remove sample_weight from fit_args if present (TabularPredictor.fit does not accept it)</span></span>
<span id="cb67-375"><a href="#cb67-375"></a>        fit_args_clean <span class="op">=</span> {k: v <span class="cf">for</span> k, v <span class="kw">in</span> <span class="va">self</span>.fit_args.items() <span class="cf">if</span> k <span class="op">!=</span> <span class="st">'sample_weight'</span>}</span>
<span id="cb67-376"><a href="#cb67-376"></a></span>
<span id="cb67-377"><a href="#cb67-377"></a>        <span class="va">self</span>.predictor <span class="op">=</span> TabularPredictor(</span>
<span id="cb67-378"><a href="#cb67-378"></a>            label<span class="op">=</span><span class="va">self</span>.label,</span>
<span id="cb67-379"><a href="#cb67-379"></a>            <span class="op">**</span><span class="va">self</span>.predictor_args</span>
<span id="cb67-380"><a href="#cb67-380"></a>        ).fit(train_data, <span class="op">**</span>fit_args_clean)</span>
<span id="cb67-381"><a href="#cb67-381"></a></span>
<span id="cb67-382"><a href="#cb67-382"></a>        <span class="cf">if</span> <span class="va">self</span>.predictor.problem_type <span class="kw">in</span> [<span class="st">'binary'</span>, <span class="st">'multiclass'</span>]:</span>
<span id="cb67-383"><a href="#cb67-383"></a>            <span class="va">self</span>.classes_ <span class="op">=</span> np.array(<span class="va">self</span>.predictor.class_labels)</span>
<span id="cb67-384"><a href="#cb67-384"></a></span>
<span id="cb67-385"><a href="#cb67-385"></a>        <span class="va">self</span>.is_fitted_ <span class="op">=</span> <span class="va">True</span></span>
<span id="cb67-386"><a href="#cb67-386"></a>        <span class="cf">return</span> <span class="va">self</span></span>
<span id="cb67-387"><a href="#cb67-387"></a></span>
<span id="cb67-388"><a href="#cb67-388"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb67-389"><a href="#cb67-389"></a>        <span class="co">"""</span></span>
<span id="cb67-390"><a href="#cb67-390"></a><span class="co">        Make class predictions</span></span>
<span id="cb67-391"><a href="#cb67-391"></a><span class="co">        </span></span>
<span id="cb67-392"><a href="#cb67-392"></a><span class="co">        Parameters</span></span>
<span id="cb67-393"><a href="#cb67-393"></a><span class="co">        ----------</span></span>
<span id="cb67-394"><a href="#cb67-394"></a><span class="co">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span></span>
<span id="cb67-395"><a href="#cb67-395"></a><span class="co">            Input data</span></span>
<span id="cb67-396"><a href="#cb67-396"></a><span class="co">            </span></span>
<span id="cb67-397"><a href="#cb67-397"></a><span class="co">        Returns</span></span>
<span id="cb67-398"><a href="#cb67-398"></a><span class="co">        -------</span></span>
<span id="cb67-399"><a href="#cb67-399"></a><span class="co">        y_pred : ndarray of shape (n_samples,)</span></span>
<span id="cb67-400"><a href="#cb67-400"></a><span class="co">            Predicted class labels</span></span>
<span id="cb67-401"><a href="#cb67-401"></a><span class="co">        """</span></span>
<span id="cb67-402"><a href="#cb67-402"></a>        check_is_fitted(<span class="va">self</span>)</span>
<span id="cb67-403"><a href="#cb67-403"></a>        <span class="va">self</span>._check_feature_names(X)</span>
<span id="cb67-404"><a href="#cb67-404"></a>        <span class="va">self</span>._check_n_features(X)</span>
<span id="cb67-405"><a href="#cb67-405"></a></span>
<span id="cb67-406"><a href="#cb67-406"></a>        df <span class="op">=</span> pd.DataFrame(X, columns<span class="op">=</span><span class="va">self</span>.feature_names_)</span>
<span id="cb67-407"><a href="#cb67-407"></a>        df <span class="op">=</span> TabularDataset(df)</span>
<span id="cb67-408"><a href="#cb67-408"></a></span>
<span id="cb67-409"><a href="#cb67-409"></a>        <span class="cf">return</span> <span class="va">self</span>.predictor.predict(df).values</span>
<span id="cb67-410"><a href="#cb67-410"></a></span>
<span id="cb67-411"><a href="#cb67-411"></a>    <span class="kw">def</span> predict_proba(<span class="va">self</span>, X):</span>
<span id="cb67-412"><a href="#cb67-412"></a>        <span class="co">"""</span></span>
<span id="cb67-413"><a href="#cb67-413"></a><span class="co">        Predict class probabilities</span></span>
<span id="cb67-414"><a href="#cb67-414"></a><span class="co">        </span></span>
<span id="cb67-415"><a href="#cb67-415"></a><span class="co">        Parameters</span></span>
<span id="cb67-416"><a href="#cb67-416"></a><span class="co">        ----------</span></span>
<span id="cb67-417"><a href="#cb67-417"></a><span class="co">        X : {array-like, sparse matrix} of shape (n_samples, n_features)</span></span>
<span id="cb67-418"><a href="#cb67-418"></a><span class="co">            Input data</span></span>
<span id="cb67-419"><a href="#cb67-419"></a><span class="co">            </span></span>
<span id="cb67-420"><a href="#cb67-420"></a><span class="co">        Returns</span></span>
<span id="cb67-421"><a href="#cb67-421"></a><span class="co">        -------</span></span>
<span id="cb67-422"><a href="#cb67-422"></a><span class="co">        proba : ndarray of shape (n_samples, n_classes)</span></span>
<span id="cb67-423"><a href="#cb67-423"></a><span class="co">            Class probabilities</span></span>
<span id="cb67-424"><a href="#cb67-424"></a><span class="co">        """</span></span>
<span id="cb67-425"><a href="#cb67-425"></a>        check_is_fitted(<span class="va">self</span>)</span>
<span id="cb67-426"><a href="#cb67-426"></a>        <span class="va">self</span>._check_feature_names(X)</span>
<span id="cb67-427"><a href="#cb67-427"></a>        <span class="va">self</span>._check_n_features(X)</span>
<span id="cb67-428"><a href="#cb67-428"></a></span>
<span id="cb67-429"><a href="#cb67-429"></a>        df <span class="op">=</span> pd.DataFrame(X, columns<span class="op">=</span><span class="va">self</span>.feature_names_)</span>
<span id="cb67-430"><a href="#cb67-430"></a>        df <span class="op">=</span> TabularDataset(df)</span>
<span id="cb67-431"><a href="#cb67-431"></a></span>
<span id="cb67-432"><a href="#cb67-432"></a>        <span class="cf">return</span> <span class="va">self</span>.predictor.predict_proba(df).values</span>
<span id="cb67-433"><a href="#cb67-433"></a></span>
<span id="cb67-434"><a href="#cb67-434"></a>    <span class="kw">def</span> get_params(<span class="va">self</span>, deep<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb67-435"><a href="#cb67-435"></a>        <span class="co">"""Get parameters for this estimator"""</span></span>
<span id="cb67-436"><a href="#cb67-436"></a>        <span class="cf">return</span> {</span>
<span id="cb67-437"><a href="#cb67-437"></a>            <span class="st">'label'</span>: <span class="va">self</span>.label,</span>
<span id="cb67-438"><a href="#cb67-438"></a>            <span class="st">'predictor_args'</span>: <span class="va">self</span>.predictor_args,</span>
<span id="cb67-439"><a href="#cb67-439"></a>            <span class="st">'fit_args'</span>: <span class="va">self</span>.fit_args</span>
<span id="cb67-440"><a href="#cb67-440"></a>        }</span>
<span id="cb67-441"><a href="#cb67-441"></a></span>
<span id="cb67-442"><a href="#cb67-442"></a>    <span class="kw">def</span> set_params(<span class="va">self</span>, <span class="op">**</span>params):</span>
<span id="cb67-443"><a href="#cb67-443"></a>        <span class="co">"""Set parameters for this estimator"""</span></span>
<span id="cb67-444"><a href="#cb67-444"></a>        <span class="cf">for</span> param, value <span class="kw">in</span> params.items():</span>
<span id="cb67-445"><a href="#cb67-445"></a>            <span class="cf">if</span> param <span class="op">==</span> <span class="st">'label'</span>:</span>
<span id="cb67-446"><a href="#cb67-446"></a>                <span class="va">self</span>.label <span class="op">=</span> value</span>
<span id="cb67-447"><a href="#cb67-447"></a>            <span class="cf">else</span>:</span>
<span id="cb67-448"><a href="#cb67-448"></a>                <span class="va">self</span>.predictor_args[param] <span class="op">=</span> value</span>
<span id="cb67-449"><a href="#cb67-449"></a>        <span class="cf">return</span> <span class="va">self</span></span>
<span id="cb67-450"><a href="#cb67-450"></a></span>
<span id="cb67-451"><a href="#cb67-451"></a>    <span class="kw">def</span> _check_n_features(<span class="va">self</span>, X, reset<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb67-452"><a href="#cb67-452"></a>        <span class="co">"""Validate number of features"""</span></span>
<span id="cb67-453"><a href="#cb67-453"></a>        n_features <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb67-454"><a href="#cb67-454"></a>        <span class="cf">if</span> reset:</span>
<span id="cb67-455"><a href="#cb67-455"></a>            <span class="va">self</span>.n_features_in_ <span class="op">=</span> n_features</span>
<span id="cb67-456"><a href="#cb67-456"></a>        <span class="cf">elif</span> n_features <span class="op">!=</span> <span class="va">self</span>.n_features_in_:</span>
<span id="cb67-457"><a href="#cb67-457"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"Expected </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>n_features_in_<span class="sc">}</span><span class="ss"> features, got </span><span class="sc">{</span>n_features<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-458"><a href="#cb67-458"></a></span>
<span id="cb67-459"><a href="#cb67-459"></a>    <span class="kw">def</span> _check_feature_names(<span class="va">self</span>, X, reset<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb67-460"><a href="#cb67-460"></a>        <span class="co">"""Validate feature names (AutoGluon requirement)"""</span></span>
<span id="cb67-461"><a href="#cb67-461"></a>        <span class="cf">if</span> reset:</span>
<span id="cb67-462"><a href="#cb67-462"></a>            <span class="cf">if</span> <span class="bu">isinstance</span>(X, np.ndarray):</span>
<span id="cb67-463"><a href="#cb67-463"></a>                <span class="va">self</span>.feature_names_ <span class="op">=</span> [<span class="ss">f'feat_</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(X.shape[<span class="dv">1</span>])]</span>
<span id="cb67-464"><a href="#cb67-464"></a>            <span class="cf">else</span>:</span>
<span id="cb67-465"><a href="#cb67-465"></a>                <span class="va">self</span>.feature_names_ <span class="op">=</span> X.columns.tolist()</span>
<span id="cb67-466"><a href="#cb67-466"></a>        <span class="cf">elif</span> <span class="bu">hasattr</span>(X, <span class="st">'columns'</span>):</span>
<span id="cb67-467"><a href="#cb67-467"></a>            <span class="cf">if</span> <span class="bu">list</span>(X.columns) <span class="op">!=</span> <span class="va">self</span>.feature_names_:</span>
<span id="cb67-468"><a href="#cb67-468"></a>                <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Feature names mismatch between fit and predict"</span>)</span>
<span id="cb67-469"><a href="#cb67-469"></a></span>
<span id="cb67-470"><a href="#cb67-470"></a><span class="kw">def</span> calculate_score(prob_default, pdo<span class="op">=</span><span class="dv">40</span>, base_score<span class="op">=</span><span class="dv">600</span>):</span>
<span id="cb67-471"><a href="#cb67-471"></a>    <span class="co">"""Converts probability of default to a 3-digit score using OddsBad."""</span></span>
<span id="cb67-472"><a href="#cb67-472"></a>    <span class="co"># Avoid log(0) or division by zero</span></span>
<span id="cb67-473"><a href="#cb67-473"></a>    eps <span class="op">=</span> <span class="fl">1e-9</span></span>
<span id="cb67-474"><a href="#cb67-474"></a>    prob_default <span class="op">=</span> np.clip(prob_default, eps, <span class="dv">1</span> <span class="op">-</span> eps)</span>
<span id="cb67-475"><a href="#cb67-475"></a>    odds_bad <span class="op">=</span> prob_default <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> prob_default)  <span class="co"># Bad/Good odds</span></span>
<span id="cb67-476"><a href="#cb67-476"></a>    factor <span class="op">=</span> pdo <span class="op">/</span> np.log(<span class="dv">2</span>)</span>
<span id="cb67-477"><a href="#cb67-477"></a>    <span class="co"># Score = Base - Factor * log(OddsBad)</span></span>
<span id="cb67-478"><a href="#cb67-478"></a>    score <span class="op">=</span> base_score <span class="op">-</span> factor <span class="op">*</span> np.log(odds_bad)</span>
<span id="cb67-479"><a href="#cb67-479"></a>    <span class="co"># Clip score to a reasonable range, e.g., 300-850</span></span>
<span id="cb67-480"><a href="#cb67-480"></a>    <span class="cf">return</span> np.clip(score, <span class="dv">300</span>, <span class="dv">850</span>).astype(<span class="bu">int</span>)</span>
<span id="cb67-481"><a href="#cb67-481"></a></span>
<span id="cb67-482"><a href="#cb67-482"></a><span class="kw">def</span> score_to_probability(score, pdo<span class="op">=</span><span class="dv">40</span>, base_score<span class="op">=</span><span class="dv">600</span>):</span>
<span id="cb67-483"><a href="#cb67-483"></a>    <span class="co">"""Converts a credit score back to probability of default.</span></span>
<span id="cb67-484"><a href="#cb67-484"></a><span class="co">    </span></span>
<span id="cb67-485"><a href="#cb67-485"></a><span class="co">    This is the inverse of calculate_score function.</span></span>
<span id="cb67-486"><a href="#cb67-486"></a><span class="co">    </span></span>
<span id="cb67-487"><a href="#cb67-487"></a><span class="co">    Parameters:</span></span>
<span id="cb67-488"><a href="#cb67-488"></a><span class="co">        score (int or float): Credit score to convert</span></span>
<span id="cb67-489"><a href="#cb67-489"></a><span class="co">        pdo (int): Points to Double the Odds, default 40</span></span>
<span id="cb67-490"><a href="#cb67-490"></a><span class="co">        base_score (int): Base score, default 600</span></span>
<span id="cb67-491"><a href="#cb67-491"></a><span class="co">    </span></span>
<span id="cb67-492"><a href="#cb67-492"></a><span class="co">    Returns:</span></span>
<span id="cb67-493"><a href="#cb67-493"></a><span class="co">        float: Probability of default [0, 1]</span></span>
<span id="cb67-494"><a href="#cb67-494"></a><span class="co">    """</span></span>
<span id="cb67-495"><a href="#cb67-495"></a>    <span class="co"># Calculate factor same as in the score calculation</span></span>
<span id="cb67-496"><a href="#cb67-496"></a>    factor <span class="op">=</span> pdo <span class="op">/</span> np.log(<span class="dv">2</span>)</span>
<span id="cb67-497"><a href="#cb67-497"></a>    </span>
<span id="cb67-498"><a href="#cb67-498"></a>    <span class="co"># Calculate the odds_bad from the score</span></span>
<span id="cb67-499"><a href="#cb67-499"></a>    odds_bad <span class="op">=</span> np.exp((score <span class="op">-</span> base_score) <span class="op">/</span> <span class="op">-</span>factor)</span>
<span id="cb67-500"><a href="#cb67-500"></a>    </span>
<span id="cb67-501"><a href="#cb67-501"></a>    <span class="co"># Convert odds to probability</span></span>
<span id="cb67-502"><a href="#cb67-502"></a>    prob_default <span class="op">=</span> odds_bad <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> odds_bad)</span>
<span id="cb67-503"><a href="#cb67-503"></a>    </span>
<span id="cb67-504"><a href="#cb67-504"></a>    <span class="cf">return</span> prob_default</span>
<span id="cb67-505"><a href="#cb67-505"></a></span>
<span id="cb67-506"><a href="#cb67-506"></a><span class="kw">def</span> parse_emp_length(x):</span>
<span id="cb67-507"><a href="#cb67-507"></a>    <span class="co">"""Convert employment length string to numeric years.</span></span>
<span id="cb67-508"><a href="#cb67-508"></a><span class="co">    </span></span>
<span id="cb67-509"><a href="#cb67-509"></a><span class="co">    Returns:</span></span>
<span id="cb67-510"><a href="#cb67-510"></a><span class="co">        int: Numeric representation of employment length.</span></span>
<span id="cb67-511"><a href="#cb67-511"></a><span class="co">             - 0 for "n/a" or "&lt; 1 year".</span></span>
<span id="cb67-512"><a href="#cb67-512"></a><span class="co">             - 11 for "10+ years".</span></span>
<span id="cb67-513"><a href="#cb67-513"></a><span class="co">             - Extracted number for other valid formats.</span></span>
<span id="cb67-514"><a href="#cb67-514"></a><span class="co">             - -1 for unexpected formats.</span></span>
<span id="cb67-515"><a href="#cb67-515"></a><span class="co">    """</span></span>
<span id="cb67-516"><a href="#cb67-516"></a>    <span class="cf">if</span> pd.isna(x) <span class="kw">or</span> x <span class="op">==</span> <span class="st">"n/a"</span>:</span>
<span id="cb67-517"><a href="#cb67-517"></a>        <span class="cf">return</span> <span class="dv">0</span></span>
<span id="cb67-518"><a href="#cb67-518"></a>    <span class="cf">elif</span> <span class="st">"&lt; 1 year"</span> <span class="kw">in</span> x:</span>
<span id="cb67-519"><a href="#cb67-519"></a>        <span class="cf">return</span> <span class="dv">0</span></span>
<span id="cb67-520"><a href="#cb67-520"></a>    <span class="cf">elif</span> <span class="st">"10+ years"</span> <span class="kw">in</span> x:</span>
<span id="cb67-521"><a href="#cb67-521"></a>        <span class="cf">return</span> <span class="dv">10</span></span>
<span id="cb67-522"><a href="#cb67-522"></a>    <span class="cf">try</span>:</span>
<span id="cb67-523"><a href="#cb67-523"></a>        <span class="cf">return</span> <span class="bu">int</span>(re.findall(<span class="vs">r'\d+'</span>, <span class="bu">str</span>(x))[<span class="dv">0</span>])</span>
<span id="cb67-524"><a href="#cb67-524"></a>    <span class="cf">except</span>:</span>
<span id="cb67-525"><a href="#cb67-525"></a>        <span class="cf">return</span> <span class="op">-</span><span class="dv">1</span>  <span class="co"># Fallback for unexpected formats</span></span>
<span id="cb67-526"><a href="#cb67-526"></a></span>
<span id="cb67-527"><a href="#cb67-527"></a></span>
<span id="cb67-528"><a href="#cb67-528"></a></span>
<span id="cb67-529"><a href="#cb67-529"></a><span class="kw">def</span> ks_table(data: pd.DataFrame, y_true_col: <span class="bu">str</span>, y_pred_col: <span class="bu">str</span>, n_bins: <span class="bu">int</span> <span class="op">=</span> <span class="dv">10</span>, is_score: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>, sample_weight_col: <span class="bu">str</span> <span class="op">|</span> <span class="va">None</span> <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb67-530"><a href="#cb67-530"></a>    <span class="co">"""</span></span>
<span id="cb67-531"><a href="#cb67-531"></a><span class="co">    Generates a KS table from a DataFrame using either probabilities or scores,</span></span>
<span id="cb67-532"><a href="#cb67-532"></a><span class="co">    optionally using sample weights.</span></span>
<span id="cb67-533"><a href="#cb67-533"></a></span>
<span id="cb67-534"><a href="#cb67-534"></a><span class="co">    Parameters:</span></span>
<span id="cb67-535"><a href="#cb67-535"></a><span class="co">        data (pd.DataFrame): DataFrame containing the true labels and predicted probabilities (or scores).</span></span>
<span id="cb67-536"><a href="#cb67-536"></a><span class="co">        y_true_col (str): Name of the column with the true binary labels (0 or 1).</span></span>
<span id="cb67-537"><a href="#cb67-537"></a><span class="co">        y_pred_col (str): Name of the column with the predicted probabilities (higher=riskier) or scores (lower=riskier).</span></span>
<span id="cb67-538"><a href="#cb67-538"></a><span class="co">        n_bins (int): Number of bins to divide the values into.</span></span>
<span id="cb67-539"><a href="#cb67-539"></a><span class="co">        is_score (bool): Set to True if y_pred_col contains scores (lower=riskier),</span></span>
<span id="cb67-540"><a href="#cb67-540"></a><span class="co">                            False if it contains probabilities (higher=riskier). Default is False.</span></span>
<span id="cb67-541"><a href="#cb67-541"></a><span class="co">        sample_weight_col (str | None): Name of the column containing sample weights.</span></span>
<span id="cb67-542"><a href="#cb67-542"></a><span class="co">                                        If None, all samples have weight 1. Default is None.</span></span>
<span id="cb67-543"><a href="#cb67-543"></a></span>
<span id="cb67-544"><a href="#cb67-544"></a><span class="co">    Returns:</span></span>
<span id="cb67-545"><a href="#cb67-545"></a><span class="co">        pd.DataFrame: The KS table.</span></span>
<span id="cb67-546"><a href="#cb67-546"></a><span class="co">    """</span></span>
<span id="cb67-547"><a href="#cb67-547"></a>    <span class="co"># Select relevant columns and work on a copy</span></span>
<span id="cb67-548"><a href="#cb67-548"></a>    cols_to_select <span class="op">=</span> [y_true_col, y_pred_col]</span>
<span id="cb67-549"><a href="#cb67-549"></a>    <span class="cf">if</span> sample_weight_col:</span>
<span id="cb67-550"><a href="#cb67-550"></a>        cols_to_select.append(sample_weight_col)</span>
<span id="cb67-551"><a href="#cb67-551"></a></span>
<span id="cb67-552"><a href="#cb67-552"></a>    df <span class="op">=</span> data[cols_to_select].copy()</span>
<span id="cb67-553"><a href="#cb67-553"></a></span>
<span id="cb67-554"><a href="#cb67-554"></a></span>
<span id="cb67-555"><a href="#cb67-555"></a>    <span class="co"># Rename columns for internal consistency</span></span>
<span id="cb67-556"><a href="#cb67-556"></a>    rename_map <span class="op">=</span> {y_true_col: <span class="st">'y_true'</span>, y_pred_col: <span class="st">'y_pred'</span>}</span>
<span id="cb67-557"><a href="#cb67-557"></a>    <span class="cf">if</span> sample_weight_col <span class="kw">in</span> df.columns:</span>
<span id="cb67-558"><a href="#cb67-558"></a>        rename_map[sample_weight_col] <span class="op">=</span> <span class="st">'weight'</span></span>
<span id="cb67-559"><a href="#cb67-559"></a>        <span class="co"># Ensure weights are numeric and fill NaNs with 1 (or raise error if preferred)</span></span>
<span id="cb67-560"><a href="#cb67-560"></a>        df[<span class="st">'weight'</span>] <span class="op">=</span> pd.to_numeric(df[sample_weight_col], errors<span class="op">=</span><span class="st">'coerce'</span>).fillna(<span class="fl">1.0</span>)</span>
<span id="cb67-561"><a href="#cb67-561"></a>    <span class="cf">else</span>:</span>
<span id="cb67-562"><a href="#cb67-562"></a>        <span class="co"># Assign weight of 1 if no weight column provided or if column name was invalid</span></span>
<span id="cb67-563"><a href="#cb67-563"></a>        df[<span class="st">'weight'</span>] <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb67-564"><a href="#cb67-564"></a>        <span class="cf">if</span> sample_weight_col <span class="kw">not</span> <span class="kw">in</span> data.columns:</span>
<span id="cb67-565"><a href="#cb67-565"></a>             <span class="bu">print</span>(<span class="ss">f"Warning: sample_weight_col '</span><span class="sc">{</span>sample_weight_col<span class="sc">}</span><span class="ss">' not found. Using weight=1 for all samples."</span>)</span>
<span id="cb67-566"><a href="#cb67-566"></a></span>
<span id="cb67-567"><a href="#cb67-567"></a>    df.rename(columns<span class="op">=</span>rename_map, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb67-568"><a href="#cb67-568"></a></span>
<span id="cb67-569"><a href="#cb67-569"></a></span>
<span id="cb67-570"><a href="#cb67-570"></a>    <span class="co"># Handle potential NaN values - drop rows where prediction is NaN</span></span>
<span id="cb67-571"><a href="#cb67-571"></a>    df.dropna(subset<span class="op">=</span>[<span class="st">'y_pred'</span>], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb67-572"><a href="#cb67-572"></a>    <span class="cf">if</span> df.empty:</span>
<span id="cb67-573"><a href="#cb67-573"></a>        <span class="bu">print</span>(<span class="st">"Warning: No valid data points after dropping NaN values for KS table."</span>)</span>
<span id="cb67-574"><a href="#cb67-574"></a>        <span class="cf">return</span> pd.DataFrame() <span class="co"># Return empty DataFrame</span></span>
<span id="cb67-575"><a href="#cb67-575"></a></span>
<span id="cb67-576"><a href="#cb67-576"></a>    <span class="co"># Bin values</span></span>
<span id="cb67-577"><a href="#cb67-577"></a>    <span class="co"># Use rank(method='first') to handle non-unique bin edges better if duplicates='drop' fails</span></span>
<span id="cb67-578"><a href="#cb67-578"></a>    <span class="cf">try</span>:</span>
<span id="cb67-579"><a href="#cb67-579"></a>        <span class="co"># Create bins based on quantiles of the value column</span></span>
<span id="cb67-580"><a href="#cb67-580"></a>        df[<span class="st">"bin"</span>] <span class="op">=</span> pd.qcut(df[<span class="st">"y_pred"</span>].rank(method<span class="op">=</span><span class="st">'first'</span>), q<span class="op">=</span>n_bins, labels<span class="op">=</span><span class="va">False</span>, duplicates<span class="op">=</span><span class="st">'drop'</span>)</span>
<span id="cb67-581"><a href="#cb67-581"></a>    <span class="cf">except</span> <span class="pp">ValueError</span> <span class="im">as</span> e:</span>
<span id="cb67-582"><a href="#cb67-582"></a>        <span class="co"># Fallback if qcut still fails (e.g., too few unique values)</span></span>
<span id="cb67-583"><a href="#cb67-583"></a>        <span class="bu">print</span>(<span class="ss">f"Warning: pd.qcut failed (</span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">). Trying with fewer bins or manual ranking."</span>)</span>
<span id="cb67-584"><a href="#cb67-584"></a>        <span class="co"># As a simple fallback, use ranking and divide into bins manually</span></span>
<span id="cb67-585"><a href="#cb67-585"></a>        ranks <span class="op">=</span> df[<span class="st">"y_pred"</span>].rank(method<span class="op">=</span><span class="st">'first'</span>)</span>
<span id="cb67-586"><a href="#cb67-586"></a>        bin_size <span class="op">=</span> <span class="bu">max</span>(<span class="dv">1</span>, <span class="bu">len</span>(df) <span class="op">//</span> n_bins) <span class="co"># Ensure bin_size is at least 1</span></span>
<span id="cb67-587"><a href="#cb67-587"></a>        df[<span class="st">'bin'</span>] <span class="op">=</span> ((ranks <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> bin_size).clip(upper<span class="op">=</span>n_bins <span class="op">-</span> <span class="dv">1</span>).astype(<span class="bu">int</span>)</span>
<span id="cb67-588"><a href="#cb67-588"></a></span>
<span id="cb67-589"><a href="#cb67-589"></a></span>
<span id="cb67-590"><a href="#cb67-590"></a>    <span class="co"># Determine sorting order based on whether input is score or probability</span></span>
<span id="cb67-591"><a href="#cb67-591"></a>    <span class="co"># If score (lower=riskier), sort bins by ascending min_value (lowest scores first)</span></span>
<span id="cb67-592"><a href="#cb67-592"></a>    <span class="co"># If probability (higher=riskier), sort bins by descending min_value (highest probs first)</span></span>
<span id="cb67-593"><a href="#cb67-593"></a>    sort_ascending <span class="op">=</span> is_score</span>
<span id="cb67-594"><a href="#cb67-594"></a></span>
<span id="cb67-595"><a href="#cb67-595"></a>    <span class="co"># Use duckdb to perform aggregations</span></span>
<span id="cb67-596"><a href="#cb67-596"></a></span>
<span id="cb67-597"><a href="#cb67-597"></a>    ks_df <span class="op">=</span> duckdb.query(</span>
<span id="cb67-598"><a href="#cb67-598"></a>        <span class="st">"""</span></span>
<span id="cb67-599"><a href="#cb67-599"></a><span class="st">        SELECT</span></span>
<span id="cb67-600"><a href="#cb67-600"></a><span class="st">            bin,</span></span>
<span id="cb67-601"><a href="#cb67-601"></a><span class="st">            MIN(y_pred) AS min_value,</span></span>
<span id="cb67-602"><a href="#cb67-602"></a><span class="st">            MAX(y_pred) AS max_value,</span></span>
<span id="cb67-603"><a href="#cb67-603"></a><span class="st">            AVG(y_pred) AS avg_value,</span></span>
<span id="cb67-604"><a href="#cb67-604"></a><span class="st">            SUM(weight) AS count,</span></span>
<span id="cb67-605"><a href="#cb67-605"></a><span class="st">            SUM(y_true * weight) AS bads</span></span>
<span id="cb67-606"><a href="#cb67-606"></a><span class="st">        FROM df</span></span>
<span id="cb67-607"><a href="#cb67-607"></a><span class="st">        GROUP BY bin</span></span>
<span id="cb67-608"><a href="#cb67-608"></a><span class="st">        """</span></span>
<span id="cb67-609"><a href="#cb67-609"></a>    ).to_df()</span>
<span id="cb67-610"><a href="#cb67-610"></a></span>
<span id="cb67-611"><a href="#cb67-611"></a>    <span class="co"># Reset index and sort</span></span>
<span id="cb67-612"><a href="#cb67-612"></a>    ks_df <span class="op">=</span> ks_df.reset_index().sort_values(<span class="st">"min_value"</span>, ascending<span class="op">=</span>sort_ascending)</span>
<span id="cb67-613"><a href="#cb67-613"></a></span>
<span id="cb67-614"><a href="#cb67-614"></a></span>
<span id="cb67-615"><a href="#cb67-615"></a>    ks_df[<span class="st">"goods"</span>] <span class="op">=</span> ks_df[<span class="st">"count"</span>] <span class="op">-</span> ks_df[<span class="st">"bads"</span>] <span class="co"># Total weight - bad weight = good weight</span></span>
<span id="cb67-616"><a href="#cb67-616"></a>    <span class="co"># Avoid division by zero if a bin has zero total weight</span></span>
<span id="cb67-617"><a href="#cb67-617"></a>    <span class="co"># Use np.where for safer division</span></span>
<span id="cb67-618"><a href="#cb67-618"></a>    ks_df[<span class="st">"bad_rate"</span>] <span class="op">=</span> np.where(ks_df[<span class="st">"count"</span>] <span class="op">&gt;</span> <span class="fl">1e-9</span>, ks_df[<span class="st">"bads"</span>] <span class="op">/</span> ks_df[<span class="st">"count"</span>], <span class="dv">0</span>)</span>
<span id="cb67-619"><a href="#cb67-619"></a></span>
<span id="cb67-620"><a href="#cb67-620"></a></span>
<span id="cb67-621"><a href="#cb67-621"></a>    total_bads <span class="op">=</span> ks_df[<span class="st">"bads"</span>].<span class="bu">sum</span>() <span class="co"># Total bad weight</span></span>
<span id="cb67-622"><a href="#cb67-622"></a>    total_goods <span class="op">=</span> ks_df[<span class="st">"goods"</span>].<span class="bu">sum</span>() <span class="co"># Total good weight</span></span>
<span id="cb67-623"><a href="#cb67-623"></a></span>
<span id="cb67-624"><a href="#cb67-624"></a>    <span class="co"># Avoid division by zero if there are no bads or no goods (based on weight)</span></span>
<span id="cb67-625"><a href="#cb67-625"></a>    <span class="cf">if</span> total_bads <span class="op">&lt;=</span> <span class="fl">1e-9</span> <span class="kw">or</span> total_goods <span class="op">&lt;=</span> <span class="fl">1e-9</span>: <span class="co"># Use small threshold for float comparison</span></span>
<span id="cb67-626"><a href="#cb67-626"></a>        ks_df[<span class="st">"cum_bads_pct"</span>] <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb67-627"><a href="#cb67-627"></a>        ks_df[<span class="st">"cum_goods_pct"</span>] <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb67-628"><a href="#cb67-628"></a>        ks_df[<span class="st">"ks"</span>] <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb67-629"><a href="#cb67-629"></a>        <span class="bu">print</span>(<span class="st">"Warning: KS calculation skipped as total weighted goods or bads is effectively zero."</span>)</span>
<span id="cb67-630"><a href="#cb67-630"></a>    <span class="cf">else</span>:</span>
<span id="cb67-631"><a href="#cb67-631"></a>        ks_df[<span class="st">"cum_bads_pct"</span>] <span class="op">=</span> (ks_df[<span class="st">"bads"</span>].cumsum() <span class="op">/</span> total_bads) <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb67-632"><a href="#cb67-632"></a>        ks_df[<span class="st">"cum_goods_pct"</span>] <span class="op">=</span> (ks_df[<span class="st">"goods"</span>].cumsum() <span class="op">/</span> total_goods) <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb67-633"><a href="#cb67-633"></a>        ks_df[<span class="st">"ks"</span>] <span class="op">=</span> np.<span class="bu">abs</span>(ks_df[<span class="st">"cum_bads_pct"</span>] <span class="op">-</span> ks_df[<span class="st">"cum_goods_pct"</span>])</span>
<span id="cb67-634"><a href="#cb67-634"></a></span>
<span id="cb67-635"><a href="#cb67-635"></a>    <span class="co"># Rename value columns back for clarity in output *before* printing max KS info</span></span>
<span id="cb67-636"><a href="#cb67-636"></a>    ks_df.rename(columns<span class="op">=</span>{</span>
<span id="cb67-637"><a href="#cb67-637"></a>        <span class="st">'min_value'</span>: <span class="ss">f'min_</span><span class="sc">{</span>y_pred_col<span class="sc">}</span><span class="ss">'</span>,</span>
<span id="cb67-638"><a href="#cb67-638"></a>        <span class="st">'max_value'</span>: <span class="ss">f'max_</span><span class="sc">{</span>y_pred_col<span class="sc">}</span><span class="ss">'</span>,</span>
<span id="cb67-639"><a href="#cb67-639"></a>        <span class="st">'avg_value'</span>: <span class="ss">f'avg_</span><span class="sc">{</span>y_pred_col<span class="sc">}</span><span class="ss">'</span> <span class="co"># Rename average column</span></span>
<span id="cb67-640"><a href="#cb67-640"></a>        }, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb67-641"><a href="#cb67-641"></a></span>
<span id="cb67-642"><a href="#cb67-642"></a>    <span class="co"># Print the KS statistic and associated bin info before returning</span></span>
<span id="cb67-643"><a href="#cb67-643"></a>    <span class="cf">if</span> <span class="st">'ks'</span> <span class="kw">in</span> ks_df.columns <span class="kw">and</span> <span class="kw">not</span> ks_df[<span class="st">'ks'</span>].empty <span class="kw">and</span> total_bads <span class="op">&gt;</span> <span class="fl">1e-9</span> <span class="kw">and</span> total_goods <span class="op">&gt;</span> <span class="fl">1e-9</span>:</span>
<span id="cb67-644"><a href="#cb67-644"></a>        max_ks <span class="op">=</span> ks_df[<span class="st">'ks'</span>].<span class="bu">max</span>()</span>
<span id="cb67-645"><a href="#cb67-645"></a>        <span class="co"># Handle potential multiple max KS values - take the first one</span></span>
<span id="cb67-646"><a href="#cb67-646"></a>        max_ks_row <span class="op">=</span> ks_df.loc[ks_df[<span class="st">'ks'</span>].idxmax()]</span>
<span id="cb67-647"><a href="#cb67-647"></a>        min_val_at_max_ks <span class="op">=</span> max_ks_row[<span class="ss">f'min_</span><span class="sc">{</span>y_pred_col<span class="sc">}</span><span class="ss">'</span>]</span>
<span id="cb67-648"><a href="#cb67-648"></a>        max_val_at_max_ks <span class="op">=</span> max_ks_row[<span class="ss">f'max_</span><span class="sc">{</span>y_pred_col<span class="sc">}</span><span class="ss">'</span>]</span>
<span id="cb67-649"><a href="#cb67-649"></a>        avg_val_at_max_ks <span class="op">=</span> max_ks_row[<span class="ss">f'avg_</span><span class="sc">{</span>y_pred_col<span class="sc">}</span><span class="ss">'</span>]</span>
<span id="cb67-650"><a href="#cb67-650"></a></span>
<span id="cb67-651"><a href="#cb67-651"></a>        <span class="bu">print</span>(<span class="ss">f"KS Statistic (Max KS): </span><span class="sc">{</span>max_ks<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb67-652"><a href="#cb67-652"></a>        <span class="bu">print</span>(<span class="ss">f"  Occurs in bin with </span><span class="sc">{</span>y_pred_col<span class="sc">}</span><span class="ss"> range: [</span><span class="sc">{</span>min_val_at_max_ks<span class="sc">:.4f}</span><span class="ss"> - </span><span class="sc">{</span>max_val_at_max_ks<span class="sc">:.4f}</span><span class="ss">]"</span>)</span>
<span id="cb67-653"><a href="#cb67-653"></a>        <span class="bu">print</span>(<span class="ss">f"  Average </span><span class="sc">{</span>y_pred_col<span class="sc">}</span><span class="ss"> in this bin: </span><span class="sc">{</span>avg_val_at_max_ks<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb67-654"><a href="#cb67-654"></a>    <span class="cf">elif</span> total_bads <span class="op">&lt;=</span> <span class="fl">1e-9</span> <span class="kw">or</span> total_goods <span class="op">&lt;=</span> <span class="fl">1e-9</span>:</span>
<span id="cb67-655"><a href="#cb67-655"></a>        <span class="bu">print</span>(<span class="st">"KS Statistic is 0 because total weighted goods or bads is effectively zero."</span>)</span>
<span id="cb67-656"><a href="#cb67-656"></a>    <span class="cf">else</span>:</span>
<span id="cb67-657"><a href="#cb67-657"></a>        <span class="bu">print</span>(<span class="st">"KS Statistic could not be calculated (check input data and binning)."</span>)</span>
<span id="cb67-658"><a href="#cb67-658"></a></span>
<span id="cb67-659"><a href="#cb67-659"></a></span>
<span id="cb67-660"><a href="#cb67-660"></a>    <span class="co"># Reorder columns for final output</span></span>
<span id="cb67-661"><a href="#cb67-661"></a>    final_cols <span class="op">=</span> [</span>
<span id="cb67-662"><a href="#cb67-662"></a>            <span class="ss">f"min_</span><span class="sc">{</span>y_pred_col<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb67-663"><a href="#cb67-663"></a>            <span class="ss">f"max_</span><span class="sc">{</span>y_pred_col<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb67-664"><a href="#cb67-664"></a>            <span class="ss">f"avg_</span><span class="sc">{</span>y_pred_col<span class="sc">}</span><span class="ss">"</span>, <span class="co"># Add average column to output list</span></span>
<span id="cb67-665"><a href="#cb67-665"></a>            <span class="st">"count"</span>, <span class="co"># Represents total weight (or count if unweighted)</span></span>
<span id="cb67-666"><a href="#cb67-666"></a>            <span class="st">"bads"</span>,  <span class="co"># Represents total bad weight (or bad count if unweighted)</span></span>
<span id="cb67-667"><a href="#cb67-667"></a>            <span class="st">"goods"</span>, <span class="co"># Represents total good weight (or good count if unweighted)</span></span>
<span id="cb67-668"><a href="#cb67-668"></a>            <span class="st">"bad_rate"</span>, <span class="co"># Weighted bad rate</span></span>
<span id="cb67-669"><a href="#cb67-669"></a>            <span class="st">"cum_bads_pct"</span>,</span>
<span id="cb67-670"><a href="#cb67-670"></a>            <span class="st">"cum_goods_pct"</span>,</span>
<span id="cb67-671"><a href="#cb67-671"></a>            <span class="st">"ks"</span>,</span>
<span id="cb67-672"><a href="#cb67-672"></a>        ]</span>
<span id="cb67-673"><a href="#cb67-673"></a>    <span class="co"># Ensure all expected columns exist before selecting</span></span>
<span id="cb67-674"><a href="#cb67-674"></a>    final_cols <span class="op">=</span> [col <span class="cf">for</span> col <span class="kw">in</span> final_cols <span class="cf">if</span> col <span class="kw">in</span> ks_df.columns]</span>
<span id="cb67-675"><a href="#cb67-675"></a>    <span class="cf">return</span> ks_df[final_cols].reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb67-676"><a href="#cb67-676"></a></span>
<span id="cb67-677"><a href="#cb67-677"></a></span>
<span id="cb67-678"><a href="#cb67-678"></a><span class="kw">def</span> show_pdp(wrappedAGModel: BaseEstimator,</span>
<span id="cb67-679"><a href="#cb67-679"></a>            list_features: <span class="bu">list</span>,</span>
<span id="cb67-680"><a href="#cb67-680"></a>            list_categ_features: <span class="bu">list</span>,</span>
<span id="cb67-681"><a href="#cb67-681"></a>            df: pd.DataFrame,</span>
<span id="cb67-682"><a href="#cb67-682"></a>            xGTzero: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb67-683"><a href="#cb67-683"></a>            sampSize: <span class="bu">int</span> <span class="op">=</span> <span class="dv">40000</span>,</span>
<span id="cb67-684"><a href="#cb67-684"></a>            show_ice: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>) <span class="op">-&gt;</span> <span class="va">None</span>: <span class="co"># Added show_ice parameter</span></span>
<span id="cb67-685"><a href="#cb67-685"></a></span>
<span id="cb67-686"><a href="#cb67-686"></a>    <span class="cf">for</span> feature <span class="kw">in</span> list_features:</span>
<span id="cb67-687"><a href="#cb67-687"></a></span>
<span id="cb67-688"><a href="#cb67-688"></a>        fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">4</span>))</span>
<span id="cb67-689"><a href="#cb67-689"></a>        ax <span class="op">=</span> fig.add_subplot(<span class="dv">111</span>)</span>
<span id="cb67-690"><a href="#cb67-690"></a></span>
<span id="cb67-691"><a href="#cb67-691"></a>        plt.rcParams.update({<span class="st">'font.size'</span>: <span class="dv">16</span>})</span>
<span id="cb67-692"><a href="#cb67-692"></a></span>
<span id="cb67-693"><a href="#cb67-693"></a>        <span class="co"># Determine kind and subsample based on show_ice</span></span>
<span id="cb67-694"><a href="#cb67-694"></a>        plot_kind <span class="op">=</span> <span class="st">'both'</span> <span class="cf">if</span> show_ice <span class="cf">else</span> <span class="st">'average'</span></span>
<span id="cb67-695"><a href="#cb67-695"></a>        ice_subsample <span class="op">=</span> <span class="dv">250</span> <span class="cf">if</span> show_ice <span class="cf">else</span> <span class="va">None</span> <span class="co"># Subsample for ICE lines</span></span>
<span id="cb67-696"><a href="#cb67-696"></a>        ice_lines_kw <span class="op">=</span> {<span class="st">"color"</span>: <span class="st">"tab:blue"</span>, <span class="st">"alpha"</span>: <span class="fl">0.2</span>, <span class="st">"linewidth"</span>: <span class="fl">0.5</span>} <span class="cf">if</span> show_ice <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb67-697"><a href="#cb67-697"></a>        pd_line_kw <span class="op">=</span> {<span class="st">"color"</span>: <span class="st">"tab:red"</span>, <span class="st">"linestyle"</span>: <span class="st">"--"</span>, <span class="st">"linewidth"</span>: <span class="dv">2</span>}</span>
<span id="cb67-698"><a href="#cb67-698"></a></span>
<span id="cb67-699"><a href="#cb67-699"></a>        <span class="co"># Get the expected feature order from the fitted model</span></span>
<span id="cb67-700"><a href="#cb67-700"></a>        feature_order <span class="op">=</span> wrappedAGModel.feature_names_</span>
<span id="cb67-701"><a href="#cb67-701"></a>        <span class="co"># Sample data for the main PDP calculation, ensuring correct column order</span></span>
<span id="cb67-702"><a href="#cb67-702"></a>        X_sample <span class="op">=</span> df[feature_order].sample(<span class="bu">min</span>(sampSize, <span class="bu">len</span>(df)), random_state<span class="op">=</span><span class="dv">2025</span>) <span class="co"># Reorder columns before sampling</span></span>
<span id="cb67-703"><a href="#cb67-703"></a></span>
<span id="cb67-704"><a href="#cb67-704"></a>        disp <span class="op">=</span> PartialDependenceDisplay.from_estimator(</span>
<span id="cb67-705"><a href="#cb67-705"></a>            estimator <span class="op">=</span> wrappedAGModel,</span>
<span id="cb67-706"><a href="#cb67-706"></a>            X <span class="op">=</span> X_sample, <span class="co"># Use the sampled data with correct column order</span></span>
<span id="cb67-707"><a href="#cb67-707"></a>            features <span class="op">=</span> [feature],</span>
<span id="cb67-708"><a href="#cb67-708"></a>            categorical_features <span class="op">=</span> list_categ_features,</span>
<span id="cb67-709"><a href="#cb67-709"></a>            method <span class="op">=</span> <span class="st">'brute'</span>,</span>
<span id="cb67-710"><a href="#cb67-710"></a>            kind <span class="op">=</span> plot_kind, <span class="co"># Use 'both' or 'average'</span></span>
<span id="cb67-711"><a href="#cb67-711"></a>            subsample <span class="op">=</span> ice_subsample, <span class="co"># Subsample for ICE lines if kind='both'</span></span>
<span id="cb67-712"><a href="#cb67-712"></a>            ice_lines_kw <span class="op">=</span> ice_lines_kw, <span class="co"># Style for ICE lines</span></span>
<span id="cb67-713"><a href="#cb67-713"></a>            pd_line_kw <span class="op">=</span> pd_line_kw, <span class="co"># Style for PDP line</span></span>
<span id="cb67-714"><a href="#cb67-714"></a>            percentiles<span class="op">=</span>(<span class="fl">0.0001</span>, <span class="fl">0.9999</span>),</span>
<span id="cb67-715"><a href="#cb67-715"></a>            grid_resolution<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb67-716"><a href="#cb67-716"></a>            ax <span class="op">=</span> ax,</span>
<span id="cb67-717"><a href="#cb67-717"></a>            random_state<span class="op">=</span><span class="dv">2025</span>,</span>
<span id="cb67-718"><a href="#cb67-718"></a>            n_jobs <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb67-719"><a href="#cb67-719"></a>        )</span>
<span id="cb67-720"><a href="#cb67-720"></a></span>
<span id="cb67-721"><a href="#cb67-721"></a>        plot_title <span class="op">=</span> <span class="ss">f"Partial Dependence for </span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb67-722"><a href="#cb67-722"></a>        <span class="cf">if</span> show_ice:</span>
<span id="cb67-723"><a href="#cb67-723"></a>            plot_title <span class="op">+=</span> <span class="st">" (with ICE)"</span></span>
<span id="cb67-724"><a href="#cb67-724"></a>        ax.set_title(plot_title)</span>
<span id="cb67-725"><a href="#cb67-725"></a></span>
<span id="cb67-726"><a href="#cb67-726"></a>        <span class="co"># Set y-axis lower limit for all axes in the current figure</span></span>
<span id="cb67-727"><a href="#cb67-727"></a>        <span class="cf">for</span> a <span class="kw">in</span> fig.get_axes():</span>
<span id="cb67-728"><a href="#cb67-728"></a>            coordy <span class="op">=</span> a.get_ylim()</span>
<span id="cb67-729"><a href="#cb67-729"></a>            <span class="co"># Adjust y-axis limits, potentially making space for ICE lines</span></span>
<span id="cb67-730"><a href="#cb67-730"></a>            y_bottom <span class="op">=</span> <span class="dv">0</span> <span class="cf">if</span> <span class="kw">not</span> show_ice <span class="cf">else</span> <span class="bu">min</span>(<span class="dv">0</span>, coordy[<span class="dv">0</span>]) <span class="co"># Allow negative if ICE shown</span></span>
<span id="cb67-731"><a href="#cb67-731"></a>            a.set_ylim(bottom<span class="op">=</span>y_bottom, top<span class="op">=</span>coordy[<span class="dv">1</span>]<span class="op">*</span><span class="fl">1.1</span>)</span>
<span id="cb67-732"><a href="#cb67-732"></a></span>
<span id="cb67-733"><a href="#cb67-733"></a>        <span class="cf">if</span> xGTzero:</span>
<span id="cb67-734"><a href="#cb67-734"></a>            <span class="co"># Set x-axis from 0 to the max</span></span>
<span id="cb67-735"><a href="#cb67-735"></a>            <span class="cf">for</span> a <span class="kw">in</span> fig.get_axes():</span>
<span id="cb67-736"><a href="#cb67-736"></a>                <span class="co"># Calculate percentile on the original feature column if possible</span></span>
<span id="cb67-737"><a href="#cb67-737"></a>                <span class="cf">if</span> feature <span class="kw">in</span> df.columns:</span>
<span id="cb67-738"><a href="#cb67-738"></a>                    max_val <span class="op">=</span> np.percentile(df[feature].dropna().values, <span class="fl">99.99</span>)</span>
<span id="cb67-739"><a href="#cb67-739"></a>                    a.set_xlim(left<span class="op">=</span><span class="dv">0</span>, right<span class="op">=</span>max_val)</span>
<span id="cb67-740"><a href="#cb67-740"></a>                <span class="cf">else</span>: <span class="co"># Fallback if feature not directly in df (e.g., transformed)</span></span>
<span id="cb67-741"><a href="#cb67-741"></a>                        max_val <span class="op">=</span> a.get_xlim()[<span class="dv">1</span>] <span class="co"># Use current max</span></span>
<span id="cb67-742"><a href="#cb67-742"></a>                        a.set_xlim(left<span class="op">=</span><span class="dv">0</span>, right<span class="op">=</span>max_val)</span>
<span id="cb67-743"><a href="#cb67-743"></a></span>
<span id="cb67-744"><a href="#cb67-744"></a></span>
<span id="cb67-745"><a href="#cb67-745"></a>        plt.show()</span>
<span id="cb67-746"><a href="#cb67-746"></a>        plt.close(<span class="st">'all'</span>)  <span class="co"># Prevent figure overload</span></span>
<span id="cb67-747"><a href="#cb67-747"></a></span>
<span id="cb67-748"><a href="#cb67-748"></a></span>
<span id="cb67-749"><a href="#cb67-749"></a><span class="kw">def</span> generate_eda_report(df: pd.DataFrame, title: <span class="bu">str</span>, output_path: <span class="bu">str</span>, sample_frac: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.1</span>, random_state: <span class="bu">int</span> <span class="op">=</span> <span class="dv">2025</span>):</span>
<span id="cb67-750"><a href="#cb67-750"></a>    <span class="co">"""Generates and saves a ydata-profiling report for a DataFrame."""</span></span>
<span id="cb67-751"><a href="#cb67-751"></a>    <span class="bu">print</span>(<span class="ss">f"Generating EDA report: </span><span class="sc">{</span>title<span class="sc">}</span><span class="ss">..."</span>)</span>
<span id="cb67-752"><a href="#cb67-752"></a>    <span class="cf">try</span>:</span>
<span id="cb67-753"><a href="#cb67-753"></a>        profile <span class="op">=</span> ProfileReport(</span>
<span id="cb67-754"><a href="#cb67-754"></a>            df.sample(frac<span class="op">=</span>sample_frac, random_state<span class="op">=</span>random_state) <span class="cf">if</span> sample_frac <span class="op">&lt;</span> <span class="fl">1.0</span> <span class="cf">else</span> df,</span>
<span id="cb67-755"><a href="#cb67-755"></a>            title<span class="op">=</span>title,</span>
<span id="cb67-756"><a href="#cb67-756"></a>            progress_bar<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb67-757"><a href="#cb67-757"></a>            duplicates<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb67-758"><a href="#cb67-758"></a>            interactions<span class="op">=</span><span class="va">None</span></span>
<span id="cb67-759"><a href="#cb67-759"></a>        )</span>
<span id="cb67-760"><a href="#cb67-760"></a>        profile.to_file(output_path)</span>
<span id="cb67-761"><a href="#cb67-761"></a>        <span class="bu">print</span>(<span class="ss">f"Report saved to: </span><span class="sc">{</span>output_path<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-762"><a href="#cb67-762"></a>    <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb67-763"><a href="#cb67-763"></a>        <span class="bu">print</span>(<span class="ss">f"Error generating report '</span><span class="sc">{</span>title<span class="sc">}</span><span class="ss">': </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-764"><a href="#cb67-764"></a></span>
<span id="cb67-765"><a href="#cb67-765"></a><span class="kw">def</span> split_data(df: pd.DataFrame, target_col: <span class="bu">str</span> <span class="op">|</span> <span class="va">None</span> <span class="op">=</span> <span class="va">None</span>, train_size: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.6</span>, calib_size_rel: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.5</span>, random_state: <span class="bu">int</span> <span class="op">=</span> <span class="dv">2025</span>) <span class="op">-&gt;</span> <span class="bu">tuple</span>[pd.DataFrame, pd.DataFrame, pd.DataFrame]:</span>
<span id="cb67-766"><a href="#cb67-766"></a>    <span class="co">"""Splits data into train, calibration, and test sets."""</span></span>
<span id="cb67-767"><a href="#cb67-767"></a>    <span class="bu">print</span>(<span class="ss">f"Splitting data (stratify=</span><span class="sc">{</span><span class="st">'Yes'</span> <span class="cf">if</span> target_col <span class="cf">else</span> <span class="st">'No'</span><span class="sc">}</span><span class="ss">)..."</span>)</span>
<span id="cb67-768"><a href="#cb67-768"></a>    y <span class="op">=</span> df[target_col] <span class="cf">if</span> target_col <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb67-769"><a href="#cb67-769"></a></span>
<span id="cb67-770"><a href="#cb67-770"></a>    <span class="cf">if</span> target_col:</span>
<span id="cb67-771"><a href="#cb67-771"></a>        stratify_param <span class="op">=</span> y</span>
<span id="cb67-772"><a href="#cb67-772"></a>        <span class="co"># First split: Train and Temp (with stratification)</span></span>
<span id="cb67-773"><a href="#cb67-773"></a>        df_train, df_temp, y_train, y_temp <span class="op">=</span> train_test_split(</span>
<span id="cb67-774"><a href="#cb67-774"></a>            df, y, train_size<span class="op">=</span>train_size, random_state<span class="op">=</span>random_state, stratify<span class="op">=</span>stratify_param</span>
<span id="cb67-775"><a href="#cb67-775"></a>        )</span>
<span id="cb67-776"><a href="#cb67-776"></a>        <span class="co"># Second split: Temp into Calibration and Test (with stratification)</span></span>
<span id="cb67-777"><a href="#cb67-777"></a>        stratify_param_temp <span class="op">=</span> y_temp</span>
<span id="cb67-778"><a href="#cb67-778"></a>        df_calib, df_test, y_calib, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb67-779"><a href="#cb67-779"></a>            df_temp, y_temp, test_size<span class="op">=</span>calib_size_rel, random_state<span class="op">=</span>random_state, stratify<span class="op">=</span>stratify_param_temp</span>
<span id="cb67-780"><a href="#cb67-780"></a>        )</span>
<span id="cb67-781"><a href="#cb67-781"></a>    <span class="cf">else</span>:</span>
<span id="cb67-782"><a href="#cb67-782"></a>        <span class="co"># First split: Train and Temp (without stratification)</span></span>
<span id="cb67-783"><a href="#cb67-783"></a>        df_train, df_temp <span class="op">=</span> train_test_split(</span>
<span id="cb67-784"><a href="#cb67-784"></a>            df, train_size<span class="op">=</span>train_size, random_state<span class="op">=</span>random_state, stratify<span class="op">=</span><span class="va">None</span></span>
<span id="cb67-785"><a href="#cb67-785"></a>        )</span>
<span id="cb67-786"><a href="#cb67-786"></a>        <span class="co"># Second split: Temp into Calibration and Test (without stratification)</span></span>
<span id="cb67-787"><a href="#cb67-787"></a>        df_calib, df_test <span class="op">=</span> train_test_split(</span>
<span id="cb67-788"><a href="#cb67-788"></a>            df_temp, test_size<span class="op">=</span>calib_size_rel, random_state<span class="op">=</span>random_state, stratify<span class="op">=</span><span class="va">None</span></span>
<span id="cb67-789"><a href="#cb67-789"></a>        )</span>
<span id="cb67-790"><a href="#cb67-790"></a>        <span class="co"># Assign None to y splits as they don't exist</span></span>
<span id="cb67-791"><a href="#cb67-791"></a>        y_train, y_temp, y_calib, y_test <span class="op">=</span> <span class="va">None</span>, <span class="va">None</span>, <span class="va">None</span>, <span class="va">None</span></span>
<span id="cb67-792"><a href="#cb67-792"></a></span>
<span id="cb67-793"><a href="#cb67-793"></a></span>
<span id="cb67-794"><a href="#cb67-794"></a>    <span class="bu">print</span>(<span class="ss">f"  Train shape: </span><span class="sc">{</span>df_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-795"><a href="#cb67-795"></a>    <span class="bu">print</span>(<span class="ss">f"  Calibration shape: </span><span class="sc">{</span>df_calib<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-796"><a href="#cb67-796"></a>    <span class="bu">print</span>(<span class="ss">f"  Test shape: </span><span class="sc">{</span>df_test<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-797"><a href="#cb67-797"></a>    <span class="cf">if</span> target_col:</span>
<span id="cb67-798"><a href="#cb67-798"></a>        <span class="bu">print</span>(<span class="ss">f"  Train target mean: </span><span class="sc">{</span>y_train<span class="sc">.</span>mean()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb67-799"><a href="#cb67-799"></a>        <span class="bu">print</span>(<span class="ss">f"  Calibration target mean: </span><span class="sc">{</span>y_calib<span class="sc">.</span>mean()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb67-800"><a href="#cb67-800"></a>        <span class="bu">print</span>(<span class="ss">f"  Test target mean: </span><span class="sc">{</span>y_test<span class="sc">.</span>mean()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb67-801"><a href="#cb67-801"></a></span>
<span id="cb67-802"><a href="#cb67-802"></a>    <span class="co"># Return copies</span></span>
<span id="cb67-803"><a href="#cb67-803"></a>    <span class="cf">return</span> df_train.copy(), df_calib.copy(), df_test.copy()</span>
<span id="cb67-804"><a href="#cb67-804"></a></span>
<span id="cb67-805"><a href="#cb67-805"></a></span>
<span id="cb67-806"><a href="#cb67-806"></a><span class="kw">def</span> train_autogluon_model(</span>
<span id="cb67-807"><a href="#cb67-807"></a>    df_train: pd.DataFrame,</span>
<span id="cb67-808"><a href="#cb67-808"></a>    label: <span class="bu">str</span>,</span>
<span id="cb67-809"><a href="#cb67-809"></a>    weight_col: <span class="bu">str</span> <span class="op">|</span> <span class="va">None</span>,</span>
<span id="cb67-810"><a href="#cb67-810"></a>    modeling_features: <span class="bu">list</span>[<span class="bu">str</span>],</span>
<span id="cb67-811"><a href="#cb67-811"></a>    model_folder: <span class="bu">str</span>,</span>
<span id="cb67-812"><a href="#cb67-812"></a>    predictor_args: <span class="bu">dict</span>,</span>
<span id="cb67-813"><a href="#cb67-813"></a>    fit_args: <span class="bu">dict</span></span>
<span id="cb67-814"><a href="#cb67-814"></a>) <span class="op">-&gt;</span> AutoGluonSklearnWrapper:</span>
<span id="cb67-815"><a href="#cb67-815"></a>    <span class="co">"""Trains an AutoGluon model using the Sklearn wrapper."""</span></span>
<span id="cb67-816"><a href="#cb67-816"></a>    <span class="bu">print</span>(<span class="ss">f"--- Training AutoGluon Model in: </span><span class="sc">{</span>model_folder<span class="sc">}</span><span class="ss"> ---"</span>)</span>
<span id="cb67-817"><a href="#cb67-817"></a>    remove_ag_folder(model_folder) <span class="co"># Clean up previous runs</span></span>
<span id="cb67-818"><a href="#cb67-818"></a></span>
<span id="cb67-819"><a href="#cb67-819"></a>    <span class="bu">print</span>(<span class="ss">f"Using features: </span><span class="sc">{</span>modeling_features<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-820"><a href="#cb67-820"></a>    <span class="bu">print</span>(<span class="ss">f"Training data shape: </span><span class="sc">{</span>df_train<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-821"><a href="#cb67-821"></a>    <span class="cf">if</span> weight_col:</span>
<span id="cb67-822"><a href="#cb67-822"></a>        <span class="bu">print</span>(<span class="ss">f"Sum of weights in training data: </span><span class="sc">{</span>df_train[weight_col]<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb67-823"><a href="#cb67-823"></a></span>
<span id="cb67-824"><a href="#cb67-824"></a>    start_time <span class="op">=</span> time.time()</span>
<span id="cb67-825"><a href="#cb67-825"></a></span>
<span id="cb67-826"><a href="#cb67-826"></a>    <span class="co"># Prepare X, y, and weights for the wrapper's fit method</span></span>
<span id="cb67-827"><a href="#cb67-827"></a>    X_train_ag <span class="op">=</span> df_train[modeling_features]</span>
<span id="cb67-828"><a href="#cb67-828"></a>    y_train_ag <span class="op">=</span> df_train[label]</span>
<span id="cb67-829"><a href="#cb67-829"></a>    weights_train_ag <span class="op">=</span> df_train[weight_col] <span class="cf">if</span> weight_col <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb67-830"><a href="#cb67-830"></a></span>
<span id="cb67-831"><a href="#cb67-831"></a>    <span class="co"># Update predictor_args with sample_weight if provided</span></span>
<span id="cb67-832"><a href="#cb67-832"></a>    <span class="cf">if</span> weight_col:</span>
<span id="cb67-833"><a href="#cb67-833"></a>        predictor_args[<span class="st">'sample_weight'</span>] <span class="op">=</span> weight_col</span>
<span id="cb67-834"><a href="#cb67-834"></a></span>
<span id="cb67-835"><a href="#cb67-835"></a>    ag_model_wrapped <span class="op">=</span> AutoGluonSklearnWrapper(</span>
<span id="cb67-836"><a href="#cb67-836"></a>        label<span class="op">=</span>label,</span>
<span id="cb67-837"><a href="#cb67-837"></a>        predictor_args<span class="op">=</span>predictor_args,</span>
<span id="cb67-838"><a href="#cb67-838"></a>        fit_args<span class="op">=</span>fit_args</span>
<span id="cb67-839"><a href="#cb67-839"></a>    )</span>
<span id="cb67-840"><a href="#cb67-840"></a></span>
<span id="cb67-841"><a href="#cb67-841"></a>    <span class="co"># Fit the model</span></span>
<span id="cb67-842"><a href="#cb67-842"></a>    ag_model_wrapped.fit(X_train_ag, y_train_ag, sample_weight<span class="op">=</span>weights_train_ag)</span>
<span id="cb67-843"><a href="#cb67-843"></a></span>
<span id="cb67-844"><a href="#cb67-844"></a>    end_time <span class="op">=</span> time.time()</span>
<span id="cb67-845"><a href="#cb67-845"></a>    <span class="bu">print</span>(<span class="ss">f"AutoGluon training completed in </span><span class="sc">{</span>end_time <span class="op">-</span> start_time<span class="sc">:.2f}</span><span class="ss"> seconds."</span>)</span>
<span id="cb67-846"><a href="#cb67-846"></a></span>
<span id="cb67-847"><a href="#cb67-847"></a>    <span class="co"># Display leaderboard</span></span>
<span id="cb67-848"><a href="#cb67-848"></a>    ag_predictor <span class="op">=</span> ag_model_wrapped.predictor</span>
<span id="cb67-849"><a href="#cb67-849"></a>    <span class="cf">if</span> ag_predictor:</span>
<span id="cb67-850"><a href="#cb67-850"></a>        <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">AutoGluon Leaderboard:"</span>)</span>
<span id="cb67-851"><a href="#cb67-851"></a>        leaderboard <span class="op">=</span> ag_predictor.leaderboard(silent<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb67-852"><a href="#cb67-852"></a>        display(leaderboard)</span>
<span id="cb67-853"><a href="#cb67-853"></a>    <span class="cf">else</span>:</span>
<span id="cb67-854"><a href="#cb67-854"></a>        <span class="bu">print</span>(<span class="st">"Could not access underlying predictor to display leaderboard."</span>)</span>
<span id="cb67-855"><a href="#cb67-855"></a></span>
<span id="cb67-856"><a href="#cb67-856"></a>    <span class="cf">return</span> ag_model_wrapped</span>
<span id="cb67-857"><a href="#cb67-857"></a></span>
<span id="cb67-858"><a href="#cb67-858"></a><span class="kw">def</span> summarize_ttd_by_source(df_ttd: pd.DataFrame, target_col: <span class="bu">str</span> <span class="op">=</span> <span class="st">'default_flag'</span>, weight_col: <span class="bu">str</span> <span class="op">=</span> <span class="st">'sample_weight'</span>, source_col: <span class="bu">str</span> <span class="op">=</span> <span class="st">'source'</span>) <span class="op">-&gt;</span> pd.DataFrame:</span>
<span id="cb67-859"><a href="#cb67-859"></a>    <span class="co">"""Calculates summary statistics (counts, weights, rates) for a TTD DataFrame grouped by source."""</span></span>
<span id="cb67-860"><a href="#cb67-860"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">--- Summarizing TTD Data by '</span><span class="sc">{</span>source_col<span class="sc">}</span><span class="ss">' ---"</span>)</span>
<span id="cb67-861"><a href="#cb67-861"></a>    <span class="co"># Step 1: Calculate standard aggregations</span></span>
<span id="cb67-862"><a href="#cb67-862"></a>    summary_stats <span class="op">=</span> df_ttd.groupby(source_col).agg(</span>
<span id="cb67-863"><a href="#cb67-863"></a>        row_count<span class="op">=</span>(target_col, <span class="st">'size'</span>),</span>
<span id="cb67-864"><a href="#cb67-864"></a>        sum_weights<span class="op">=</span>(weight_col, <span class="st">'sum'</span>),</span>
<span id="cb67-865"><a href="#cb67-865"></a>        unweighted_default_rate<span class="op">=</span>(target_col, <span class="st">'mean'</span>)</span>
<span id="cb67-866"><a href="#cb67-866"></a>    ).reset_index()</span>
<span id="cb67-867"><a href="#cb67-867"></a></span>
<span id="cb67-868"><a href="#cb67-868"></a>    <span class="co"># Step 2: Calculate weighted default rate separately using apply</span></span>
<span id="cb67-869"><a href="#cb67-869"></a>    weighted_rates <span class="op">=</span> df_ttd.groupby(source_col).<span class="bu">apply</span>(</span>
<span id="cb67-870"><a href="#cb67-870"></a>        <span class="kw">lambda</span> x: np.average(x[target_col], weights<span class="op">=</span>x[weight_col]) <span class="cf">if</span> x[weight_col].<span class="bu">sum</span>() <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> np.nan</span>
<span id="cb67-871"><a href="#cb67-871"></a>    ).reset_index(name<span class="op">=</span><span class="st">'weighted_default_rate'</span>)</span>
<span id="cb67-872"><a href="#cb67-872"></a></span>
<span id="cb67-873"><a href="#cb67-873"></a>    <span class="co"># Step 3: Merge the results</span></span>
<span id="cb67-874"><a href="#cb67-874"></a>    summary_df <span class="op">=</span> pd.merge(summary_stats, weighted_rates, on<span class="op">=</span>source_col)</span>
<span id="cb67-875"><a href="#cb67-875"></a></span>
<span id="cb67-876"><a href="#cb67-876"></a>    <span class="co"># Display the summary</span></span>
<span id="cb67-877"><a href="#cb67-877"></a>    <span class="bu">print</span>(<span class="st">"Summary of Default Rates by Source:"</span>)</span>
<span id="cb67-878"><a href="#cb67-878"></a>    display(summary_df[[source_col, <span class="st">'row_count'</span>, <span class="st">'unweighted_default_rate'</span>, <span class="st">'sum_weights'</span>, <span class="st">'weighted_default_rate'</span>]])</span>
<span id="cb67-879"><a href="#cb67-879"></a>    <span class="cf">return</span> summary_df</span>
<span id="cb67-880"><a href="#cb67-880"></a></span>
<span id="cb67-881"><a href="#cb67-881"></a><span class="kw">def</span> create_TTD_data(</span>
<span id="cb67-882"><a href="#cb67-882"></a>    ri_model: BaseEstimator <span class="op">|</span> <span class="va">None</span>,  <span class="co"># Modified to accept None</span></span>
<span id="cb67-883"><a href="#cb67-883"></a>    df_rejected: pd.DataFrame,</span>
<span id="cb67-884"><a href="#cb67-884"></a>    df_accepted: pd.DataFrame,</span>
<span id="cb67-885"><a href="#cb67-885"></a>    ri_features: <span class="bu">list</span>[<span class="bu">str</span>],</span>
<span id="cb67-886"><a href="#cb67-886"></a>    modeling_features: <span class="bu">list</span>[<span class="bu">str</span>],</span>
<span id="cb67-887"><a href="#cb67-887"></a>    target_col: <span class="bu">str</span> <span class="op">=</span> <span class="st">'default_flag'</span>,</span>
<span id="cb67-888"><a href="#cb67-888"></a>    clone_rejected: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span></span>
<span id="cb67-889"><a href="#cb67-889"></a>) <span class="op">-&gt;</span> pd.DataFrame:</span>
<span id="cb67-890"><a href="#cb67-890"></a>    <span class="co">"""</span></span>
<span id="cb67-891"><a href="#cb67-891"></a><span class="co">    Creates a Through-the-Door (TTD) dataset using Fuzzy Augmentation.</span></span>
<span id="cb67-892"><a href="#cb67-892"></a></span>
<span id="cb67-893"><a href="#cb67-893"></a><span class="co">    Applies a trained reject inference (RI) model to rejected applicants to estimate</span></span>
<span id="cb67-894"><a href="#cb67-894"></a><span class="co">    their probability of default, then creates weighted copies of the rejected data</span></span>
<span id="cb67-895"><a href="#cb67-895"></a><span class="co">    and combines them with the accepted data. Adds a 'source' column to indicate</span></span>
<span id="cb67-896"><a href="#cb67-896"></a><span class="co">    origin ('Accepted' or 'Rejected').</span></span>
<span id="cb67-897"><a href="#cb67-897"></a></span>
<span id="cb67-898"><a href="#cb67-898"></a><span class="co">    If ri_model is None, skips fuzzy augmentation and uses weight=1 for all records.</span></span>
<span id="cb67-899"><a href="#cb67-899"></a></span>
<span id="cb67-900"><a href="#cb67-900"></a><span class="co">    Args:</span></span>
<span id="cb67-901"><a href="#cb67-901"></a><span class="co">        ri_model: A trained scikit-learn compatible model used for reject inference.</span></span>
<span id="cb67-902"><a href="#cb67-902"></a><span class="co">                    Must have a `predict_proba` method. If None, fuzzy augmentation</span></span>
<span id="cb67-903"><a href="#cb67-903"></a><span class="co">                    is skipped and all weights are set to 1. Setting ri_model to None is intended for production-use, where a decision has yet to be made on an applicant.</span></span>
<span id="cb67-904"><a href="#cb67-904"></a><span class="co">        df_rejected: DataFrame containing rejected applicant data with common features.</span></span>
<span id="cb67-905"><a href="#cb67-905"></a><span class="co">        df_accepted: DataFrame containing accepted applicant data with common features</span></span>
<span id="cb67-906"><a href="#cb67-906"></a><span class="co">                        and the target variable.</span></span>
<span id="cb67-907"><a href="#cb67-907"></a><span class="co">        ri_features: List of feature names used by the ri_model.</span></span>
<span id="cb67-908"><a href="#cb67-908"></a><span class="co">        modeling_features: List of feature names to include in the final TTD dataset</span></span>
<span id="cb67-909"><a href="#cb67-909"></a><span class="co">                            (should be present in both df_rejected and df_accepted).</span></span>
<span id="cb67-910"><a href="#cb67-910"></a><span class="co">        target_col: Name of the target variable column in df_accepted.</span></span>
<span id="cb67-911"><a href="#cb67-911"></a><span class="co">        clone_rejected: If True and using fuzzy augmentation, creates two copies of</span></span>
<span id="cb67-912"><a href="#cb67-912"></a><span class="co">                        rejected data (one for each class). If False or not using</span></span>
<span id="cb67-913"><a href="#cb67-913"></a><span class="co">                        fuzzy augmentation, only includes one copy of rejected data.</span></span>
<span id="cb67-914"><a href="#cb67-914"></a></span>
<span id="cb67-915"><a href="#cb67-915"></a><span class="co">    Returns:</span></span>
<span id="cb67-916"><a href="#cb67-916"></a><span class="co">        A pandas DataFrame representing the augmented TTD dataset with features,</span></span>
<span id="cb67-917"><a href="#cb67-917"></a><span class="co">        the target column, a 'sample_weight' column, and a 'source' column.</span></span>
<span id="cb67-918"><a href="#cb67-918"></a><span class="co">    """</span></span>
<span id="cb67-919"><a href="#cb67-919"></a>    <span class="co"># Ensure dataframes are copies to avoid modifying originals</span></span>
<span id="cb67-920"><a href="#cb67-920"></a>    df_rejected_proc <span class="op">=</span> df_rejected.copy()</span>
<span id="cb67-921"><a href="#cb67-921"></a>    df_accepted_proc <span class="op">=</span> df_accepted.copy()</span>
<span id="cb67-922"><a href="#cb67-922"></a></span>
<span id="cb67-923"><a href="#cb67-923"></a>    <span class="co"># Check if ri_model is None - skip fuzzy augmentation if so</span></span>
<span id="cb67-924"><a href="#cb67-924"></a>    <span class="cf">if</span> ri_model <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb67-925"><a href="#cb67-925"></a>        <span class="bu">print</span>(<span class="st">"No RI model provided. Creating TTD data with uniform weights (sample_weight=1)..."</span>)</span>
<span id="cb67-926"><a href="#cb67-926"></a>        </span>
<span id="cb67-927"><a href="#cb67-927"></a>        <span class="co"># Prepare Accepted Data - select modeling features + target, assign weight = 1</span></span>
<span id="cb67-928"><a href="#cb67-928"></a>        df_accepted_weighted <span class="op">=</span> df_accepted_proc[modeling_features <span class="op">+</span> [target_col]].copy()</span>
<span id="cb67-929"><a href="#cb67-929"></a>        df_accepted_weighted[<span class="st">'sample_weight'</span>] <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb67-930"><a href="#cb67-930"></a>        df_accepted_weighted[<span class="st">'source'</span>] <span class="op">=</span> <span class="st">'Accepted'</span></span>
<span id="cb67-931"><a href="#cb67-931"></a>        </span>
<span id="cb67-932"><a href="#cb67-932"></a>        <span class="co"># Prepare Rejected Data - all assigned target_col = 1 (assume bad) with weight = 1</span></span>
<span id="cb67-933"><a href="#cb67-933"></a>        df_rejected_weighted <span class="op">=</span> df_rejected_proc[modeling_features].copy()</span>
<span id="cb67-934"><a href="#cb67-934"></a>        df_rejected_weighted[<span class="st">'sample_weight'</span>] <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb67-935"><a href="#cb67-935"></a>        df_rejected_weighted[target_col] <span class="op">=</span> <span class="va">None</span>  <span class="co"># Target col does not exist for rejects</span></span>
<span id="cb67-936"><a href="#cb67-936"></a>        df_rejected_weighted[<span class="st">'source'</span>] <span class="op">=</span> <span class="st">'Rejected'</span></span>
<span id="cb67-937"><a href="#cb67-937"></a>        </span>
<span id="cb67-938"><a href="#cb67-938"></a>        <span class="co"># Define columns for the final TTD dataset</span></span>
<span id="cb67-939"><a href="#cb67-939"></a>        cols_for_ttd <span class="op">=</span> modeling_features <span class="op">+</span> [target_col, <span class="st">'sample_weight'</span>, <span class="st">'source'</span>]</span>
<span id="cb67-940"><a href="#cb67-940"></a>        </span>
<span id="cb67-941"><a href="#cb67-941"></a>        <span class="co"># Concatenate accepted and rejected data</span></span>
<span id="cb67-942"><a href="#cb67-942"></a>        df_ttd <span class="op">=</span> pd.concat(</span>
<span id="cb67-943"><a href="#cb67-943"></a>            [df_accepted_weighted[cols_for_ttd], df_rejected_weighted[cols_for_ttd]],</span>
<span id="cb67-944"><a href="#cb67-944"></a>            ignore_index<span class="op">=</span><span class="va">True</span></span>
<span id="cb67-945"><a href="#cb67-945"></a>        )</span>
<span id="cb67-946"><a href="#cb67-946"></a>    <span class="cf">else</span>:</span>
<span id="cb67-947"><a href="#cb67-947"></a>        <span class="bu">print</span>(<span class="st">"Applying RI model to rejected data and calculating weights..."</span>)</span>
<span id="cb67-948"><a href="#cb67-948"></a>        </span>
<span id="cb67-949"><a href="#cb67-949"></a>        <span class="co"># 1. Prepare Rejected Data for RI Prediction</span></span>
<span id="cb67-950"><a href="#cb67-950"></a>        X_rej_ri <span class="op">=</span> df_rejected_proc[ri_features]</span>
<span id="cb67-951"><a href="#cb67-951"></a>        </span>
<span id="cb67-952"><a href="#cb67-952"></a>        <span class="co"># 2. Predict Probabilities for Rejected Applicants</span></span>
<span id="cb67-953"><a href="#cb67-953"></a>        prob_default_rejected <span class="op">=</span> ri_model.predict_proba(X_rej_ri)[:, <span class="dv">1</span>]</span>
<span id="cb67-954"><a href="#cb67-954"></a>        prob_good_rejected <span class="op">=</span> <span class="fl">1.0</span> <span class="op">-</span> prob_default_rejected  <span class="co"># P(default=0)</span></span>
<span id="cb67-955"><a href="#cb67-955"></a>        </span>
<span id="cb67-956"><a href="#cb67-956"></a>        <span class="co"># --- Fuzzy Augmentation Implementation ---</span></span>
<span id="cb67-957"><a href="#cb67-957"></a>        <span class="co"># 3. Create two weighted copies of rejected data</span></span>
<span id="cb67-958"><a href="#cb67-958"></a>        </span>
<span id="cb67-959"><a href="#cb67-959"></a>        <span class="co"># Copy 1: Assumed Bad (target_col = 1)</span></span>
<span id="cb67-960"><a href="#cb67-960"></a>        df_rejected_bad <span class="op">=</span> df_rejected_proc[modeling_features].copy()</span>
<span id="cb67-961"><a href="#cb67-961"></a>        df_rejected_bad[<span class="st">'sample_weight'</span>] <span class="op">=</span> prob_default_rejected  <span class="co"># Weight = P(default=1)</span></span>
<span id="cb67-962"><a href="#cb67-962"></a>        df_rejected_bad[target_col] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb67-963"><a href="#cb67-963"></a>        df_rejected_bad[<span class="st">'source'</span>] <span class="op">=</span> <span class="st">'Rejected'</span></span>
<span id="cb67-964"><a href="#cb67-964"></a>        </span>
<span id="cb67-965"><a href="#cb67-965"></a>        <span class="co"># Copy 2: Assumed Good (target_col = 0)</span></span>
<span id="cb67-966"><a href="#cb67-966"></a>        df_rejected_good <span class="op">=</span> df_rejected_proc[modeling_features].copy()</span>
<span id="cb67-967"><a href="#cb67-967"></a>        df_rejected_good[<span class="st">'sample_weight'</span>] <span class="op">=</span> prob_good_rejected  <span class="co"># Weight = P(default=0)</span></span>
<span id="cb67-968"><a href="#cb67-968"></a>        df_rejected_good[target_col] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb67-969"><a href="#cb67-969"></a>        df_rejected_good[<span class="st">'source'</span>] <span class="op">=</span> <span class="st">'Rejected'</span></span>
<span id="cb67-970"><a href="#cb67-970"></a>        </span>
<span id="cb67-971"><a href="#cb67-971"></a>        <span class="co"># 4. Prepare Accepted Data</span></span>
<span id="cb67-972"><a href="#cb67-972"></a>        <span class="co"># Select modeling features + target, assign weight = 1</span></span>
<span id="cb67-973"><a href="#cb67-973"></a>        df_accepted_weighted <span class="op">=</span> df_accepted_proc[modeling_features <span class="op">+</span> [target_col]].copy()</span>
<span id="cb67-974"><a href="#cb67-974"></a>        df_accepted_weighted[<span class="st">'sample_weight'</span>] <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb67-975"><a href="#cb67-975"></a>        df_accepted_weighted[<span class="st">'source'</span>] <span class="op">=</span> <span class="st">'Accepted'</span></span>
<span id="cb67-976"><a href="#cb67-976"></a>        </span>
<span id="cb67-977"><a href="#cb67-977"></a>        <span class="co"># 5. Define columns needed for the final TTD dataset</span></span>
<span id="cb67-978"><a href="#cb67-978"></a>        cols_for_ttd <span class="op">=</span> modeling_features <span class="op">+</span> [target_col, <span class="st">'sample_weight'</span>, <span class="st">'source'</span>]</span>
<span id="cb67-979"><a href="#cb67-979"></a>        </span>
<span id="cb67-980"><a href="#cb67-980"></a>        <span class="co"># 6. Concatenate accepted data and the weighted rejected datasets</span></span>
<span id="cb67-981"><a href="#cb67-981"></a>        <span class="cf">if</span> clone_rejected:</span>
<span id="cb67-982"><a href="#cb67-982"></a>            df_ttd <span class="op">=</span> pd.concat(</span>
<span id="cb67-983"><a href="#cb67-983"></a>                [df_accepted_weighted[cols_for_ttd],</span>
<span id="cb67-984"><a href="#cb67-984"></a>                    df_rejected_bad[cols_for_ttd],</span>
<span id="cb67-985"><a href="#cb67-985"></a>                    df_rejected_good[cols_for_ttd]],</span>
<span id="cb67-986"><a href="#cb67-986"></a>                ignore_index<span class="op">=</span><span class="va">True</span></span>
<span id="cb67-987"><a href="#cb67-987"></a>            )</span>
<span id="cb67-988"><a href="#cb67-988"></a>        <span class="cf">else</span>:</span>
<span id="cb67-989"><a href="#cb67-989"></a>            <span class="co"># If not cloning, just use the bad copy</span></span>
<span id="cb67-990"><a href="#cb67-990"></a>            df_ttd <span class="op">=</span> pd.concat(</span>
<span id="cb67-991"><a href="#cb67-991"></a>                [df_accepted_weighted[cols_for_ttd],</span>
<span id="cb67-992"><a href="#cb67-992"></a>                    df_rejected_bad[cols_for_ttd]],</span>
<span id="cb67-993"><a href="#cb67-993"></a>                ignore_index<span class="op">=</span><span class="va">True</span></span>
<span id="cb67-994"><a href="#cb67-994"></a>            )</span>
<span id="cb67-995"><a href="#cb67-995"></a></span>
<span id="cb67-996"><a href="#cb67-996"></a>    <span class="bu">print</span>(<span class="ss">f"TTD dataset created. Shape: </span><span class="sc">{</span>df_ttd<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-997"><a href="#cb67-997"></a>    <span class="bu">print</span>(<span class="ss">f"Sum of weights in TTD dataset: </span><span class="sc">{</span>df_ttd[<span class="st">'sample_weight'</span>]<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb67-998"><a href="#cb67-998"></a>    <span class="bu">print</span>(<span class="ss">f"Source distribution:</span><span class="ch">\n</span><span class="sc">{</span>df_ttd[<span class="st">'source'</span>]<span class="sc">.</span>value_counts(normalize<span class="op">=</span><span class="va">True</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-999"><a href="#cb67-999"></a></span>
<span id="cb67-1000"><a href="#cb67-1000"></a>    <span class="cf">return</span> df_ttd</span>
<span id="cb67-1001"><a href="#cb67-1001"></a></span>
<span id="cb67-1002"><a href="#cb67-1002"></a><span class="bu">print</span>(<span class="st">"Helper functions and classes defined."</span>)</span>
<span id="cb67-1003"><a href="#cb67-1003"></a><span class="in">```</span></span>
<span id="cb67-1004"><a href="#cb67-1004"></a></span>
<span id="cb67-1005"><a href="#cb67-1005"></a><span class="fu"># Data Loading &amp; EDA</span></span>
<span id="cb67-1006"><a href="#cb67-1006"></a></span>
<span id="cb67-1007"><a href="#cb67-1007"></a>**Goal:** Load the LendingClub accepted and rejected datasets, define the target variable (<span class="in">`default_flag`</span>), split the data, and perform basic EDA.</span>
<span id="cb67-1008"><a href="#cb67-1008"></a></span>
<span id="cb67-1009"><a href="#cb67-1009"></a><span class="fu">## Read Parquet Files</span></span>
<span id="cb67-1010"><a href="#cb67-1010"></a></span>
<span id="cb67-1011"><a href="#cb67-1011"></a>Load the datasets containing information on accepted loans and rejected applications.</span>
<span id="cb67-1012"><a href="#cb67-1012"></a></span>
<span id="cb67-1015"><a href="#cb67-1015"></a><span class="in">```{python}</span></span>
<span id="cb67-1016"><a href="#cb67-1016"></a><span class="co">#| label: data-load</span></span>
<span id="cb67-1017"><a href="#cb67-1017"></a></span>
<span id="cb67-1018"><a href="#cb67-1018"></a><span class="co"># Define file paths relative to the current script location</span></span>
<span id="cb67-1019"><a href="#cb67-1019"></a>accepted_path <span class="op">=</span> <span class="st">'../Data/lendingclub/accepted_2007_to_2018Q4.parquet'</span></span>
<span id="cb67-1020"><a href="#cb67-1020"></a>rejected_path <span class="op">=</span> <span class="st">'../Data/lendingclub/rejected_2007_to_2018Q4.parquet'</span></span>
<span id="cb67-1021"><a href="#cb67-1021"></a></span>
<span id="cb67-1022"><a href="#cb67-1022"></a><span class="co"># Load data using pandas</span></span>
<span id="cb67-1023"><a href="#cb67-1023"></a><span class="cf">try</span>:</span>
<span id="cb67-1024"><a href="#cb67-1024"></a>    df_accepted <span class="op">=</span> pd.read_parquet(accepted_path)</span>
<span id="cb67-1025"><a href="#cb67-1025"></a>    df_rejected <span class="op">=</span> pd.read_parquet(rejected_path)</span>
<span id="cb67-1026"><a href="#cb67-1026"></a></span>
<span id="cb67-1027"><a href="#cb67-1027"></a>    <span class="co"># Sample rejected data to reduce memory usage</span></span>
<span id="cb67-1028"><a href="#cb67-1028"></a>    max_rejected_rows <span class="op">=</span> <span class="dv">500_000</span></span>
<span id="cb67-1029"><a href="#cb67-1029"></a>    df_rejected <span class="op">=</span> df_rejected.sample(n<span class="op">=</span>max_rejected_rows, random_state<span class="op">=</span><span class="dv">2025</span>)</span>
<span id="cb67-1030"><a href="#cb67-1030"></a></span>
<span id="cb67-1031"><a href="#cb67-1031"></a>    <span class="bu">print</span>(<span class="st">"Data loaded successfully."</span>)</span>
<span id="cb67-1032"><a href="#cb67-1032"></a>    <span class="bu">print</span>(<span class="ss">f"Accepted data shape: </span><span class="sc">{</span>df_accepted<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-1033"><a href="#cb67-1033"></a>    <span class="bu">print</span>(<span class="ss">f"Rejected data shape: </span><span class="sc">{</span>df_rejected<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-1034"><a href="#cb67-1034"></a><span class="cf">except</span> <span class="pp">FileNotFoundError</span>:</span>
<span id="cb67-1035"><a href="#cb67-1035"></a>    <span class="bu">print</span>(<span class="st">"Error: Parquet files not found. Make sure the paths are correct and the data generation scripts have been run."</span>)</span>
<span id="cb67-1036"><a href="#cb67-1036"></a>    <span class="co"># Stop execution or handle error appropriately</span></span>
<span id="cb67-1037"><a href="#cb67-1037"></a>    df_accepted, df_rejected <span class="op">=</span> <span class="va">None</span>, <span class="va">None</span> <span class="co"># Set to None to avoid errors later</span></span>
<span id="cb67-1038"><a href="#cb67-1038"></a><span class="in">```</span></span>
<span id="cb67-1039"><a href="#cb67-1039"></a></span>
<span id="cb67-1040"><a href="#cb67-1040"></a><span class="fu">## Define Target Variable</span></span>
<span id="cb67-1041"><a href="#cb67-1041"></a></span>
<span id="cb67-1042"><a href="#cb67-1042"></a>Create the binary <span class="in">`default_flag`</span> (1 for default, 0 for non-default) based on the <span class="in">`loan_status`</span> in the accepted dataset. Rejected applicants do not have an observed <span class="in">`default_flag`</span>.</span>
<span id="cb67-1043"><a href="#cb67-1043"></a></span>
<span id="cb67-1046"><a href="#cb67-1046"></a><span class="in">```{python}</span></span>
<span id="cb67-1047"><a href="#cb67-1047"></a><span class="co">#| label: define-target</span></span>
<span id="cb67-1048"><a href="#cb67-1048"></a></span>
<span id="cb67-1049"><a href="#cb67-1049"></a><span class="co"># Define default status based on 'loan_status'</span></span>
<span id="cb67-1050"><a href="#cb67-1050"></a>default_statuses <span class="op">=</span> [</span>
<span id="cb67-1051"><a href="#cb67-1051"></a>    <span class="st">"Charged Off"</span>, </span>
<span id="cb67-1052"><a href="#cb67-1052"></a>    <span class="st">"Late (31-120 days)"</span>, </span>
<span id="cb67-1053"><a href="#cb67-1053"></a>    <span class="st">"Does not meet the credit policy. Status:Charged Off"</span>, </span>
<span id="cb67-1054"><a href="#cb67-1054"></a>    <span class="st">"Default"</span></span>
<span id="cb67-1055"><a href="#cb67-1055"></a>] </span>
<span id="cb67-1056"><a href="#cb67-1056"></a>df_accepted[<span class="st">'default_flag'</span>] <span class="op">=</span> df_accepted[<span class="st">'loan_status'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="cf">if</span> x <span class="kw">in</span> default_statuses <span class="cf">else</span> <span class="dv">0</span>)</span>
<span id="cb67-1057"><a href="#cb67-1057"></a></span>
<span id="cb67-1058"><a href="#cb67-1058"></a><span class="bu">print</span>(<span class="st">"Target variable 'default_flag' created."</span>)</span>
<span id="cb67-1059"><a href="#cb67-1059"></a><span class="bu">print</span>(<span class="st">"Default Flag Distribution (Accepted):"</span>)</span>
<span id="cb67-1060"><a href="#cb67-1060"></a><span class="bu">print</span>(df_accepted[<span class="st">'default_flag'</span>].value_counts(normalize<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb67-1061"><a href="#cb67-1061"></a></span>
<span id="cb67-1062"><a href="#cb67-1062"></a><span class="co"># Display loan status counts for context</span></span>
<span id="cb67-1063"><a href="#cb67-1063"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Loan Status Distribution (Accepted):"</span>)</span>
<span id="cb67-1064"><a href="#cb67-1064"></a><span class="bu">print</span>(df_accepted[<span class="st">'loan_status'</span>].value_counts())</span>
<span id="cb67-1065"><a href="#cb67-1065"></a></span>
<span id="cb67-1066"><a href="#cb67-1066"></a></span>
<span id="cb67-1067"><a href="#cb67-1067"></a><span class="in">```</span></span>
<span id="cb67-1068"><a href="#cb67-1068"></a></span>
<span id="cb67-1069"><a href="#cb67-1069"></a><span class="fu">## Train, Calibration, and Test Split</span></span>
<span id="cb67-1070"><a href="#cb67-1070"></a></span>
<span id="cb67-1071"><a href="#cb67-1071"></a>Split both accepted and rejected datasets into training (60%), calibration (20%), and test (20%) sets. We use stratification for the accepted data based on the <span class="in">`default_flag`</span> to maintain similar default rates across splits.</span>
<span id="cb67-1072"><a href="#cb67-1072"></a></span>
<span id="cb67-1075"><a href="#cb67-1075"></a><span class="in">```{python}</span></span>
<span id="cb67-1076"><a href="#cb67-1076"></a><span class="co">#| label: train-test-split</span></span>
<span id="cb67-1077"><a href="#cb67-1077"></a></span>
<span id="cb67-1078"><a href="#cb67-1078"></a><span class="co"># Define split proportions</span></span>
<span id="cb67-1079"><a href="#cb67-1079"></a>train_size <span class="op">=</span> <span class="fl">0.6</span></span>
<span id="cb67-1080"><a href="#cb67-1080"></a>calib_size_rel_to_remaining <span class="op">=</span> <span class="fl">0.5</span> <span class="co"># 0.5 * (1 - 0.6) = 0.2 -&gt; test_size for split_data</span></span>
<span id="cb67-1081"><a href="#cb67-1081"></a>random_seed <span class="op">=</span> <span class="dv">2025</span></span>
<span id="cb67-1082"><a href="#cb67-1082"></a>global_set_seed(random_seed) <span class="co"># Set global seed</span></span>
<span id="cb67-1083"><a href="#cb67-1083"></a></span>
<span id="cb67-1084"><a href="#cb67-1084"></a><span class="co"># --- Split Accepted Data (Stratified) ---</span></span>
<span id="cb67-1085"><a href="#cb67-1085"></a><span class="bu">print</span>(<span class="st">"Splitting Accepted Data..."</span>)</span>
<span id="cb67-1086"><a href="#cb67-1086"></a>df_accepted_train, df_accepted_calib, df_accepted_test <span class="op">=</span> split_data(</span>
<span id="cb67-1087"><a href="#cb67-1087"></a>    df<span class="op">=</span>df_accepted,</span>
<span id="cb67-1088"><a href="#cb67-1088"></a>    target_col<span class="op">=</span><span class="st">'default_flag'</span>,</span>
<span id="cb67-1089"><a href="#cb67-1089"></a>    train_size<span class="op">=</span>train_size,</span>
<span id="cb67-1090"><a href="#cb67-1090"></a>    calib_size_rel<span class="op">=</span>calib_size_rel_to_remaining, <span class="co"># This is test_size in split_data context</span></span>
<span id="cb67-1091"><a href="#cb67-1091"></a>    random_state<span class="op">=</span>random_seed</span>
<span id="cb67-1092"><a href="#cb67-1092"></a>)</span>
<span id="cb67-1093"><a href="#cb67-1093"></a></span>
<span id="cb67-1094"><a href="#cb67-1094"></a><span class="co"># --- Split Rejected Data (Not Stratified) ---</span></span>
<span id="cb67-1095"><a href="#cb67-1095"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Splitting Rejected Data..."</span>)</span>
<span id="cb67-1096"><a href="#cb67-1096"></a>df_rejected_train, df_rejected_calib, df_rejected_test <span class="op">=</span> split_data(</span>
<span id="cb67-1097"><a href="#cb67-1097"></a>    df<span class="op">=</span>df_rejected,</span>
<span id="cb67-1098"><a href="#cb67-1098"></a>    target_col<span class="op">=</span><span class="va">None</span>, <span class="co"># No stratification</span></span>
<span id="cb67-1099"><a href="#cb67-1099"></a>    train_size<span class="op">=</span>train_size,</span>
<span id="cb67-1100"><a href="#cb67-1100"></a>    calib_size_rel<span class="op">=</span>calib_size_rel_to_remaining, <span class="co"># This is test_size in split_data context</span></span>
<span id="cb67-1101"><a href="#cb67-1101"></a>    random_state<span class="op">=</span>random_seed</span>
<span id="cb67-1102"><a href="#cb67-1102"></a>)</span>
<span id="cb67-1103"><a href="#cb67-1103"></a></span>
<span id="cb67-1104"><a href="#cb67-1104"></a><span class="co"># Clean up the original large dataframes</span></span>
<span id="cb67-1105"><a href="#cb67-1105"></a><span class="kw">del</span> df_accepted</span>
<span id="cb67-1106"><a href="#cb67-1106"></a><span class="kw">del</span> df_rejected</span>
<span id="cb67-1107"><a href="#cb67-1107"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Removed original df_accepted and df_rejected to free up memory."</span>)</span>
<span id="cb67-1108"><a href="#cb67-1108"></a>gc.collect() <span class="co"># Call garbage collector</span></span>
<span id="cb67-1109"><a href="#cb67-1109"></a></span>
<span id="cb67-1110"><a href="#cb67-1110"></a></span>
<span id="cb67-1111"><a href="#cb67-1111"></a><span class="in">```</span></span>
<span id="cb67-1112"><a href="#cb67-1112"></a></span>
<span id="cb67-1113"><a href="#cb67-1113"></a><span class="fu">## Exploratory Data Analysis (EDA)</span></span>
<span id="cb67-1114"><a href="#cb67-1114"></a></span>
<span id="cb67-1115"><a href="#cb67-1115"></a>Perform initial EDA on the *training* portions of the accepted and rejected datasets to understand distributions, missing values, and potential issues. We use <span class="in">`ydata-profiling`</span> for automated report generation.</span>
<span id="cb67-1116"><a href="#cb67-1116"></a></span>
<span id="cb67-1119"><a href="#cb67-1119"></a><span class="in">```{python}</span></span>
<span id="cb67-1120"><a href="#cb67-1120"></a><span class="co">#| label: eda-accepted-rejected</span></span>
<span id="cb67-1121"><a href="#cb67-1121"></a></span>
<span id="cb67-1122"><a href="#cb67-1122"></a></span>
<span id="cb67-1123"><a href="#cb67-1123"></a><span class="bu">print</span>(<span class="st">"Generating EDA reports (this might take a while)..."</span>)</span>
<span id="cb67-1124"><a href="#cb67-1124"></a></span>
<span id="cb67-1125"><a href="#cb67-1125"></a><span class="co"># Define sampling fraction for large datasets</span></span>
<span id="cb67-1126"><a href="#cb67-1126"></a>p_frac <span class="op">=</span> <span class="fl">0.05</span> <span class="co"># Sample 5% for faster report generation</span></span>
<span id="cb67-1127"><a href="#cb67-1127"></a></span>
<span id="cb67-1128"><a href="#cb67-1128"></a><span class="co"># --- EDA for Accepted Training Data ---</span></span>
<span id="cb67-1129"><a href="#cb67-1129"></a>generate_eda_report(</span>
<span id="cb67-1130"><a href="#cb67-1130"></a>    df<span class="op">=</span>df_accepted_train,</span>
<span id="cb67-1131"><a href="#cb67-1131"></a>    title<span class="op">=</span><span class="st">"Accepted Train Data Profile"</span>,</span>
<span id="cb67-1132"><a href="#cb67-1132"></a>    output_path<span class="op">=</span><span class="st">"Lab02_eda_report_accepted_train.html"</span>,</span>
<span id="cb67-1133"><a href="#cb67-1133"></a>    sample_frac<span class="op">=</span>p_frac,</span>
<span id="cb67-1134"><a href="#cb67-1134"></a>    random_state<span class="op">=</span>random_seed</span>
<span id="cb67-1135"><a href="#cb67-1135"></a>)</span>
<span id="cb67-1136"><a href="#cb67-1136"></a></span>
<span id="cb67-1137"><a href="#cb67-1137"></a><span class="co"># --- EDA for Rejected Training Data ---</span></span>
<span id="cb67-1138"><a href="#cb67-1138"></a>generate_eda_report(</span>
<span id="cb67-1139"><a href="#cb67-1139"></a>    df<span class="op">=</span>df_rejected_train,</span>
<span id="cb67-1140"><a href="#cb67-1140"></a>    title<span class="op">=</span><span class="st">"Rejected Train Data Profile"</span>,</span>
<span id="cb67-1141"><a href="#cb67-1141"></a>    output_path<span class="op">=</span><span class="st">"Lab02_eda_report_rejected_train.html"</span>,</span>
<span id="cb67-1142"><a href="#cb67-1142"></a>    sample_frac<span class="op">=</span>p_frac,</span>
<span id="cb67-1143"><a href="#cb67-1143"></a>    random_state<span class="op">=</span>random_seed</span>
<span id="cb67-1144"><a href="#cb67-1144"></a>)</span>
<span id="cb67-1145"><a href="#cb67-1145"></a>    </span>
<span id="cb67-1146"><a href="#cb67-1146"></a></span>
<span id="cb67-1147"><a href="#cb67-1147"></a><span class="in">```</span></span>
<span id="cb67-1148"><a href="#cb67-1148"></a></span>
<span id="cb67-1149"><a href="#cb67-1149"></a><span class="fu"># Monotonicity Checks</span></span>
<span id="cb67-1150"><a href="#cb67-1150"></a></span>
<span id="cb67-1151"><a href="#cb67-1151"></a>**Goal:** Process the raw data to create a set of common features present in both accepted and rejected datasets. Then, analyze the relationship between these features and the default outcome (using accepted data) to check for expected monotonic trends.</span>
<span id="cb67-1152"><a href="#cb67-1152"></a></span>
<span id="cb67-1153"><a href="#cb67-1153"></a><span class="fu">## Identify and Process Common Features</span></span>
<span id="cb67-1154"><a href="#cb67-1154"></a></span>
<span id="cb67-1155"><a href="#cb67-1155"></a>Select relevant features, rename columns for consistency, handle missing values, and perform necessary transformations (e.g., parsing employment length, calculating FICO score).</span>
<span id="cb67-1156"><a href="#cb67-1156"></a></span>
<span id="cb67-1159"><a href="#cb67-1159"></a><span class="in">```{python}</span></span>
<span id="cb67-1160"><a href="#cb67-1160"></a><span class="co">#| label: common-features</span></span>
<span id="cb67-1161"><a href="#cb67-1161"></a></span>
<span id="cb67-1162"><a href="#cb67-1162"></a></span>
<span id="cb67-1163"><a href="#cb67-1163"></a><span class="co"># Define common feature names globally</span></span>
<span id="cb67-1164"><a href="#cb67-1164"></a>common_feature_names <span class="op">=</span> [<span class="st">'loan_amnt'</span>, <span class="st">'emp_length'</span>, <span class="st">'addr_state'</span>, <span class="st">'dti'</span>, <span class="st">'credit_score'</span>]</span>
<span id="cb67-1165"><a href="#cb67-1165"></a>target_col <span class="op">=</span> <span class="st">'default_flag'</span></span>
<span id="cb67-1166"><a href="#cb67-1166"></a></span>
<span id="cb67-1167"><a href="#cb67-1167"></a><span class="co"># Remove non-numeric chars from dti in df_rejected_train</span></span>
<span id="cb67-1168"><a href="#cb67-1168"></a>df_rejected_train[<span class="st">'Debt-To-Income Ratio'</span>] <span class="op">=</span> df_rejected_train[<span class="st">'Debt-To-Income Ratio'</span>].astype(<span class="bu">str</span>).<span class="bu">str</span>.replace(<span class="vs">r'[^0-9\.\-]'</span>, <span class="st">''</span>, regex<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb67-1169"><a href="#cb67-1169"></a>df_rejected_train[<span class="st">'Debt-To-Income Ratio'</span>] <span class="op">=</span> pd.to_numeric(df_rejected_train[<span class="st">'Debt-To-Income Ratio'</span>], errors<span class="op">=</span><span class="st">'coerce'</span>)</span>
<span id="cb67-1170"><a href="#cb67-1170"></a></span>
<span id="cb67-1171"><a href="#cb67-1171"></a>rejected_train_median_loan_amnt <span class="op">=</span> df_rejected_train[<span class="st">'Amount Requested'</span>].median(skipna<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb67-1172"><a href="#cb67-1172"></a>rejected_train_median_emp_length <span class="op">=</span> df_rejected_train[<span class="st">'Employment Length'</span>].<span class="bu">apply</span>(parse_emp_length).median(skipna<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb67-1173"><a href="#cb67-1173"></a>rejected_train_p90_dti <span class="op">=</span> df_rejected_train[<span class="st">'Debt-To-Income Ratio'</span>].quantile(<span class="fl">0.90</span>, interpolation<span class="op">=</span><span class="st">'nearest'</span>)</span>
<span id="cb67-1174"><a href="#cb67-1174"></a>rejected_train_p10_credit_score <span class="op">=</span> df_rejected_train[<span class="st">'Risk_Score'</span>].quantile(<span class="fl">0.10</span>, interpolation<span class="op">=</span><span class="st">'nearest'</span>)</span>
<span id="cb67-1175"><a href="#cb67-1175"></a></span>
<span id="cb67-1176"><a href="#cb67-1176"></a><span class="kw">def</span> process_lending_data(df: pd.DataFrame,</span>
<span id="cb67-1177"><a href="#cb67-1177"></a>                        source_type: <span class="bu">str</span>,</span>
<span id="cb67-1178"><a href="#cb67-1178"></a>                        fillMissing_loanamnt: <span class="bu">int</span>,</span>
<span id="cb67-1179"><a href="#cb67-1179"></a>                        fillMissing_emp_length: <span class="bu">int</span>, </span>
<span id="cb67-1180"><a href="#cb67-1180"></a>                        fillMissing_dti: <span class="bu">float</span>,</span>
<span id="cb67-1181"><a href="#cb67-1181"></a>                        fillMissing_credit_score: <span class="bu">int</span>) <span class="op">-&gt;</span> pd.DataFrame <span class="op">|</span> <span class="va">None</span>:</span>
<span id="cb67-1182"><a href="#cb67-1182"></a>    <span class="co">"""</span></span>
<span id="cb67-1183"><a href="#cb67-1183"></a><span class="co">    Processes LendingClub data (accepted or rejected) to extract common features.</span></span>
<span id="cb67-1184"><a href="#cb67-1184"></a><span class="co">    </span></span>
<span id="cb67-1185"><a href="#cb67-1185"></a><span class="co">    This function standardizes column names, handles missing values, transforms</span></span>
<span id="cb67-1186"><a href="#cb67-1186"></a><span class="co">    data types, and applies validations to ensure consistent data formats across</span></span>
<span id="cb67-1187"><a href="#cb67-1187"></a><span class="co">    accepted and rejected loan applications.</span></span>
<span id="cb67-1188"><a href="#cb67-1188"></a></span>
<span id="cb67-1189"><a href="#cb67-1189"></a><span class="co">    Args:</span></span>
<span id="cb67-1190"><a href="#cb67-1190"></a><span class="co">        df (pd.DataFrame): Input DataFrame (either accepted or rejected).</span></span>
<span id="cb67-1191"><a href="#cb67-1191"></a><span class="co">        source_type (str): 'accepted' or 'rejected'.</span></span>
<span id="cb67-1192"><a href="#cb67-1192"></a><span class="co">        fillMissing_loanamnt (int): Value to fill missing loan amount entries.</span></span>
<span id="cb67-1193"><a href="#cb67-1193"></a><span class="co">        fillMissing_emp_length (int): Value to fill missing employment length entries.</span></span>
<span id="cb67-1194"><a href="#cb67-1194"></a><span class="co">        fillMissing_dti (float): Value to fill missing debt-to-income ratio entries.</span></span>
<span id="cb67-1195"><a href="#cb67-1195"></a><span class="co">        fillMissing_credit_score (int): Value to fill missing credit score entries.</span></span>
<span id="cb67-1196"><a href="#cb67-1196"></a></span>
<span id="cb67-1197"><a href="#cb67-1197"></a><span class="co">    Returns:</span></span>
<span id="cb67-1198"><a href="#cb67-1198"></a><span class="co">        pd.DataFrame | None: Processed DataFrame with common features (and target for accepted),</span></span>
<span id="cb67-1199"><a href="#cb67-1199"></a><span class="co">                             or None if input df is None.</span></span>
<span id="cb67-1200"><a href="#cb67-1200"></a><span class="co">                             </span></span>
<span id="cb67-1201"><a href="#cb67-1201"></a><span class="co">    Raises:</span></span>
<span id="cb67-1202"><a href="#cb67-1202"></a><span class="co">        ValueError: If source_type is not 'accepted' or 'rejected'.</span></span>
<span id="cb67-1203"><a href="#cb67-1203"></a><span class="co">    """</span></span>
<span id="cb67-1204"><a href="#cb67-1204"></a>    <span class="cf">if</span> df <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb67-1205"><a href="#cb67-1205"></a>        <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb67-1206"><a href="#cb67-1206"></a></span>
<span id="cb67-1207"><a href="#cb67-1207"></a>    df_proc <span class="op">=</span> df.copy()</span>
<span id="cb67-1208"><a href="#cb67-1208"></a></span>
<span id="cb67-1209"><a href="#cb67-1209"></a>    <span class="co"># Column mapping and credit_score logic</span></span>
<span id="cb67-1210"><a href="#cb67-1210"></a>    <span class="cf">if</span> source_type <span class="op">==</span> <span class="st">'accepted'</span>:</span>
<span id="cb67-1211"><a href="#cb67-1211"></a>        rename_map <span class="op">=</span> {</span>
<span id="cb67-1212"><a href="#cb67-1212"></a>            <span class="st">'loan_amnt'</span>: <span class="st">'loan_amnt'</span>,</span>
<span id="cb67-1213"><a href="#cb67-1213"></a>            <span class="st">'emp_length'</span>: <span class="st">'emp_length'</span>,</span>
<span id="cb67-1214"><a href="#cb67-1214"></a>            <span class="st">'addr_state'</span>: <span class="st">'addr_state'</span>,</span>
<span id="cb67-1215"><a href="#cb67-1215"></a>            <span class="st">'dti'</span>: <span class="st">'dti'</span>,</span>
<span id="cb67-1216"><a href="#cb67-1216"></a>            <span class="st">'fico_range_low'</span>: <span class="st">'fico_range_low'</span>,</span>
<span id="cb67-1217"><a href="#cb67-1217"></a>            <span class="st">'fico_range_high'</span>: <span class="st">'fico_range_high'</span></span>
<span id="cb67-1218"><a href="#cb67-1218"></a>        }</span>
<span id="cb67-1219"><a href="#cb67-1219"></a>    <span class="cf">elif</span> source_type <span class="op">==</span> <span class="st">'rejected'</span>:</span>
<span id="cb67-1220"><a href="#cb67-1220"></a>        rename_map <span class="op">=</span> {</span>
<span id="cb67-1221"><a href="#cb67-1221"></a>            <span class="st">'Amount Requested'</span>: <span class="st">'loan_amnt'</span>,</span>
<span id="cb67-1222"><a href="#cb67-1222"></a>            <span class="st">'Employment Length'</span>: <span class="st">'emp_length'</span>,</span>
<span id="cb67-1223"><a href="#cb67-1223"></a>            <span class="st">'State'</span>: <span class="st">'addr_state'</span>,</span>
<span id="cb67-1224"><a href="#cb67-1224"></a>            <span class="st">'Debt-To-Income Ratio'</span>: <span class="st">'dti'</span>,</span>
<span id="cb67-1225"><a href="#cb67-1225"></a>            <span class="st">'Risk_Score'</span>: <span class="st">'credit_score'</span></span>
<span id="cb67-1226"><a href="#cb67-1226"></a>        }</span>
<span id="cb67-1227"><a href="#cb67-1227"></a>    <span class="cf">else</span>:</span>
<span id="cb67-1228"><a href="#cb67-1228"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"source_type must be 'accepted' or 'rejected'"</span>)</span>
<span id="cb67-1229"><a href="#cb67-1229"></a></span>
<span id="cb67-1230"><a href="#cb67-1230"></a>    <span class="co"># Warn if missing columns</span></span>
<span id="cb67-1231"><a href="#cb67-1231"></a>    missing_cols <span class="op">=</span> [col <span class="cf">for</span> col <span class="kw">in</span> rename_map.keys() <span class="cf">if</span> col <span class="kw">not</span> <span class="kw">in</span> df_proc.columns]</span>
<span id="cb67-1232"><a href="#cb67-1232"></a>    <span class="cf">if</span> missing_cols:</span>
<span id="cb67-1233"><a href="#cb67-1233"></a>        <span class="bu">print</span>(<span class="ss">f"Warning: Missing columns in </span><span class="sc">{</span>source_type<span class="sc">}</span><span class="ss"> data: </span><span class="sc">{</span>missing_cols<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-1234"><a href="#cb67-1234"></a>    df_proc <span class="op">=</span> df_proc.rename(columns<span class="op">=</span>rename_map)</span>
<span id="cb67-1235"><a href="#cb67-1235"></a></span>
<span id="cb67-1236"><a href="#cb67-1236"></a>    <span class="co"># loan_amnt</span></span>
<span id="cb67-1237"><a href="#cb67-1237"></a>    df_proc[<span class="st">'loan_amnt'</span>] <span class="op">=</span> pd.to_numeric(df_proc.get(<span class="st">'loan_amnt'</span>), errors<span class="op">=</span><span class="st">'coerce'</span>).fillna(fillMissing_loanamnt).clip(lower<span class="op">=</span><span class="dv">500</span>)</span>
<span id="cb67-1238"><a href="#cb67-1238"></a></span>
<span id="cb67-1239"><a href="#cb67-1239"></a>    <span class="co"># emp_length</span></span>
<span id="cb67-1240"><a href="#cb67-1240"></a>    df_proc[<span class="st">'emp_length'</span>] <span class="op">=</span> df_proc.get(<span class="st">'emp_length'</span>).<span class="bu">apply</span>(parse_emp_length).fillna(fillMissing_emp_length) </span>
<span id="cb67-1241"><a href="#cb67-1241"></a></span>
<span id="cb67-1242"><a href="#cb67-1242"></a>    <span class="co"># dti</span></span>
<span id="cb67-1243"><a href="#cb67-1243"></a>    dti_series <span class="op">=</span> df_proc.get(<span class="st">'dti'</span>).astype(<span class="bu">str</span>).<span class="bu">str</span>.replace(<span class="vs">r'[^0-9\.\-]'</span>, <span class="st">''</span>, regex<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb67-1244"><a href="#cb67-1244"></a>    df_proc[<span class="st">'dti'</span>] <span class="op">=</span> pd.to_numeric(dti_series, errors<span class="op">=</span><span class="st">'coerce'</span>).fillna(fillMissing_dti).astype(<span class="st">"float32"</span>).clip(lower<span class="op">=</span><span class="dv">0</span>, upper<span class="op">=</span><span class="dv">300</span>)</span>
<span id="cb67-1245"><a href="#cb67-1245"></a></span>
<span id="cb67-1246"><a href="#cb67-1246"></a>    <span class="co"># credit_score</span></span>
<span id="cb67-1247"><a href="#cb67-1247"></a>    <span class="cf">if</span> source_type <span class="op">==</span> <span class="st">'accepted'</span>:</span>
<span id="cb67-1248"><a href="#cb67-1248"></a>        <span class="co"># Use FICO if available, else fallback</span></span>
<span id="cb67-1249"><a href="#cb67-1249"></a>        <span class="cf">if</span> <span class="st">'fico_range_low'</span> <span class="kw">in</span> df_proc.columns <span class="kw">and</span> <span class="st">'fico_range_high'</span> <span class="kw">in</span> df_proc.columns:</span>
<span id="cb67-1250"><a href="#cb67-1250"></a>            df_proc[<span class="st">'credit_score'</span>] <span class="op">=</span> (df_proc[<span class="st">'fico_range_low'</span>] <span class="op">+</span> df_proc[<span class="st">'fico_range_high'</span>]) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb67-1251"><a href="#cb67-1251"></a>        <span class="cf">else</span>:</span>
<span id="cb67-1252"><a href="#cb67-1252"></a>            df_proc[<span class="st">'credit_score'</span>] <span class="op">=</span> np.nan</span>
<span id="cb67-1253"><a href="#cb67-1253"></a>        df_proc[<span class="st">'credit_score'</span>] <span class="op">=</span> df_proc[<span class="st">'credit_score'</span>].fillna(fillMissing_credit_score).clip(lower<span class="op">=</span><span class="dv">300</span>, upper<span class="op">=</span><span class="dv">850</span>)</span>
<span id="cb67-1254"><a href="#cb67-1254"></a>    <span class="cf">else</span>:</span>
<span id="cb67-1255"><a href="#cb67-1255"></a>        df_proc[<span class="st">'credit_score'</span>] <span class="op">=</span> pd.to_numeric(df_proc.get(<span class="st">'credit_score'</span>), errors<span class="op">=</span><span class="st">'coerce'</span>).fillna(fillMissing_credit_score).clip(lower<span class="op">=</span><span class="dv">300</span>, upper<span class="op">=</span><span class="dv">850</span>)</span>
<span id="cb67-1256"><a href="#cb67-1256"></a></span>
<span id="cb67-1257"><a href="#cb67-1257"></a>    <span class="co"># Target column for accepted</span></span>
<span id="cb67-1258"><a href="#cb67-1258"></a>    <span class="cf">if</span> source_type <span class="op">==</span> <span class="st">'accepted'</span>:</span>
<span id="cb67-1259"><a href="#cb67-1259"></a>        <span class="cf">if</span> target_col <span class="kw">not</span> <span class="kw">in</span> df_proc.columns:</span>
<span id="cb67-1260"><a href="#cb67-1260"></a>            <span class="bu">print</span>(<span class="ss">f"Warning: Target column '</span><span class="sc">{</span>target_col<span class="sc">}</span><span class="ss">' not found in accepted data. Adding placeholder."</span>)</span>
<span id="cb67-1261"><a href="#cb67-1261"></a>            df_proc[target_col] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb67-1262"><a href="#cb67-1262"></a>        cols_to_keep <span class="op">=</span> [col <span class="cf">for</span> col <span class="kw">in</span> common_feature_names <span class="cf">if</span> col <span class="kw">in</span> df_proc.columns] <span class="op">+</span> [target_col]</span>
<span id="cb67-1263"><a href="#cb67-1263"></a>    <span class="cf">else</span>:</span>
<span id="cb67-1264"><a href="#cb67-1264"></a>        cols_to_keep <span class="op">=</span> [col <span class="cf">for</span> col <span class="kw">in</span> common_feature_names <span class="cf">if</span> col <span class="kw">in</span> df_proc.columns]</span>
<span id="cb67-1265"><a href="#cb67-1265"></a></span>
<span id="cb67-1266"><a href="#cb67-1266"></a>    <span class="cf">return</span> df_proc[cols_to_keep].copy()</span>
<span id="cb67-1267"><a href="#cb67-1267"></a></span>
<span id="cb67-1268"><a href="#cb67-1268"></a><span class="co"># --- Apply the function to the training data ---</span></span>
<span id="cb67-1269"><a href="#cb67-1269"></a><span class="bu">print</span>(<span class="st">"Processing training data using the defined function..."</span>)</span>
<span id="cb67-1270"><a href="#cb67-1270"></a></span>
<span id="cb67-1271"><a href="#cb67-1271"></a>df_accepted_train_common <span class="op">=</span> process_lending_data(</span>
<span id="cb67-1272"><a href="#cb67-1272"></a>    df<span class="op">=</span>df_accepted_train, </span>
<span id="cb67-1273"><a href="#cb67-1273"></a>    source_type<span class="op">=</span><span class="st">'accepted'</span>,</span>
<span id="cb67-1274"><a href="#cb67-1274"></a>    fillMissing_loanamnt<span class="op">=</span>rejected_train_median_loan_amnt,</span>
<span id="cb67-1275"><a href="#cb67-1275"></a>    fillMissing_emp_length<span class="op">=</span>rejected_train_median_emp_length, </span>
<span id="cb67-1276"><a href="#cb67-1276"></a>    fillMissing_dti<span class="op">=</span>rejected_train_p90_dti,</span>
<span id="cb67-1277"><a href="#cb67-1277"></a>    fillMissing_credit_score<span class="op">=</span>rejected_train_p10_credit_score</span>
<span id="cb67-1278"><a href="#cb67-1278"></a>)</span>
<span id="cb67-1279"><a href="#cb67-1279"></a></span>
<span id="cb67-1280"><a href="#cb67-1280"></a>df_rejected_train_common <span class="op">=</span> process_lending_data(</span>
<span id="cb67-1281"><a href="#cb67-1281"></a>    df<span class="op">=</span>df_rejected_train, </span>
<span id="cb67-1282"><a href="#cb67-1282"></a>    source_type<span class="op">=</span><span class="st">'rejected'</span>,</span>
<span id="cb67-1283"><a href="#cb67-1283"></a>    fillMissing_loanamnt<span class="op">=</span>rejected_train_median_loan_amnt,</span>
<span id="cb67-1284"><a href="#cb67-1284"></a>    fillMissing_emp_length<span class="op">=</span>rejected_train_median_emp_length, </span>
<span id="cb67-1285"><a href="#cb67-1285"></a>    fillMissing_dti<span class="op">=</span>rejected_train_p90_dti,</span>
<span id="cb67-1286"><a href="#cb67-1286"></a>    fillMissing_credit_score<span class="op">=</span>rejected_train_p10_credit_score</span>
<span id="cb67-1287"><a href="#cb67-1287"></a>)</span>
<span id="cb67-1288"><a href="#cb67-1288"></a></span>
<span id="cb67-1289"><a href="#cb67-1289"></a><span class="bu">print</span>(<span class="ss">f"Accepted training data processed. Shape: </span><span class="sc">{</span>df_accepted_train_common<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-1290"><a href="#cb67-1290"></a><span class="co"># Verify common features are present</span></span>
<span id="cb67-1291"><a href="#cb67-1291"></a>missing_acc <span class="op">=</span> [f <span class="cf">for</span> f <span class="kw">in</span> common_feature_names <span class="cf">if</span> f <span class="kw">not</span> <span class="kw">in</span> df_accepted_train_common.columns]</span>
<span id="cb67-1292"><a href="#cb67-1292"></a><span class="cf">if</span> missing_acc: <span class="bu">print</span>(<span class="ss">f"  Warning: Missing common features after processing accepted: </span><span class="sc">{</span>missing_acc<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-1293"><a href="#cb67-1293"></a></span>
<span id="cb67-1294"><a href="#cb67-1294"></a></span>
<span id="cb67-1295"><a href="#cb67-1295"></a><span class="bu">print</span>(<span class="ss">f"Rejected training data processed. Shape: </span><span class="sc">{</span>df_rejected_train_common<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-1296"><a href="#cb67-1296"></a><span class="co"># Verify common features are present</span></span>
<span id="cb67-1297"><a href="#cb67-1297"></a>missing_rej <span class="op">=</span> [f <span class="cf">for</span> f <span class="kw">in</span> common_feature_names <span class="cf">if</span> f <span class="kw">not</span> <span class="kw">in</span> df_rejected_train_common.columns]</span>
<span id="cb67-1298"><a href="#cb67-1298"></a><span class="cf">if</span> missing_rej: <span class="bu">print</span>(<span class="ss">f"  Warning: Missing common features after processing rejected: </span><span class="sc">{</span>missing_rej<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-1299"><a href="#cb67-1299"></a></span>
<span id="cb67-1300"><a href="#cb67-1300"></a></span>
<span id="cb67-1301"><a href="#cb67-1301"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Common features expected:"</span>)</span>
<span id="cb67-1302"><a href="#cb67-1302"></a><span class="bu">print</span>(common_feature_names)</span>
<span id="cb67-1303"><a href="#cb67-1303"></a></span>
<span id="cb67-1304"><a href="#cb67-1304"></a></span>
<span id="cb67-1305"><a href="#cb67-1305"></a><span class="in">```</span></span>
<span id="cb67-1306"><a href="#cb67-1306"></a></span>
<span id="cb67-1307"><a href="#cb67-1307"></a>Check dtypes</span>
<span id="cb67-1308"><a href="#cb67-1308"></a></span>
<span id="cb67-1311"><a href="#cb67-1311"></a><span class="in">```{python}</span></span>
<span id="cb67-1312"><a href="#cb67-1312"></a><span class="co">#| label: tbl-train-dtypes</span></span>
<span id="cb67-1313"><a href="#cb67-1313"></a><span class="co">#| tbl-cap: "Data types in each data frame"</span></span>
<span id="cb67-1314"><a href="#cb67-1314"></a></span>
<span id="cb67-1315"><a href="#cb67-1315"></a><span class="co"># Convert .info() output to DataFrame for accepted training data</span></span>
<span id="cb67-1316"><a href="#cb67-1316"></a>accepted_info <span class="op">=</span> pd.DataFrame({</span>
<span id="cb67-1317"><a href="#cb67-1317"></a>    <span class="st">"column"</span>: df_accepted_train_common.columns,</span>
<span id="cb67-1318"><a href="#cb67-1318"></a>    <span class="st">"dtype"</span>: [df_accepted_train_common[col].dtype <span class="cf">for</span> col <span class="kw">in</span> df_accepted_train_common.columns],</span>
<span id="cb67-1319"><a href="#cb67-1319"></a>    <span class="st">"null_count"</span>: [df_accepted_train_common[col].isnull().<span class="bu">sum</span>() <span class="cf">for</span> col <span class="kw">in</span> df_accepted_train_common.columns],</span>
<span id="cb67-1320"><a href="#cb67-1320"></a>    <span class="st">"non_null_count"</span>: [df_accepted_train_common[col].notnull().<span class="bu">sum</span>() <span class="cf">for</span> col <span class="kw">in</span> df_accepted_train_common.columns],</span>
<span id="cb67-1321"><a href="#cb67-1321"></a>    <span class="st">"unique_count"</span>: [df_accepted_train_common[col].nunique() <span class="cf">for</span> col <span class="kw">in</span> df_accepted_train_common.columns]</span>
<span id="cb67-1322"><a href="#cb67-1322"></a>})</span>
<span id="cb67-1323"><a href="#cb67-1323"></a>display(accepted_info)</span>
<span id="cb67-1324"><a href="#cb67-1324"></a></span>
<span id="cb67-1325"><a href="#cb67-1325"></a><span class="co"># Convert .info() output to DataFrame for rejected training data</span></span>
<span id="cb67-1326"><a href="#cb67-1326"></a>rejected_info <span class="op">=</span> pd.DataFrame({</span>
<span id="cb67-1327"><a href="#cb67-1327"></a>    <span class="st">"column"</span>: df_rejected_train_common.columns,</span>
<span id="cb67-1328"><a href="#cb67-1328"></a>    <span class="st">"dtype"</span>: [df_rejected_train_common[col].dtype <span class="cf">for</span> col <span class="kw">in</span> df_rejected_train_common.columns],</span>
<span id="cb67-1329"><a href="#cb67-1329"></a>    <span class="st">"null_count"</span>: [df_rejected_train_common[col].isnull().<span class="bu">sum</span>() <span class="cf">for</span> col <span class="kw">in</span> df_rejected_train_common.columns],</span>
<span id="cb67-1330"><a href="#cb67-1330"></a>    <span class="st">"non_null_count"</span>: [df_rejected_train_common[col].notnull().<span class="bu">sum</span>() <span class="cf">for</span> col <span class="kw">in</span> df_rejected_train_common.columns],</span>
<span id="cb67-1331"><a href="#cb67-1331"></a>    <span class="st">"unique_count"</span>: [df_rejected_train_common[col].nunique() <span class="cf">for</span> col <span class="kw">in</span> df_rejected_train_common.columns]</span>
<span id="cb67-1332"><a href="#cb67-1332"></a>})</span>
<span id="cb67-1333"><a href="#cb67-1333"></a></span>
<span id="cb67-1334"><a href="#cb67-1334"></a>display(rejected_info)</span>
<span id="cb67-1335"><a href="#cb67-1335"></a><span class="in">```</span></span>
<span id="cb67-1336"><a href="#cb67-1336"></a></span>
<span id="cb67-1337"><a href="#cb67-1337"></a><span class="fu">## Compare Data Distributions</span></span>
<span id="cb67-1338"><a href="#cb67-1338"></a></span>
<span id="cb67-1339"><a href="#cb67-1339"></a>Compare data distributions between <span class="in">`df_accepted_train_common`</span> and <span class="in">`df_rejected_train_common`</span> using <span class="in">`ProfileReport`</span>.</span>
<span id="cb67-1340"><a href="#cb67-1340"></a></span>
<span id="cb67-1343"><a href="#cb67-1343"></a><span class="in">```{python}</span></span>
<span id="cb67-1344"><a href="#cb67-1344"></a><span class="co">#| label: eda-compare-common</span></span>
<span id="cb67-1345"><a href="#cb67-1345"></a></span>
<span id="cb67-1346"><a href="#cb67-1346"></a></span>
<span id="cb67-1347"><a href="#cb67-1347"></a><span class="co"># Check if dataframes and ProfileReport exist</span></span>
<span id="cb67-1348"><a href="#cb67-1348"></a><span class="cf">if</span> <span class="st">'df_accepted_train_common'</span> <span class="kw">in</span> <span class="bu">locals</span>() <span class="kw">and</span> df_accepted_train_common <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> <span class="op">\</span></span>
<span id="cb67-1349"><a href="#cb67-1349"></a>    <span class="st">'df_rejected_train_common'</span> <span class="kw">in</span> <span class="bu">locals</span>() <span class="kw">and</span> df_rejected_train_common <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> <span class="op">\</span></span>
<span id="cb67-1350"><a href="#cb67-1350"></a>    <span class="st">'ProfileReport'</span> <span class="kw">in</span> <span class="bu">locals</span>() <span class="kw">and</span> ProfileReport <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb67-1351"><a href="#cb67-1351"></a></span>
<span id="cb67-1352"><a href="#cb67-1352"></a>     <span class="bu">print</span>(<span class="st">"Generating comparison report for common features (this might take a while)..."</span>)</span>
<span id="cb67-1353"><a href="#cb67-1353"></a></span>
<span id="cb67-1354"><a href="#cb67-1354"></a>     <span class="co"># Define sampling fraction for faster report generation</span></span>
<span id="cb67-1355"><a href="#cb67-1355"></a>     p_frac_compare <span class="op">=</span> <span class="fl">0.1</span> <span class="co"># Sample 10% for comparison</span></span>
<span id="cb67-1356"><a href="#cb67-1356"></a></span>
<span id="cb67-1357"><a href="#cb67-1357"></a>     <span class="cf">try</span>:</span>
<span id="cb67-1358"><a href="#cb67-1358"></a>          <span class="co"># Profile for Accepted Common Features</span></span>
<span id="cb67-1359"><a href="#cb67-1359"></a>          accepted_common_profile <span class="op">=</span> ProfileReport(</span>
<span id="cb67-1360"><a href="#cb67-1360"></a>                df_accepted_train_common.sample(frac<span class="op">=</span>p_frac_compare, random_state<span class="op">=</span><span class="dv">2025</span>),</span>
<span id="cb67-1361"><a href="#cb67-1361"></a>                title<span class="op">=</span><span class="st">"Accepted Train"</span>,</span>
<span id="cb67-1362"><a href="#cb67-1362"></a>                progress_bar<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb67-1363"><a href="#cb67-1363"></a>                duplicates<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb67-1364"><a href="#cb67-1364"></a>                interactions<span class="op">=</span><span class="va">None</span></span>
<span id="cb67-1365"><a href="#cb67-1365"></a>          )</span>
<span id="cb67-1366"><a href="#cb67-1366"></a></span>
<span id="cb67-1367"><a href="#cb67-1367"></a>          <span class="co"># Profile for Rejected Common Features</span></span>
<span id="cb67-1368"><a href="#cb67-1368"></a>          rejected_common_profile <span class="op">=</span> ProfileReport(</span>
<span id="cb67-1369"><a href="#cb67-1369"></a>                df_rejected_train_common.sample(frac<span class="op">=</span>p_frac_compare, random_state<span class="op">=</span><span class="dv">2025</span>),</span>
<span id="cb67-1370"><a href="#cb67-1370"></a>                title<span class="op">=</span><span class="st">"Rejected Train"</span>,</span>
<span id="cb67-1371"><a href="#cb67-1371"></a>                progress_bar<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb67-1372"><a href="#cb67-1372"></a>                duplicates<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb67-1373"><a href="#cb67-1373"></a>                interactions<span class="op">=</span><span class="va">None</span></span>
<span id="cb67-1374"><a href="#cb67-1374"></a>          )</span>
<span id="cb67-1375"><a href="#cb67-1375"></a></span>
<span id="cb67-1376"><a href="#cb67-1376"></a>          <span class="co"># Compare the two profiles</span></span>
<span id="cb67-1377"><a href="#cb67-1377"></a>          comparison_report <span class="op">=</span> accepted_common_profile.compare(rejected_common_profile)</span>
<span id="cb67-1378"><a href="#cb67-1378"></a></span>
<span id="cb67-1379"><a href="#cb67-1379"></a>          <span class="co"># Save the comparison report</span></span>
<span id="cb67-1380"><a href="#cb67-1380"></a>          comparison_report_path <span class="op">=</span> <span class="st">"Lab02_eda_report_compare_common_features.html"</span></span>
<span id="cb67-1381"><a href="#cb67-1381"></a>          comparison_report.to_file(comparison_report_path)</span>
<span id="cb67-1382"><a href="#cb67-1382"></a>          <span class="bu">print</span>(<span class="ss">f"Comparison report saved to: </span><span class="sc">{</span>comparison_report_path<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-1383"><a href="#cb67-1383"></a></span>
<span id="cb67-1384"><a href="#cb67-1384"></a>     <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb67-1385"><a href="#cb67-1385"></a>          <span class="bu">print</span>(<span class="ss">f"Error generating comparison report: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-1386"><a href="#cb67-1386"></a></span>
<span id="cb67-1387"><a href="#cb67-1387"></a><span class="cf">else</span>:</span>
<span id="cb67-1388"><a href="#cb67-1388"></a>     <span class="bu">print</span>(<span class="st">"Skipping comparison report generation: Required dataframes or ProfileReport not available."</span>)</span>
<span id="cb67-1389"><a href="#cb67-1389"></a></span>
<span id="cb67-1390"><a href="#cb67-1390"></a><span class="in">```</span></span>
<span id="cb67-1391"><a href="#cb67-1391"></a></span>
<span id="cb67-1392"><a href="#cb67-1392"></a><span class="fu">## Assess Feature Monotonicity (Accepted Data)</span></span>
<span id="cb67-1393"><a href="#cb67-1393"></a></span>
<span id="cb67-1394"><a href="#cb67-1394"></a>**Concept:** Before building complex models, it's essential to understand the fundamental relationship between key features and the target variable (probability of default). We expect certain features to have a monotonic relationship with risk  meaning, as the feature value increases, the risk should consistently increase or consistently decrease.</span>
<span id="cb67-1395"><a href="#cb67-1395"></a></span>
<span id="cb67-1396"><a href="#cb67-1396"></a><span class="ss">*   </span>**Example:** We expect higher Debt-to-Income (<span class="in">`dti`</span>) ratios to correspond to higher default risk (monotonic increasing). We expect higher credit scores (<span class="in">`credit_score`</span>) to correspond to lower default risk (monotonic decreasing).</span>
<span id="cb67-1397"><a href="#cb67-1397"></a></span>
<span id="cb67-1398"><a href="#cb67-1398"></a>**Method:** We use the <span class="in">`binned_prob_plot`</span> function on the *accepted training data* (<span class="in">`df_accepted_train_common`</span>) to visualize the average default rate across bins of each feature.</span>
<span id="cb67-1399"><a href="#cb67-1399"></a><span class="ss">*   </span>For **continuous features**, the plot shows the trend across quantiles, and we calculate the **Spearman rank correlation** between the bin rank and the default rate (or log-odds). A correlation near +1 (increasing) or -1 (decreasing) suggests strong monotonicity.</span>
<span id="cb67-1400"><a href="#cb67-1400"></a><span class="ss">*   </span>For **categorical features**, the plot shows the default rate per category, and we calculate **Mutual Information** to measure if the feature provides information about the target, indicating varying risk levels across categories.</span>
<span id="cb67-1401"><a href="#cb67-1401"></a></span>
<span id="cb67-1402"><a href="#cb67-1402"></a>**Goal:** Visualize the relationship between selected common features (<span class="in">`loan_amnt`</span>, <span class="in">`dti`</span>, <span class="in">`credit_score`</span>, <span class="in">`emp_length`</span>) and <span class="in">`default_flag`</span> using the accepted training data. Check if the observed trends align with business intuition.</span>
<span id="cb67-1403"><a href="#cb67-1403"></a></span>
<span id="cb67-1406"><a href="#cb67-1406"></a><span class="in">```{python}</span></span>
<span id="cb67-1407"><a href="#cb67-1407"></a><span class="co">#| label: fig-check-monotonicity-accepted</span></span>
<span id="cb67-1408"><a href="#cb67-1408"></a><span class="co">#| fig-cap: "Binned probability plots for the accept population."</span></span>
<span id="cb67-1409"><a href="#cb67-1409"></a></span>
<span id="cb67-1410"><a href="#cb67-1410"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb67-1411"><a href="#cb67-1411"></a></span>
<span id="cb67-1412"><a href="#cb67-1412"></a></span>
<span id="cb67-1413"><a href="#cb67-1413"></a><span class="cf">if</span> <span class="st">'df_accepted_train_common'</span> <span class="kw">in</span> <span class="bu">locals</span>() <span class="kw">and</span> df_accepted_train_common <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb67-1414"><a href="#cb67-1414"></a>    <span class="bu">print</span>(<span class="st">"--- Assessing Monotonicity on Accepted Training Data ---"</span>)</span>
<span id="cb67-1415"><a href="#cb67-1415"></a>    features_to_plot <span class="op">=</span> [<span class="st">'loan_amnt'</span>, <span class="st">'dti'</span>, <span class="st">'credit_score'</span>, <span class="st">'emp_length'</span>]</span>
<span id="cb67-1416"><a href="#cb67-1416"></a>    monotonicity_results_accepted <span class="op">=</span> []</span>
<span id="cb67-1417"><a href="#cb67-1417"></a>    </span>
<span id="cb67-1418"><a href="#cb67-1418"></a>    <span class="cf">for</span> feature <span class="kw">in</span> features_to_plot:</span>
<span id="cb67-1419"><a href="#cb67-1419"></a>        <span class="cf">if</span> feature <span class="kw">in</span> df_accepted_train_common.columns:</span>
<span id="cb67-1420"><a href="#cb67-1420"></a>            <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Plotting for feature: </span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-1421"><a href="#cb67-1421"></a>            </span>
<span id="cb67-1422"><a href="#cb67-1422"></a>            output <span class="op">=</span> binned_prob_plot(</span>
<span id="cb67-1423"><a href="#cb67-1423"></a>                data<span class="op">=</span>df_accepted_train_common, </span>
<span id="cb67-1424"><a href="#cb67-1424"></a>                feature<span class="op">=</span>feature, </span>
<span id="cb67-1425"><a href="#cb67-1425"></a>                target_binary<span class="op">=</span><span class="st">'default_flag'</span>,</span>
<span id="cb67-1426"><a href="#cb67-1426"></a>                cont_feat_flag<span class="op">=</span><span class="va">True</span></span>
<span id="cb67-1427"><a href="#cb67-1427"></a>            )</span>
<span id="cb67-1428"><a href="#cb67-1428"></a>            monotonicity_results_accepted.append(output)</span>
<span id="cb67-1429"><a href="#cb67-1429"></a>            <span class="bu">print</span>(<span class="ss">f"  Measure (</span><span class="sc">{</span>output[<span class="st">'measure_name'</span>]<span class="sc">}</span><span class="ss">): </span><span class="sc">{</span>output[<span class="st">'measure_value'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb67-1430"><a href="#cb67-1430"></a>            <span class="cf">if</span> output[<span class="st">'p_value'</span>] <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb67-1431"><a href="#cb67-1431"></a>                 <span class="bu">print</span>(<span class="ss">f"  P-value: </span><span class="sc">{</span>output[<span class="st">'p_value'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb67-1432"><a href="#cb67-1432"></a>        <span class="cf">else</span>:</span>
<span id="cb67-1433"><a href="#cb67-1433"></a>            <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Skipping plot for feature '</span><span class="sc">{</span>feature<span class="sc">}</span><span class="ss">' as it's not in df_accepted_train_common."</span>)</span>
<span id="cb67-1434"><a href="#cb67-1434"></a>            </span>
<span id="cb67-1435"><a href="#cb67-1435"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">--- Monotonicity Assessment Summary (Accepted Data) ---"</span>)</span>
<span id="cb67-1436"><a href="#cb67-1436"></a>    <span class="cf">for</span> res <span class="kw">in</span> monotonicity_results_accepted:</span>
<span id="cb67-1437"><a href="#cb67-1437"></a>        pval_str <span class="op">=</span> <span class="ss">f", p-value: </span><span class="sc">{</span>res[<span class="st">'p_value'</span>]<span class="sc">:.4f}</span><span class="ss">"</span> <span class="cf">if</span> res[<span class="st">'p_value'</span>] <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> <span class="st">""</span></span>
<span id="cb67-1438"><a href="#cb67-1438"></a>        <span class="bu">print</span>(<span class="ss">f"Feature: </span><span class="sc">{</span>res[<span class="st">'feature'</span>]<span class="sc">}</span><span class="ss">, Measure: </span><span class="sc">{</span>res[<span class="st">'measure_name'</span>]<span class="sc">}</span><span class="ss">, Value: </span><span class="sc">{</span>res[<span class="st">'measure_value'</span>]<span class="sc">:.4f}{</span>pval_str<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-1439"><a href="#cb67-1439"></a></span>
<span id="cb67-1440"><a href="#cb67-1440"></a><span class="cf">else</span>:</span>
<span id="cb67-1441"><a href="#cb67-1441"></a>    <span class="bu">print</span>(<span class="st">"Skipping monotonicity check as df_accepted_train_common is not available."</span>)</span>
<span id="cb67-1442"><a href="#cb67-1442"></a></span>
<span id="cb67-1443"><a href="#cb67-1443"></a><span class="in">```</span></span>
<span id="cb67-1444"><a href="#cb67-1444"></a></span>
<span id="cb67-1445"><a href="#cb67-1445"></a>**Interpretation:** Based on the binned probability plots:</span>
<span id="cb67-1446"><a href="#cb67-1446"></a></span>
<span id="cb67-1447"><a href="#cb67-1447"></a><span class="ss">-   </span>**loan_amnt:** Higher loan amounts are associated with higher default rates, indicating a positive correlation.</span>
<span id="cb67-1448"><a href="#cb67-1448"></a><span class="ss">-   </span>**dti:** Higher Debt-to-Income ratios correlate with higher default rates, confirming the expected monotonic relationship.</span>
<span id="cb67-1449"><a href="#cb67-1449"></a><span class="ss">-   </span>**credit_score:** Higher credit scores are associated with lower default rates, indicating a negative correlation.</span>
<span id="cb67-1450"><a href="#cb67-1450"></a><span class="ss">-   </span>**emp_length:** Higher employment lengths are associated with lower default rates, which is expected.</span>
<span id="cb67-1451"><a href="#cb67-1451"></a></span>
<span id="cb67-1452"><a href="#cb67-1452"></a>**Categorical Feature Check (State):**</span>
<span id="cb67-1453"><a href="#cb67-1453"></a></span>
<span id="cb67-1454"><a href="#cb67-1454"></a>For high-cardinality categorical features like <span class="in">`addr_state`</span>, visualizing the binned probability might be less informative due to the large number of categories. However, we can still calculate the Mutual Information (MI) score to quantify the relationship between the state and the default flag without generating the plot. A higher MI suggests the state provides more information about the likelihood of default.</span>
<span id="cb67-1455"><a href="#cb67-1455"></a></span>
<span id="cb67-1458"><a href="#cb67-1458"></a><span class="in">```{python}</span></span>
<span id="cb67-1459"><a href="#cb67-1459"></a><span class="co">#| label: check-monotonicity-state</span></span>
<span id="cb67-1460"><a href="#cb67-1460"></a></span>
<span id="cb67-1461"><a href="#cb67-1461"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb67-1462"><a href="#cb67-1462"></a></span>
<span id="cb67-1463"><a href="#cb67-1463"></a></span>
<span id="cb67-1464"><a href="#cb67-1464"></a><span class="cf">if</span> <span class="st">'df_accepted_train_common'</span> <span class="kw">in</span> <span class="bu">locals</span>() <span class="kw">and</span> df_accepted_train_common <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb67-1465"><a href="#cb67-1465"></a>    feature_state <span class="op">=</span> <span class="st">'addr_state'</span></span>
<span id="cb67-1466"><a href="#cb67-1466"></a>    <span class="cf">if</span> feature_state <span class="kw">in</span> df_accepted_train_common.columns:</span>
<span id="cb67-1467"><a href="#cb67-1467"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">--- Calculating Mutual Information for </span><span class="sc">{</span>feature_state<span class="sc">}</span><span class="ss"> (Accepted Data) ---"</span>)</span>
<span id="cb67-1468"><a href="#cb67-1468"></a>        </span>
<span id="cb67-1469"><a href="#cb67-1469"></a>        <span class="co"># Calculate MI without plotting</span></span>
<span id="cb67-1470"><a href="#cb67-1470"></a>        state_mi_output <span class="op">=</span> binned_prob_plot(</span>
<span id="cb67-1471"><a href="#cb67-1471"></a>            data<span class="op">=</span>df_accepted_train_common, </span>
<span id="cb67-1472"><a href="#cb67-1472"></a>            feature<span class="op">=</span>feature_state, </span>
<span id="cb67-1473"><a href="#cb67-1473"></a>            target_binary<span class="op">=</span><span class="st">'default_flag'</span>,</span>
<span id="cb67-1474"><a href="#cb67-1474"></a>            cont_feat_flag<span class="op">=</span><span class="va">False</span>, <span class="co"># Explicitly categorical</span></span>
<span id="cb67-1475"><a href="#cb67-1475"></a>            show_plot<span class="op">=</span><span class="va">False</span> <span class="co"># Do not generate the plot</span></span>
<span id="cb67-1476"><a href="#cb67-1476"></a>        )</span>
<span id="cb67-1477"><a href="#cb67-1477"></a>        </span>
<span id="cb67-1478"><a href="#cb67-1478"></a>        <span class="bu">print</span>(<span class="ss">f"Feature: </span><span class="sc">{</span>state_mi_output[<span class="st">'feature'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-1479"><a href="#cb67-1479"></a>        <span class="bu">print</span>(<span class="ss">f"  Measure: </span><span class="sc">{</span>state_mi_output[<span class="st">'measure_name'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-1480"><a href="#cb67-1480"></a>        <span class="bu">print</span>(<span class="ss">f"  Value: </span><span class="sc">{</span>state_mi_output[<span class="st">'measure_value'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb67-1481"><a href="#cb67-1481"></a>        <span class="co"># Store result if needed</span></span>
<span id="cb67-1482"><a href="#cb67-1482"></a>        <span class="co"># monotonicity_results_accepted.append(state_mi_output) </span></span>
<span id="cb67-1483"><a href="#cb67-1483"></a>    <span class="cf">else</span>:</span>
<span id="cb67-1484"><a href="#cb67-1484"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Skipping MI calculation for feature '</span><span class="sc">{</span>feature_state<span class="sc">}</span><span class="ss">' as it's not in df_accepted_train_common."</span>)</span>
<span id="cb67-1485"><a href="#cb67-1485"></a><span class="cf">else</span>:</span>
<span id="cb67-1486"><a href="#cb67-1486"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Skipping MI calculation for 'addr_state' as df_accepted_train_common is not available."</span>)</span>
<span id="cb67-1487"><a href="#cb67-1487"></a><span class="in">```</span></span>
<span id="cb67-1488"><a href="#cb67-1488"></a></span>
<span id="cb67-1489"><a href="#cb67-1489"></a>Mutual Information for <span class="in">`addr_state`</span> is calculated, but the plot is not generated due to the high cardinality of the feature. The MI value is very close to zero.</span>
<span id="cb67-1490"><a href="#cb67-1490"></a></span>
<span id="cb67-1491"><a href="#cb67-1491"></a><span class="fu">## Define Modeling Features</span></span>
<span id="cb67-1492"><a href="#cb67-1492"></a></span>
<span id="cb67-1493"><a href="#cb67-1493"></a>In subsequent sections, we will use only the following features: <span class="in">`loan_amnt`</span>, <span class="in">`dti`</span>, <span class="in">`credit_score`</span>, and <span class="in">`emp_length`</span> for modeling. The <span class="in">`addr_state`</span> feature will be excluded from the modeling process due to low mutual information.</span>
<span id="cb67-1494"><a href="#cb67-1494"></a></span>
<span id="cb67-1497"><a href="#cb67-1497"></a><span class="in">```{python}</span></span>
<span id="cb67-1498"><a href="#cb67-1498"></a><span class="co">#| label: define-modeling-features</span></span>
<span id="cb67-1499"><a href="#cb67-1499"></a></span>
<span id="cb67-1500"><a href="#cb67-1500"></a>modeling_features <span class="op">=</span> [<span class="st">'loan_amnt'</span>, <span class="st">'dti'</span>, <span class="st">'credit_score'</span>, <span class="st">'emp_length'</span>]</span>
<span id="cb67-1501"><a href="#cb67-1501"></a><span class="bu">print</span>(<span class="ss">f"Features selected for modeling: </span><span class="sc">{</span>modeling_features<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-1502"><a href="#cb67-1502"></a><span class="in">```</span></span>
<span id="cb67-1503"><a href="#cb67-1503"></a></span>
<span id="cb67-1504"><a href="#cb67-1504"></a></span>
<span id="cb67-1505"><a href="#cb67-1505"></a><span class="fu"># Fuzzy Augmentation</span></span>
<span id="cb67-1506"><a href="#cb67-1506"></a></span>
<span id="cb67-1507"><a href="#cb67-1507"></a>**Concept:** Models trained only on accepted applicants (KGB - Known Good/Bad) suffer from selection bias because they don't learn from the rejected population, which often represents higher risk. Reject Inference (RI) techniques address this by incorporating information from rejects.</span>
<span id="cb67-1508"><a href="#cb67-1508"></a></span>
<span id="cb67-1509"><a href="#cb67-1509"></a>**Fuzzy Augmentation:**</span>
<span id="cb67-1510"><a href="#cb67-1510"></a></span>
<span id="cb67-1511"><a href="#cb67-1511"></a><span class="ss">1. </span>Train a model (e.g., Logistic Regression) on the *accepted training data* (<span class="in">`df_accepted_train_common`</span>) to predict the probability of default (PD), <span class="in">`PD = P(default=1 | features)`</span>.</span>
<span id="cb67-1512"><a href="#cb67-1512"></a><span class="ss">2. </span>Apply this model to the *rejected* applicants to get their predicted <span class="in">`PD`</span>.</span>
<span id="cb67-1513"><a href="#cb67-1513"></a><span class="ss">3. </span>Create two copies of the rejected applicant data:</span>
<span id="cb67-1514"><a href="#cb67-1514"></a><span class="ss">    1. </span>**Copy 1 (Assumed Bad):** Assign <span class="in">`default_flag = 1`</span> and <span class="in">`sample_weight = PD`</span>.</span>
<span id="cb67-1515"><a href="#cb67-1515"></a><span class="ss">    2. </span>**Copy 2 (Assumed Good):** Assign <span class="in">`default_flag = 0`</span> and <span class="in">`sample_weight = 1 - PD`</span>.</span>
<span id="cb67-1516"><a href="#cb67-1516"></a><span class="ss">4. </span>Combine the original accepted data (with <span class="in">`sample_weight = 1`</span>) and the two weighted sets of rejected data to create the augmented "Through-the-Door" (TTD) dataset. This approach assigns fractional counts to both default and non-default outcomes for each rejected applicant based on their predicted risk.</span>
<span id="cb67-1517"><a href="#cb67-1517"></a></span>
<span id="cb67-1518"><a href="#cb67-1518"></a></span>
<span id="cb67-1519"><a href="#cb67-1519"></a>**Goal:** Implement fuzzy augmentation to create the TTD training dataset.</span>
<span id="cb67-1520"><a href="#cb67-1520"></a></span>
<span id="cb67-1521"><a href="#cb67-1521"></a><span class="fu">## Train Initial Model for Weighting</span></span>
<span id="cb67-1522"><a href="#cb67-1522"></a></span>
<span id="cb67-1523"><a href="#cb67-1523"></a>Train a Logistic Regression model on the *accepted training data* (<span class="in">`df_accepted_train_common`</span>) using a subset of the common features. This model estimates the PD needed for weighting.</span>
<span id="cb67-1524"><a href="#cb67-1524"></a></span>
<span id="cb67-1527"><a href="#cb67-1527"></a><span class="in">```{python}</span></span>
<span id="cb67-1528"><a href="#cb67-1528"></a><span class="co">#| label: ri-train-logit</span></span>
<span id="cb67-1529"><a href="#cb67-1529"></a></span>
<span id="cb67-1530"><a href="#cb67-1530"></a></span>
<span id="cb67-1531"><a href="#cb67-1531"></a><span class="co"># Using only features selected for the main model for consistency in RI</span></span>
<span id="cb67-1532"><a href="#cb67-1532"></a>ri_features <span class="op">=</span> modeling_features </span>
<span id="cb67-1533"><a href="#cb67-1533"></a></span>
<span id="cb67-1534"><a href="#cb67-1534"></a><span class="bu">print</span>(<span class="ss">f"Using RI features (same as modeling features): </span><span class="sc">{</span>ri_features<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-1535"><a href="#cb67-1535"></a></span>
<span id="cb67-1536"><a href="#cb67-1536"></a><span class="co"># Prepare data for Logistic Regression - use .copy() to avoid SettingWithCopyWarning</span></span>
<span id="cb67-1537"><a href="#cb67-1537"></a>X_acc_ri <span class="op">=</span> df_accepted_train_common[ri_features].copy()</span>
<span id="cb67-1538"><a href="#cb67-1538"></a>y_acc_ri <span class="op">=</span> df_accepted_train_common[<span class="st">'default_flag'</span>] </span>
<span id="cb67-1539"><a href="#cb67-1539"></a></span>
<span id="cb67-1540"><a href="#cb67-1540"></a><span class="co"># Basic Preprocessing Pipeline for Logistic Regression</span></span>
<span id="cb67-1541"><a href="#cb67-1541"></a><span class="co"># All ri_features are numeric now</span></span>
<span id="cb67-1542"><a href="#cb67-1542"></a>numeric_features_ri <span class="op">=</span> ri_features <span class="co"># Use the selected features</span></span>
<span id="cb67-1543"><a href="#cb67-1543"></a></span>
<span id="cb67-1544"><a href="#cb67-1544"></a><span class="co"># Define preprocessing steps</span></span>
<span id="cb67-1545"><a href="#cb67-1545"></a>numeric_transformer_ri <span class="op">=</span> Pipeline(steps<span class="op">=</span>[</span>
<span id="cb67-1546"><a href="#cb67-1546"></a>    (<span class="st">'scaler'</span>, StandardScaler())</span>
<span id="cb67-1547"><a href="#cb67-1547"></a>])</span>
<span id="cb67-1548"><a href="#cb67-1548"></a></span>
<span id="cb67-1549"><a href="#cb67-1549"></a><span class="co"># Create the preprocessor - Apply the numeric transformer to the numeric features</span></span>
<span id="cb67-1550"><a href="#cb67-1550"></a>preprocessor_ri <span class="op">=</span> ColumnTransformer(</span>
<span id="cb67-1551"><a href="#cb67-1551"></a>    transformers<span class="op">=</span>[</span>
<span id="cb67-1552"><a href="#cb67-1552"></a>        (<span class="st">'num'</span>, numeric_transformer_ri, numeric_features_ri)</span>
<span id="cb67-1553"><a href="#cb67-1553"></a>    ],</span>
<span id="cb67-1554"><a href="#cb67-1554"></a>    remainder<span class="op">=</span><span class="st">'passthrough'</span> <span class="co"># Keep other columns if any (though there shouldn't be here)</span></span>
<span id="cb67-1555"><a href="#cb67-1555"></a>)</span>
<span id="cb67-1556"><a href="#cb67-1556"></a></span>
<span id="cb67-1557"><a href="#cb67-1557"></a></span>
<span id="cb67-1558"><a href="#cb67-1558"></a><span class="co"># Define LogisticRegressionCV with cross-validation parameters</span></span>
<span id="cb67-1559"><a href="#cb67-1559"></a>logreg_cv <span class="op">=</span> LogisticRegressionCV(</span>
<span id="cb67-1560"><a href="#cb67-1560"></a>    Cs<span class="op">=</span><span class="dv">10</span>,  <span class="co"># Try 10 C values on a logarithmic scale</span></span>
<span id="cb67-1561"><a href="#cb67-1561"></a>    cv<span class="op">=</span><span class="dv">5</span>,   <span class="co"># Use 5-fold cross-validation</span></span>
<span id="cb67-1562"><a href="#cb67-1562"></a>    penalty<span class="op">=</span><span class="st">'l2'</span>, <span class="co"># Use L2 regularization</span></span>
<span id="cb67-1563"><a href="#cb67-1563"></a>    scoring<span class="op">=</span><span class="st">'roc_auc'</span>, <span class="co"># Optimize for AUC</span></span>
<span id="cb67-1564"><a href="#cb67-1564"></a>    random_state<span class="op">=</span><span class="dv">2025</span>,</span>
<span id="cb67-1565"><a href="#cb67-1565"></a>    max_iter<span class="op">=</span><span class="dv">1000</span>, <span class="co"># Increase max iterations for convergence</span></span>
<span id="cb67-1566"><a href="#cb67-1566"></a>    solver<span class="op">=</span><span class="st">'liblinear'</span> <span class="co"># Suitable solver for this problem size and penalty</span></span>
<span id="cb67-1567"><a href="#cb67-1567"></a>)</span>
<span id="cb67-1568"><a href="#cb67-1568"></a></span>
<span id="cb67-1569"><a href="#cb67-1569"></a><span class="co"># Create the full pipeline with Logistic Regression CV</span></span>
<span id="cb67-1570"><a href="#cb67-1570"></a>ri_model <span class="op">=</span> Pipeline(steps<span class="op">=</span>[(<span class="st">'preprocessor'</span>, preprocessor_ri),</span>
<span id="cb67-1571"><a href="#cb67-1571"></a>                (<span class="st">'classifier'</span>, logreg_cv)]) <span class="co"># Use LogisticRegressionCV</span></span>
<span id="cb67-1572"><a href="#cb67-1572"></a></span>
<span id="cb67-1573"><a href="#cb67-1573"></a>global_set_seed(random_seed) <span class="co"># Set global seed</span></span>
<span id="cb67-1574"><a href="#cb67-1574"></a></span>
<span id="cb67-1575"><a href="#cb67-1575"></a><span class="bu">print</span>(<span class="st">"Training Reject Inference Logistic Regression model..."</span>)</span>
<span id="cb67-1576"><a href="#cb67-1576"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb67-1577"><a href="#cb67-1577"></a>ri_model.fit(X_acc_ri, y_acc_ri)</span>
<span id="cb67-1578"><a href="#cb67-1578"></a>end_time <span class="op">=</span> time.time()</span>
<span id="cb67-1579"><a href="#cb67-1579"></a><span class="bu">print</span>(<span class="ss">f"RI model training completed in </span><span class="sc">{</span>end_time <span class="op">-</span> start_time<span class="sc">:.2f}</span><span class="ss"> seconds."</span>)</span>
<span id="cb67-1580"><a href="#cb67-1580"></a></span>
<span id="cb67-1581"><a href="#cb67-1581"></a></span>
<span id="cb67-1582"><a href="#cb67-1582"></a><span class="in">```</span></span>
<span id="cb67-1583"><a href="#cb67-1583"></a></span>
<span id="cb67-1584"><a href="#cb67-1584"></a><span class="fu">## Check RI Model Coefficients</span></span>
<span id="cb67-1585"><a href="#cb67-1585"></a></span>
<span id="cb67-1586"><a href="#cb67-1586"></a>**Goal:** After training the RI model, we need to check the coefficients to ensure they align with our expectations based on the monotonicity checks. This helps validate that the model is capturing the expected relationships between features and default risk.</span>
<span id="cb67-1587"><a href="#cb67-1587"></a></span>
<span id="cb67-1588"><a href="#cb67-1588"></a>Check the <span class="in">`ri_model`</span> (also known as the KGB model) to ensure the coefficients are reasonable.</span>
<span id="cb67-1589"><a href="#cb67-1589"></a></span>
<span id="cb67-1592"><a href="#cb67-1592"></a><span class="in">```{python}</span></span>
<span id="cb67-1593"><a href="#cb67-1593"></a><span class="co">#| label: tbl-ri-model-coefficients</span></span>
<span id="cb67-1594"><a href="#cb67-1594"></a><span class="co">#| tbl-cap: "Reject Inference Model Coefficients"</span></span>
<span id="cb67-1595"><a href="#cb67-1595"></a></span>
<span id="cb67-1596"><a href="#cb67-1596"></a></span>
<span id="cb67-1597"><a href="#cb67-1597"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">--- RI Model Coefficients ---"</span>)</span>
<span id="cb67-1598"><a href="#cb67-1598"></a><span class="co"># Extract coefficients and intercept from the Logistic Regression model</span></span>
<span id="cb67-1599"><a href="#cb67-1599"></a><span class="cf">if</span> <span class="bu">hasattr</span>(ri_model.named_steps[<span class="st">'classifier'</span>], <span class="st">'coef_'</span>) <span class="kw">and</span> <span class="op">\</span></span>
<span id="cb67-1600"><a href="#cb67-1600"></a>    <span class="bu">hasattr</span>(ri_model.named_steps[<span class="st">'classifier'</span>], <span class="st">'intercept_'</span>):</span>
<span id="cb67-1601"><a href="#cb67-1601"></a>    </span>
<span id="cb67-1602"><a href="#cb67-1602"></a>    coeffs <span class="op">=</span> ri_model.named_steps[<span class="st">'classifier'</span>].coef_[<span class="dv">0</span>] <span class="co"># Get the coefficients</span></span>
<span id="cb67-1603"><a href="#cb67-1603"></a>    intercept <span class="op">=</span> ri_model.named_steps[<span class="st">'classifier'</span>].intercept_[<span class="dv">0</span>] <span class="co"># Get the intercept</span></span>
<span id="cb67-1604"><a href="#cb67-1604"></a>    </span>
<span id="cb67-1605"><a href="#cb67-1605"></a>    <span class="co"># Get feature names from the preprocessor step if possible</span></span>
<span id="cb67-1606"><a href="#cb67-1606"></a>    <span class="cf">try</span>:</span>
<span id="cb67-1607"><a href="#cb67-1607"></a>        <span class="co"># Access the fitted ColumnTransformer to get output feature names</span></span>
<span id="cb67-1608"><a href="#cb67-1608"></a>        preprocessor <span class="op">=</span> ri_model.named_steps[<span class="st">'preprocessor'</span>]</span>
<span id="cb67-1609"><a href="#cb67-1609"></a>        <span class="co"># Get feature names after transformation (e.g., scaled numeric features)</span></span>
<span id="cb67-1610"><a href="#cb67-1610"></a>        <span class="co"># Note: This relies on the structure of the preprocessor</span></span>
<span id="cb67-1611"><a href="#cb67-1611"></a>        feature_names <span class="op">=</span> preprocessor.get_feature_names_out()</span>
<span id="cb67-1612"><a href="#cb67-1612"></a>    <span class="cf">except</span> <span class="pp">Exception</span>:</span>
<span id="cb67-1613"><a href="#cb67-1613"></a>        <span class="co"># Fallback to original feature names if getting transformed names fails</span></span>
<span id="cb67-1614"><a href="#cb67-1614"></a>        <span class="bu">print</span>(<span class="st">"Warning: Could not get transformed feature names. Using original RI feature names."</span>)</span>
<span id="cb67-1615"><a href="#cb67-1615"></a>        feature_names <span class="op">=</span> ri_features <span class="co"># Use the original input feature names</span></span>
<span id="cb67-1616"><a href="#cb67-1616"></a></span>
<span id="cb67-1617"><a href="#cb67-1617"></a>    <span class="co"># Create DataFrame for coefficients</span></span>
<span id="cb67-1618"><a href="#cb67-1618"></a>    coeff_df <span class="op">=</span> pd.DataFrame({<span class="st">'Feature'</span>: feature_names, <span class="st">'Coefficient'</span>: coeffs})</span>
<span id="cb67-1619"><a href="#cb67-1619"></a>    </span>
<span id="cb67-1620"><a href="#cb67-1620"></a>    <span class="co"># Add the intercept as a separate row</span></span>
<span id="cb67-1621"><a href="#cb67-1621"></a>    intercept_df <span class="op">=</span> pd.DataFrame({<span class="st">'Feature'</span>: [<span class="st">'Intercept'</span>], <span class="st">'Coefficient'</span>: [intercept]})</span>
<span id="cb67-1622"><a href="#cb67-1622"></a>    </span>
<span id="cb67-1623"><a href="#cb67-1623"></a>    <span class="co"># Combine coefficients and intercept</span></span>
<span id="cb67-1624"><a href="#cb67-1624"></a>    full_coeff_df <span class="op">=</span> pd.concat([intercept_df, coeff_df], ignore_index<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb67-1625"><a href="#cb67-1625"></a>    </span>
<span id="cb67-1626"><a href="#cb67-1626"></a>    <span class="co"># Sort by absolute coefficient value might be more informative, but sorting by value is fine</span></span>
<span id="cb67-1627"><a href="#cb67-1627"></a>    <span class="co"># full_coeff_df.sort_values(by='Coefficient', ascending=False, inplace=True) </span></span>
<span id="cb67-1628"><a href="#cb67-1628"></a>    </span>
<span id="cb67-1629"><a href="#cb67-1629"></a>    display(full_coeff_df)</span>
<span id="cb67-1630"><a href="#cb67-1630"></a>        </span>
<span id="cb67-1631"><a href="#cb67-1631"></a></span>
<span id="cb67-1632"><a href="#cb67-1632"></a></span>
<span id="cb67-1633"><a href="#cb67-1633"></a><span class="in">```</span></span>
<span id="cb67-1634"><a href="#cb67-1634"></a></span>
<span id="cb67-1635"><a href="#cb67-1635"></a>**Interpretation:** The coefficients of the RI model should align with our expectations based on the monotonicity checks. For example, we expect a positive coefficient for <span class="in">`dti`</span> (higher DTI leads to higher default risk) and a negative coefficient for <span class="in">`credit_score`</span> (higher credit score leads to lower default risk).</span>
<span id="cb67-1636"><a href="#cb67-1636"></a></span>
<span id="cb67-1637"><a href="#cb67-1637"></a><span class="fu">## Calculate Weights and Create TTD Dataset</span></span>
<span id="cb67-1638"><a href="#cb67-1638"></a></span>
<span id="cb67-1639"><a href="#cb67-1639"></a>Apply the trained <span class="in">`ri_model`</span> to the *rejected training data* (<span class="in">`df_rejected_train_common`</span>), calculate fuzzy weights, assign <span class="in">`default_flag=1`</span>, and combine with <span class="in">`df_accepted_train_common`</span> to create <span class="in">`df_ttd_train`</span>.</span>
<span id="cb67-1640"><a href="#cb67-1640"></a></span>
<span id="cb67-1643"><a href="#cb67-1643"></a><span class="in">```{python}</span></span>
<span id="cb67-1644"><a href="#cb67-1644"></a><span class="co">#| label: tbl-ri-apply-weights</span></span>
<span id="cb67-1645"><a href="#cb67-1645"></a><span class="co">#| tbl-cap: "Weighted default rates for TTD Train Data"</span></span>
<span id="cb67-1646"><a href="#cb67-1646"></a></span>
<span id="cb67-1647"><a href="#cb67-1647"></a><span class="bu">print</span>(<span class="st">"Applying RI model to rejected training data and calculating weights..."</span>)</span>
<span id="cb67-1648"><a href="#cb67-1648"></a></span>
<span id="cb67-1649"><a href="#cb67-1649"></a><span class="co"># Create df_ttd_train with the source column</span></span>
<span id="cb67-1650"><a href="#cb67-1650"></a>df_ttd_train <span class="op">=</span> create_TTD_data( <span class="co"># Renamed variable</span></span>
<span id="cb67-1651"><a href="#cb67-1651"></a>    ri_model<span class="op">=</span>ri_model,</span>
<span id="cb67-1652"><a href="#cb67-1652"></a>    df_rejected<span class="op">=</span>df_rejected_train_common,</span>
<span id="cb67-1653"><a href="#cb67-1653"></a>    df_accepted<span class="op">=</span>df_accepted_train_common,</span>
<span id="cb67-1654"><a href="#cb67-1654"></a>    ri_features<span class="op">=</span>ri_features,</span>
<span id="cb67-1655"><a href="#cb67-1655"></a>    modeling_features<span class="op">=</span>modeling_features,</span>
<span id="cb67-1656"><a href="#cb67-1656"></a>    target_col<span class="op">=</span>target_col</span>
<span id="cb67-1657"><a href="#cb67-1657"></a>)</span>
<span id="cb67-1658"><a href="#cb67-1658"></a></span>
<span id="cb67-1659"><a href="#cb67-1659"></a><span class="co"># Calculate and display summary statistics by source using helper function</span></span>
<span id="cb67-1660"><a href="#cb67-1660"></a>summary_default_rates <span class="op">=</span> summarize_ttd_by_source(</span>
<span id="cb67-1661"><a href="#cb67-1661"></a>    df_ttd<span class="op">=</span>df_ttd_train,</span>
<span id="cb67-1662"><a href="#cb67-1662"></a>    target_col<span class="op">=</span>target_col,</span>
<span id="cb67-1663"><a href="#cb67-1663"></a>    weight_col<span class="op">=</span><span class="st">'sample_weight'</span>,</span>
<span id="cb67-1664"><a href="#cb67-1664"></a>    source_col<span class="op">=</span><span class="st">'source'</span></span>
<span id="cb67-1665"><a href="#cb67-1665"></a>)</span>
<span id="cb67-1666"><a href="#cb67-1666"></a></span>
<span id="cb67-1667"><a href="#cb67-1667"></a><span class="in">```</span></span>
<span id="cb67-1668"><a href="#cb67-1668"></a></span>
<span id="cb67-1669"><a href="#cb67-1669"></a>For rejected applicants in the TTD dataset, the <span class="in">`default_flag`</span> and <span class="in">`sample_weight`</span> are not observed outcomes. Instead, they are assigned based on the RI model's predicted probability of default (PD). Each rejected applicant is represented twice: once as an assumed default (<span class="in">`default_flag=1`</span>, weighted by PD) and once as an assumed non-default (<span class="in">`default_flag=0`</span>, weighted by 1-PD). This approach reflects the model's belief about their likely outcome, rather than an actual observed default status.</span>
<span id="cb67-1670"><a href="#cb67-1670"></a></span>
<span id="cb67-1673"><a href="#cb67-1673"></a><span class="in">```{python}</span></span>
<span id="cb67-1674"><a href="#cb67-1674"></a><span class="co">#| label: tbl-ttd-sample</span></span>
<span id="cb67-1675"><a href="#cb67-1675"></a><span class="co">#| tbl-cap: "Sample of Augmented TTD Training Data (Fuzzy Augmentation)"</span></span>
<span id="cb67-1676"><a href="#cb67-1676"></a></span>
<span id="cb67-1677"><a href="#cb67-1677"></a></span>
<span id="cb67-1678"><a href="#cb67-1678"></a>df_sample <span class="op">=</span> df_ttd_train.sample(<span class="dv">10</span>, random_state<span class="op">=</span><span class="dv">2025</span>)</span>
<span id="cb67-1679"><a href="#cb67-1679"></a></span>
<span id="cb67-1680"><a href="#cb67-1680"></a>display(df_sample)</span>
<span id="cb67-1681"><a href="#cb67-1681"></a><span class="in">```</span></span>
<span id="cb67-1682"><a href="#cb67-1682"></a></span>
<span id="cb67-1683"><a href="#cb67-1683"></a><span class="fu"># Building the TTD Scorecard Model (Initial)</span></span>
<span id="cb67-1684"><a href="#cb67-1684"></a></span>
<span id="cb67-1685"><a href="#cb67-1685"></a>**Goal:** Train a preliminary scorecard model using AutoGluon on the augmented TTD training dataset (<span class="in">`df_ttd_train`</span>). This model will incorporate the <span class="in">`sample_weight`</span> calculated during reject inference but will *not* yet have monotonic constraints applied.</span>
<span id="cb67-1686"><a href="#cb67-1686"></a></span>
<span id="cb67-1687"><a href="#cb67-1687"></a><span class="fu">## Configure and Train AutoGluon Model</span></span>
<span id="cb67-1688"><a href="#cb67-1688"></a></span>
<span id="cb67-1689"><a href="#cb67-1689"></a>Set up AutoGluon to train on the TTD data, specifying the label (<span class="in">`default_flag`</span>), sample weight column, and excluding complex models like Neural Networks to favor interpretability.</span>
<span id="cb67-1690"><a href="#cb67-1690"></a></span>
<span id="cb67-1691"><a href="#cb67-1691"></a>Sample weights are used only during the fit() process to influence how the model learns from the training data. Once the model is trained, predictions are made based solely on the input features the model learned from. You only need to provide the feature columns in the DataFrame passed to <span class="in">`predict()`</span> or <span class="in">`predict_proba()`</span>.</span>
<span id="cb67-1692"><a href="#cb67-1692"></a></span>
<span id="cb67-1695"><a href="#cb67-1695"></a><span class="in">```{python}</span></span>
<span id="cb67-1696"><a href="#cb67-1696"></a><span class="co">#| label: tbl-ttd-modeling-initial</span></span>
<span id="cb67-1697"><a href="#cb67-1697"></a><span class="co">#| tbl-cap: "Leaderboard for initial TTD Model"</span></span>
<span id="cb67-1698"><a href="#cb67-1698"></a></span>
<span id="cb67-1699"><a href="#cb67-1699"></a>label <span class="op">=</span> <span class="st">'default_flag'</span></span>
<span id="cb67-1700"><a href="#cb67-1700"></a>weight_col <span class="op">=</span> <span class="st">'sample_weight'</span></span>
<span id="cb67-1701"><a href="#cb67-1701"></a>        </span>
<span id="cb67-1702"><a href="#cb67-1702"></a><span class="co"># --- AutoGluon Configuration ---</span></span>
<span id="cb67-1703"><a href="#cb67-1703"></a>model_folder_ttd_initial <span class="op">=</span> <span class="st">'Lab02_ag_models_TTD_Initial'</span></span>
<span id="cb67-1704"><a href="#cb67-1704"></a></span>
<span id="cb67-1705"><a href="#cb67-1705"></a><span class="co"># Hyperparameters: Limit tree depth for simplicity and interpretability</span></span>
<span id="cb67-1706"><a href="#cb67-1706"></a>custom_hyperparameters <span class="op">=</span> {</span>
<span id="cb67-1707"><a href="#cb67-1707"></a>    <span class="st">'GBM'</span>: {<span class="st">'num_boost_round'</span>: <span class="dv">10000</span>, <span class="st">'num_leaves'</span>: <span class="dv">4</span>},</span>
<span id="cb67-1708"><a href="#cb67-1708"></a>    <span class="st">'CAT'</span>: {<span class="st">'iterations'</span>: <span class="dv">10000</span>, <span class="st">'depth'</span>: <span class="dv">2</span>}</span>
<span id="cb67-1709"><a href="#cb67-1709"></a>}</span>
<span id="cb67-1710"><a href="#cb67-1710"></a>excluded_model_types <span class="op">=</span> [<span class="st">'NN_TORCH'</span>, <span class="st">'FASTAI'</span>, <span class="st">'KNN'</span>] </span>
<span id="cb67-1711"><a href="#cb67-1711"></a></span>
<span id="cb67-1712"><a href="#cb67-1712"></a><span class="co"># Arguments for TabularPredictor initialization (passed via wrapper)</span></span>
<span id="cb67-1713"><a href="#cb67-1713"></a>predictor_args <span class="op">=</span> {</span>
<span id="cb67-1714"><a href="#cb67-1714"></a>    <span class="st">'problem_type'</span>: <span class="st">'binary'</span>,</span>
<span id="cb67-1715"><a href="#cb67-1715"></a>    <span class="st">'eval_metric'</span>: <span class="st">'roc_auc'</span>, </span>
<span id="cb67-1716"><a href="#cb67-1716"></a>    <span class="st">'path'</span>: model_folder_ttd_initial,</span>
<span id="cb67-1717"><a href="#cb67-1717"></a>    <span class="st">'sample_weight'</span>: weight_col         <span class="co"># key parameter for Fuzz Augmentation</span></span>
<span id="cb67-1718"><a href="#cb67-1718"></a>}</span>
<span id="cb67-1719"><a href="#cb67-1719"></a></span>
<span id="cb67-1720"><a href="#cb67-1720"></a><span class="co"># Arguments for TabularPredictor.fit (passed via wrapper)</span></span>
<span id="cb67-1721"><a href="#cb67-1721"></a>fit_args <span class="op">=</span> {</span>
<span id="cb67-1722"><a href="#cb67-1722"></a>    <span class="st">'presets'</span>: {</span>
<span id="cb67-1723"><a href="#cb67-1723"></a>        <span class="st">'holdout_frac'</span>: <span class="fl">0.2</span>,</span>
<span id="cb67-1724"><a href="#cb67-1724"></a>        <span class="st">'excluded_model_types'</span>: excluded_model_types,</span>
<span id="cb67-1725"><a href="#cb67-1725"></a>        <span class="st">'hyperparameters'</span>: custom_hyperparameters, </span>
<span id="cb67-1726"><a href="#cb67-1726"></a>        <span class="st">'time_limit'</span>: <span class="dv">300</span></span>
<span id="cb67-1727"><a href="#cb67-1727"></a>    }</span>
<span id="cb67-1728"><a href="#cb67-1728"></a>}</span>
<span id="cb67-1729"><a href="#cb67-1729"></a></span>
<span id="cb67-1730"><a href="#cb67-1730"></a>global_set_seed(random_seed) <span class="co"># Set global seed</span></span>
<span id="cb67-1731"><a href="#cb67-1731"></a></span>
<span id="cb67-1732"><a href="#cb67-1732"></a><span class="co"># Train model using the helper function</span></span>
<span id="cb67-1733"><a href="#cb67-1733"></a>ag_model_initial_wrapped <span class="op">=</span> train_autogluon_model(</span>
<span id="cb67-1734"><a href="#cb67-1734"></a>    df_train<span class="op">=</span>df_ttd_train,</span>
<span id="cb67-1735"><a href="#cb67-1735"></a>    label<span class="op">=</span>label,</span>
<span id="cb67-1736"><a href="#cb67-1736"></a>    weight_col<span class="op">=</span>weight_col,</span>
<span id="cb67-1737"><a href="#cb67-1737"></a>    modeling_features<span class="op">=</span>modeling_features, <span class="co"># Use derived features list</span></span>
<span id="cb67-1738"><a href="#cb67-1738"></a>    model_folder<span class="op">=</span>model_folder_ttd_initial,</span>
<span id="cb67-1739"><a href="#cb67-1739"></a>    predictor_args<span class="op">=</span>predictor_args,</span>
<span id="cb67-1740"><a href="#cb67-1740"></a>    fit_args<span class="op">=</span>fit_args</span>
<span id="cb67-1741"><a href="#cb67-1741"></a>)</span>
<span id="cb67-1742"><a href="#cb67-1742"></a></span>
<span id="cb67-1743"><a href="#cb67-1743"></a><span class="co"># Access the underlying predictor for leaderboard etc. (optional, as helper prints it)</span></span>
<span id="cb67-1744"><a href="#cb67-1744"></a>ag_predictor_initial <span class="op">=</span> ag_model_initial_wrapped.predictor</span>
<span id="cb67-1745"><a href="#cb67-1745"></a></span>
<span id="cb67-1746"><a href="#cb67-1746"></a></span>
<span id="cb67-1747"><a href="#cb67-1747"></a><span class="in">```</span></span>
<span id="cb67-1748"><a href="#cb67-1748"></a></span>
<span id="cb67-1749"><a href="#cb67-1749"></a><span class="fu"># Initial Model Diagnostics (PDP/ICE)</span></span>
<span id="cb67-1750"><a href="#cb67-1750"></a></span>
<span id="cb67-1751"><a href="#cb67-1751"></a>**Goal:** Analyze the behavior of the *initial, unconstrained* TTD model. Use Partial Dependence Plots (PDP) and Individual Conditional Expectation (ICE) plots to understand how the model's predictions change on average (PDP) and for individual instances (ICE) as key feature values vary. This helps identify if the model learned relationships that contradict business logic (e.g., non-monotonic trends).</span>
<span id="cb67-1752"><a href="#cb67-1752"></a></span>
<span id="cb67-1753"><a href="#cb67-1753"></a></span>
<span id="cb67-1756"><a href="#cb67-1756"></a><span class="in">```{python}</span></span>
<span id="cb67-1757"><a href="#cb67-1757"></a><span class="co">#| label: fig-pdp-ice-initial</span></span>
<span id="cb67-1758"><a href="#cb67-1758"></a><span class="co">#| fig-cap: "PDP/ICE plots for the initial TTD model"</span></span>
<span id="cb67-1759"><a href="#cb67-1759"></a></span>
<span id="cb67-1760"><a href="#cb67-1760"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb67-1761"><a href="#cb67-1761"></a></span>
<span id="cb67-1762"><a href="#cb67-1762"></a><span class="co"># Get all features from ag_model.predictor</span></span>
<span id="cb67-1763"><a href="#cb67-1763"></a>all_features <span class="op">=</span> ag_model_initial_wrapped.predictor.features()</span>
<span id="cb67-1764"><a href="#cb67-1764"></a></span>
<span id="cb67-1765"><a href="#cb67-1765"></a><span class="co"># Get categorical features from ag_model.predictor's feature metadata</span></span>
<span id="cb67-1766"><a href="#cb67-1766"></a>categorical_features <span class="op">=</span> ag_model_initial_wrapped.predictor.feature_metadata.get_features(valid_raw_types<span class="op">=</span>[<span class="st">'category'</span>])</span>
<span id="cb67-1767"><a href="#cb67-1767"></a></span>
<span id="cb67-1768"><a href="#cb67-1768"></a><span class="co"># Get numeric features from ag_model.predictor's feature metadata</span></span>
<span id="cb67-1769"><a href="#cb67-1769"></a>numeric_features <span class="op">=</span> ag_model_initial_wrapped.predictor.feature_metadata.get_features(valid_raw_types<span class="op">=</span>[<span class="st">'int'</span>, <span class="st">'float'</span>, <span class="st">'int64'</span>, <span class="st">'float64'</span>, <span class="st">'int32'</span>, <span class="st">'float32'</span>])</span>
<span id="cb67-1770"><a href="#cb67-1770"></a></span>
<span id="cb67-1771"><a href="#cb67-1771"></a></span>
<span id="cb67-1772"><a href="#cb67-1772"></a>show_pdp(wrappedAGModel <span class="op">=</span> ag_model_initial_wrapped,</span>
<span id="cb67-1773"><a href="#cb67-1773"></a>        list_features <span class="op">=</span> all_features, </span>
<span id="cb67-1774"><a href="#cb67-1774"></a>        list_categ_features <span class="op">=</span> categorical_features <span class="cf">if</span> categorical_features <span class="cf">else</span> <span class="va">None</span>,</span>
<span id="cb67-1775"><a href="#cb67-1775"></a>        df <span class="op">=</span> df_ttd_train.drop(columns<span class="op">=</span>[label, weight_col]),</span>
<span id="cb67-1776"><a href="#cb67-1776"></a>        show_ice<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb67-1777"><a href="#cb67-1777"></a>        sampSize<span class="op">=</span><span class="dv">25_000</span></span>
<span id="cb67-1778"><a href="#cb67-1778"></a>        )</span>
<span id="cb67-1779"><a href="#cb67-1779"></a><span class="in">```</span></span>
<span id="cb67-1780"><a href="#cb67-1780"></a></span>
<span id="cb67-1781"><a href="#cb67-1781"></a>**Interpretation:** </span>
<span id="cb67-1782"><a href="#cb67-1782"></a></span>
<span id="cb67-1783"><a href="#cb67-1783"></a>Examine the PDP (red dashed line) and ICE (thin blue lines) for each feature:</span>
<span id="cb67-1784"><a href="#cb67-1784"></a></span>
<span id="cb67-1785"><a href="#cb67-1785"></a><span class="ss">- </span>Does <span class="in">`loan_amnt`</span> consistently increase the predicted probability?</span>
<span id="cb67-1786"><a href="#cb67-1786"></a><span class="ss">- </span>Does <span class="in">`dti`</span> consistently increase the predicted probability?</span>
<span id="cb67-1787"><a href="#cb67-1787"></a><span class="ss">- </span>Does <span class="in">`credit_score`</span> consistently decrease the predicted probability?</span>
<span id="cb67-1788"><a href="#cb67-1788"></a></span>
<span id="cb67-1789"><a href="#cb67-1789"></a>If any of these plots show non-monotonic behavior (e.g., the average trend goes up then down, or vice-versa), it violates our business intuition and suggests that applying monotonic constraints is necessary. The ICE lines show if this behavior is consistent across all samples or if there's significant heterogeneity.</span>
<span id="cb67-1790"><a href="#cb67-1790"></a></span>
<span id="cb67-1791"><a href="#cb67-1791"></a><span class="fu"># Applying Monotonic Constraints</span></span>
<span id="cb67-1792"><a href="#cb67-1792"></a></span>
<span id="cb67-1793"><a href="#cb67-1793"></a>**Concept:** Monotonic constraints force the model to learn relationships that align with business expectations. We specify whether a feature should have a non-decreasing (+) or non-increasing (-) relationship with the target probability. AutoGluon passes these constraints to underlying models that support them (like LightGBM, XGBoost).</span>
<span id="cb67-1794"><a href="#cb67-1794"></a></span>
<span id="cb67-1795"><a href="#cb67-1795"></a>**Goal:** Define monotonic constraints based on business logic (e.g., <span class="in">`dti`</span> increases risk, <span class="in">`credit_score`</span> decreases risk) and retrain the AutoGluon model on the TTD data with these constraints enforced.</span>
<span id="cb67-1796"><a href="#cb67-1796"></a></span>
<span id="cb67-1797"><a href="#cb67-1797"></a><span class="fu">## Define Constraints and Retrain Model</span></span>
<span id="cb67-1798"><a href="#cb67-1798"></a></span>
<span id="cb67-1799"><a href="#cb67-1799"></a>Specify the desired monotonic relationship for key features using AutoGluon's <span class="in">`feature_metadata`</span> and retrain the model.</span>
<span id="cb67-1800"><a href="#cb67-1800"></a></span>
<span id="cb67-1803"><a href="#cb67-1803"></a><span class="in">```{python}</span></span>
<span id="cb67-1804"><a href="#cb67-1804"></a><span class="co">#| label: tbl-monotonic-constraints-setup-train</span></span>
<span id="cb67-1805"><a href="#cb67-1805"></a><span class="co">#| tbl-cap: "Leaderboard for constrained TTD Model"</span></span>
<span id="cb67-1806"><a href="#cb67-1806"></a></span>
<span id="cb67-1807"><a href="#cb67-1807"></a></span>
<span id="cb67-1808"><a href="#cb67-1808"></a><span class="bu">print</span>(<span class="st">"Training constrained AutoGluon model with monotonic constraints via hyperparameters..."</span>)</span>
<span id="cb67-1809"><a href="#cb67-1809"></a><span class="co"># Define monotonic constraints for boosting models</span></span>
<span id="cb67-1810"><a href="#cb67-1810"></a></span>
<span id="cb67-1811"><a href="#cb67-1811"></a></span>
<span id="cb67-1812"><a href="#cb67-1812"></a>monotone_constraints_dict <span class="op">=</span> {<span class="st">'loan_amnt'</span>: <span class="dv">1</span>, <span class="st">'dti'</span>: <span class="dv">1</span>, <span class="st">'credit_score'</span>: <span class="op">-</span><span class="dv">1</span>, <span class="st">'emp_length'</span>: <span class="op">-</span><span class="dv">1</span>}</span>
<span id="cb67-1813"><a href="#cb67-1813"></a></span>
<span id="cb67-1814"><a href="#cb67-1814"></a>monotone_constraints_ordered <span class="op">=</span> [monotone_constraints_dict.get(f, <span class="dv">0</span>) <span class="cf">for</span> f <span class="kw">in</span> all_features] <span class="co"># Default to 0 if feature not in dict</span></span>
<span id="cb67-1815"><a href="#cb67-1815"></a></span>
<span id="cb67-1816"><a href="#cb67-1816"></a><span class="co"># Update hyperparameters to include monotonic constraints</span></span>
<span id="cb67-1817"><a href="#cb67-1817"></a>custom_hyperparameters_constrained <span class="op">=</span> {</span>
<span id="cb67-1818"><a href="#cb67-1818"></a>    <span class="st">'GBM'</span>: {<span class="op">**</span>custom_hyperparameters[<span class="st">'GBM'</span>], <span class="st">'monotone_constraints'</span>: monotone_constraints_ordered},</span>
<span id="cb67-1819"><a href="#cb67-1819"></a>    <span class="st">'CAT'</span>: {<span class="op">**</span>custom_hyperparameters[<span class="st">'CAT'</span>], <span class="st">'monotone_constraints'</span>: monotone_constraints_dict} <span class="co"># CatBoost uses dict</span></span>
<span id="cb67-1820"><a href="#cb67-1820"></a>}</span>
<span id="cb67-1821"><a href="#cb67-1821"></a></span>
<span id="cb67-1822"><a href="#cb67-1822"></a><span class="co"># Clean up previous model folder</span></span>
<span id="cb67-1823"><a href="#cb67-1823"></a>model_folder_ttd_constrained <span class="op">=</span> <span class="st">'Lab02_ag_models_TTD_Constrained'</span></span>
<span id="cb67-1824"><a href="#cb67-1824"></a></span>
<span id="cb67-1825"><a href="#cb67-1825"></a><span class="co"># Prepare predictor and fit arguments (reuse from initial, update path and hyperparameters)</span></span>
<span id="cb67-1826"><a href="#cb67-1826"></a>predictor_args_constrained <span class="op">=</span> predictor_args.copy()</span>
<span id="cb67-1827"><a href="#cb67-1827"></a>predictor_args_constrained[<span class="st">'path'</span>] <span class="op">=</span> model_folder_ttd_constrained</span>
<span id="cb67-1828"><a href="#cb67-1828"></a></span>
<span id="cb67-1829"><a href="#cb67-1829"></a>fit_args_constrained <span class="op">=</span> {</span>
<span id="cb67-1830"><a href="#cb67-1830"></a>    <span class="st">'presets'</span>: {</span>
<span id="cb67-1831"><a href="#cb67-1831"></a>        <span class="st">'hyperparameters'</span>: custom_hyperparameters_constrained,</span>
<span id="cb67-1832"><a href="#cb67-1832"></a>        <span class="st">'excluded_model_types'</span>: excluded_model_types,</span>
<span id="cb67-1833"><a href="#cb67-1833"></a>        <span class="st">'holdout_frac'</span>: <span class="fl">0.2</span>,</span>
<span id="cb67-1834"><a href="#cb67-1834"></a>        <span class="st">'time_limit'</span>: <span class="dv">300</span></span>
<span id="cb67-1835"><a href="#cb67-1835"></a>    }</span>
<span id="cb67-1836"><a href="#cb67-1836"></a>}</span>
<span id="cb67-1837"><a href="#cb67-1837"></a></span>
<span id="cb67-1838"><a href="#cb67-1838"></a>global_set_seed(random_seed) <span class="co"># Set global seed</span></span>
<span id="cb67-1839"><a href="#cb67-1839"></a></span>
<span id="cb67-1840"><a href="#cb67-1840"></a><span class="co"># Train constrained model using the helper function</span></span>
<span id="cb67-1841"><a href="#cb67-1841"></a>ag_model_constrained_wrapped <span class="op">=</span> train_autogluon_model(</span>
<span id="cb67-1842"><a href="#cb67-1842"></a>    df_train<span class="op">=</span>df_ttd_train,</span>
<span id="cb67-1843"><a href="#cb67-1843"></a>    label<span class="op">=</span>label,</span>
<span id="cb67-1844"><a href="#cb67-1844"></a>    weight_col<span class="op">=</span>weight_col,</span>
<span id="cb67-1845"><a href="#cb67-1845"></a>    modeling_features<span class="op">=</span>modeling_features, <span class="co"># Use features from initial model</span></span>
<span id="cb67-1846"><a href="#cb67-1846"></a>    model_folder<span class="op">=</span>model_folder_ttd_constrained,</span>
<span id="cb67-1847"><a href="#cb67-1847"></a>    predictor_args<span class="op">=</span>predictor_args_constrained,</span>
<span id="cb67-1848"><a href="#cb67-1848"></a>    fit_args<span class="op">=</span>fit_args_constrained</span>
<span id="cb67-1849"><a href="#cb67-1849"></a>)</span>
<span id="cb67-1850"><a href="#cb67-1850"></a></span>
<span id="cb67-1851"><a href="#cb67-1851"></a><span class="co"># Access the underlying predictor (optional, as helper prints leaderboard)</span></span>
<span id="cb67-1852"><a href="#cb67-1852"></a>ag_predictor_constrained <span class="op">=</span> ag_model_constrained_wrapped.predictor</span>
<span id="cb67-1853"><a href="#cb67-1853"></a></span>
<span id="cb67-1854"><a href="#cb67-1854"></a></span>
<span id="cb67-1855"><a href="#cb67-1855"></a><span class="in">```</span></span>
<span id="cb67-1856"><a href="#cb67-1856"></a></span>
<span id="cb67-1857"><a href="#cb67-1857"></a><span class="fu"># Verifying Constraints (PDP/ICE)</span></span>
<span id="cb67-1858"><a href="#cb67-1858"></a></span>
<span id="cb67-1859"><a href="#cb67-1859"></a>**Goal:** Re-run the PDP/ICE plots on the *constrained* model to visually confirm that the specified monotonic relationships for <span class="in">`loan_amnt`</span>, <span class="in">`dti`</span>, and <span class="in">`credit_score`</span> are now enforced.</span>
<span id="cb67-1860"><a href="#cb67-1860"></a></span>
<span id="cb67-1863"><a href="#cb67-1863"></a><span class="in">```{python}</span></span>
<span id="cb67-1864"><a href="#cb67-1864"></a><span class="co">#| label: fig-pdp-ice-constrained</span></span>
<span id="cb67-1865"><a href="#cb67-1865"></a><span class="co">#| fig-cap: "PDP/ICE plots for the constrained TTD model"</span></span>
<span id="cb67-1866"><a href="#cb67-1866"></a></span>
<span id="cb67-1867"><a href="#cb67-1867"></a></span>
<span id="cb67-1868"><a href="#cb67-1868"></a><span class="bu">print</span>(<span class="st">"Generating PDP/ICE plots for the constrained model to verify monotonic behavior..."</span>)</span>
<span id="cb67-1869"><a href="#cb67-1869"></a><span class="co"># Extract feature lists from the constrained predictor</span></span>
<span id="cb67-1870"><a href="#cb67-1870"></a>all_features_constrained <span class="op">=</span> ag_model_constrained_wrapped.predictor.features()</span>
<span id="cb67-1871"><a href="#cb67-1871"></a>categorical_features_constrained <span class="op">=</span> ag_model_constrained_wrapped.predictor.feature_metadata.get_features(valid_raw_types<span class="op">=</span>[<span class="st">'category'</span>])</span>
<span id="cb67-1872"><a href="#cb67-1872"></a><span class="co"># Prepare DataFrame for plotting (drop label and weight)</span></span>
<span id="cb67-1873"><a href="#cb67-1873"></a>plot_df_constrained <span class="op">=</span> df_ttd_train.drop(columns<span class="op">=</span>[label, weight_col])</span>
<span id="cb67-1874"><a href="#cb67-1874"></a><span class="co"># Display PDP/ICE using the helper</span></span>
<span id="cb67-1875"><a href="#cb67-1875"></a>show_pdp(</span>
<span id="cb67-1876"><a href="#cb67-1876"></a>    wrappedAGModel<span class="op">=</span>ag_model_constrained_wrapped,</span>
<span id="cb67-1877"><a href="#cb67-1877"></a>    list_features<span class="op">=</span>all_features_constrained,</span>
<span id="cb67-1878"><a href="#cb67-1878"></a>    list_categ_features<span class="op">=</span>categorical_features_constrained <span class="cf">if</span> categorical_features_constrained <span class="cf">else</span> <span class="va">None</span>,</span>
<span id="cb67-1879"><a href="#cb67-1879"></a>    df<span class="op">=</span>plot_df_constrained,</span>
<span id="cb67-1880"><a href="#cb67-1880"></a>    show_ice<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb67-1881"><a href="#cb67-1881"></a>    sampSize<span class="op">=</span><span class="dv">25_000</span></span>
<span id="cb67-1882"><a href="#cb67-1882"></a>)</span>
<span id="cb67-1883"><a href="#cb67-1883"></a></span>
<span id="cb67-1884"><a href="#cb67-1884"></a><span class="in">```</span></span>
<span id="cb67-1885"><a href="#cb67-1885"></a></span>
<span id="cb67-1886"><a href="#cb67-1886"></a>**Interpretation:** Carefully examine the PDP curves (red dashed lines) for <span class="in">`loan_amnt`</span>, <span class="in">`dti`</span>, and <span class="in">`credit_score`</span>. They should now exhibit the enforced monotonic behavior: non-decreasing for <span class="in">`loan_amnt`</span> and <span class="in">`dti`</span>, and non-increasing for <span class="in">`credit_score`</span>. If the curves are flat or follow the expected trend without reversals, the constraints have been successfully applied.</span>
<span id="cb67-1887"><a href="#cb67-1887"></a></span>
<span id="cb67-1888"><a href="#cb67-1888"></a><span class="fu"># Evaluation &amp; Scoring the Test Set</span></span>
<span id="cb67-1889"><a href="#cb67-1889"></a></span>
<span id="cb67-1890"><a href="#cb67-1890"></a>**Goal:** Evaluate the performance of the final *constrained* model on the held-out TTD test set. Convert predicted PDs into a 3-digit scores and analyze the results using a KS table.</span>
<span id="cb67-1891"><a href="#cb67-1891"></a></span>
<span id="cb67-1892"><a href="#cb67-1892"></a><span class="fu">## Prepare Test Set and Evaluate</span></span>
<span id="cb67-1893"><a href="#cb67-1893"></a></span>
<span id="cb67-1894"><a href="#cb67-1894"></a>**Goal:** Apply the same feature processing steps used for the training data to the *accepted* and *rejected* test sets. For the rejected applicants, use the <span class="in">`ri_model`</span> to estimate their reject inference weights. Combine the accepted and rejected test sets, ensuring that the <span class="in">`sample_weight`</span> is set to 1 for accepted applicants and the rejected inference weights are used for rejected applicants.</span>
<span id="cb67-1895"><a href="#cb67-1895"></a></span>
<span id="cb67-1898"><a href="#cb67-1898"></a><span class="in">```{python}</span></span>
<span id="cb67-1899"><a href="#cb67-1899"></a><span class="co">#| label: tbl-test-weighted-default-rates</span></span>
<span id="cb67-1900"><a href="#cb67-1900"></a><span class="co">#| tbl-cap: "Weighted default rates for TTD Test Data"</span></span>
<span id="cb67-1901"><a href="#cb67-1901"></a></span>
<span id="cb67-1902"><a href="#cb67-1902"></a><span class="bu">print</span>(<span class="st">"--- Preparing TTD Test Set ---"</span>)</span>
<span id="cb67-1903"><a href="#cb67-1903"></a></span>
<span id="cb67-1904"><a href="#cb67-1904"></a><span class="co"># 1. Process Accepted Test Data</span></span>
<span id="cb67-1905"><a href="#cb67-1905"></a><span class="bu">print</span>(<span class="st">"Processing accepted test data..."</span>)</span>
<span id="cb67-1906"><a href="#cb67-1906"></a>df_accepted_test_common <span class="op">=</span> process_lending_data(</span>
<span id="cb67-1907"><a href="#cb67-1907"></a>    df<span class="op">=</span>df_accepted_test,</span>
<span id="cb67-1908"><a href="#cb67-1908"></a>    source_type<span class="op">=</span><span class="st">'accepted'</span>,</span>
<span id="cb67-1909"><a href="#cb67-1909"></a>    fillMissing_loanamnt<span class="op">=</span>rejected_train_median_loan_amnt, <span class="co"># Use values derived from training</span></span>
<span id="cb67-1910"><a href="#cb67-1910"></a>    fillMissing_emp_length<span class="op">=</span>rejected_train_median_emp_length,</span>
<span id="cb67-1911"><a href="#cb67-1911"></a>    fillMissing_dti<span class="op">=</span>rejected_train_p90_dti,</span>
<span id="cb67-1912"><a href="#cb67-1912"></a>    fillMissing_credit_score<span class="op">=</span>rejected_train_p10_credit_score</span>
<span id="cb67-1913"><a href="#cb67-1913"></a>)</span>
<span id="cb67-1914"><a href="#cb67-1914"></a></span>
<span id="cb67-1915"><a href="#cb67-1915"></a><span class="co"># 2. Process Rejected Test Data</span></span>
<span id="cb67-1916"><a href="#cb67-1916"></a><span class="bu">print</span>(<span class="st">"Processing rejected test data..."</span>)</span>
<span id="cb67-1917"><a href="#cb67-1917"></a>df_rejected_test_common <span class="op">=</span> process_lending_data(</span>
<span id="cb67-1918"><a href="#cb67-1918"></a>    df<span class="op">=</span>df_rejected_test,</span>
<span id="cb67-1919"><a href="#cb67-1919"></a>    source_type<span class="op">=</span><span class="st">'rejected'</span>,</span>
<span id="cb67-1920"><a href="#cb67-1920"></a>    fillMissing_loanamnt<span class="op">=</span>rejected_train_median_loan_amnt, <span class="co"># Use values derived from training</span></span>
<span id="cb67-1921"><a href="#cb67-1921"></a>    fillMissing_emp_length<span class="op">=</span>rejected_train_median_emp_length,</span>
<span id="cb67-1922"><a href="#cb67-1922"></a>    fillMissing_dti<span class="op">=</span>rejected_train_p90_dti,</span>
<span id="cb67-1923"><a href="#cb67-1923"></a>    fillMissing_credit_score<span class="op">=</span>rejected_train_p10_credit_score</span>
<span id="cb67-1924"><a href="#cb67-1924"></a>)</span>
<span id="cb67-1925"><a href="#cb67-1925"></a></span>
<span id="cb67-1926"><a href="#cb67-1926"></a><span class="co"># 3. Create TTD Test Set using Fuzzy Augmentation</span></span>
<span id="cb67-1927"><a href="#cb67-1927"></a><span class="bu">print</span>(<span class="st">"Creating TTD test set using RI model and Fuzzy Augmentation..."</span>)</span>
<span id="cb67-1928"><a href="#cb67-1928"></a>df_ttd_test_full <span class="op">=</span> create_TTD_data(</span>
<span id="cb67-1929"><a href="#cb67-1929"></a>    ri_model<span class="op">=</span>ri_model,</span>
<span id="cb67-1930"><a href="#cb67-1930"></a>    df_rejected<span class="op">=</span>df_rejected_test_common,</span>
<span id="cb67-1931"><a href="#cb67-1931"></a>    df_accepted<span class="op">=</span>df_accepted_test_common,</span>
<span id="cb67-1932"><a href="#cb67-1932"></a>    ri_features<span class="op">=</span>ri_features, <span class="co"># Features used by ri_model</span></span>
<span id="cb67-1933"><a href="#cb67-1933"></a>    modeling_features<span class="op">=</span>modeling_features, <span class="co"># Features to keep in the final TTD set</span></span>
<span id="cb67-1934"><a href="#cb67-1934"></a>    target_col<span class="op">=</span>target_col</span>
<span id="cb67-1935"><a href="#cb67-1935"></a>)</span>
<span id="cb67-1936"><a href="#cb67-1936"></a></span>
<span id="cb67-1937"><a href="#cb67-1937"></a><span class="bu">print</span>(<span class="ss">f"TTD test set created. Shape: </span><span class="sc">{</span>df_ttd_test_full<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-1938"><a href="#cb67-1938"></a></span>
<span id="cb67-1939"><a href="#cb67-1939"></a><span class="co"># 4. Summarize the TTD Test Set by Source using helper function</span></span>
<span id="cb67-1940"><a href="#cb67-1940"></a>summary_default_rates_test <span class="op">=</span> summarize_ttd_by_source(</span>
<span id="cb67-1941"><a href="#cb67-1941"></a>    df_ttd<span class="op">=</span>df_ttd_test_full,</span>
<span id="cb67-1942"><a href="#cb67-1942"></a>    target_col<span class="op">=</span>target_col,</span>
<span id="cb67-1943"><a href="#cb67-1943"></a>    weight_col<span class="op">=</span><span class="st">'sample_weight'</span>,</span>
<span id="cb67-1944"><a href="#cb67-1944"></a>    source_col<span class="op">=</span><span class="st">'source'</span></span>
<span id="cb67-1945"><a href="#cb67-1945"></a>)</span>
<span id="cb67-1946"><a href="#cb67-1946"></a></span>
<span id="cb67-1947"><a href="#cb67-1947"></a><span class="in">```</span></span>
<span id="cb67-1948"><a href="#cb67-1948"></a></span>
<span id="cb67-1951"><a href="#cb67-1951"></a><span class="in">```{python}</span></span>
<span id="cb67-1952"><a href="#cb67-1952"></a><span class="co"># | label: weighted-AUC</span></span>
<span id="cb67-1953"><a href="#cb67-1953"></a></span>
<span id="cb67-1954"><a href="#cb67-1954"></a><span class="co"># 5. Evaluate the Constrained Model on the TTD Test Set</span></span>
<span id="cb67-1955"><a href="#cb67-1955"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">--- Evaluating Constrained Model on TTD Test Set ---"</span>)</span>
<span id="cb67-1956"><a href="#cb67-1956"></a></span>
<span id="cb67-1957"><a href="#cb67-1957"></a><span class="co"># Prepare features for prediction - use the features the model was trained on</span></span>
<span id="cb67-1958"><a href="#cb67-1958"></a>X_test_ttd <span class="op">=</span> df_ttd_test_full[ag_model_constrained_wrapped.feature_names_] </span>
<span id="cb67-1959"><a href="#cb67-1959"></a>y_true_ttd <span class="op">=</span> df_ttd_test_full[<span class="st">'default_flag'</span>]</span>
<span id="cb67-1960"><a href="#cb67-1960"></a>weights_test_ttd <span class="op">=</span> df_ttd_test_full[<span class="st">'sample_weight'</span>]</span>
<span id="cb67-1961"><a href="#cb67-1961"></a></span>
<span id="cb67-1962"><a href="#cb67-1962"></a><span class="co"># Predict probabilities using the wrapped model</span></span>
<span id="cb67-1963"><a href="#cb67-1963"></a>pred_proba_ttd <span class="op">=</span> ag_model_constrained_wrapped.predict_proba(X_test_ttd)[:, <span class="dv">1</span>]</span>
<span id="cb67-1964"><a href="#cb67-1964"></a></span>
<span id="cb67-1965"><a href="#cb67-1965"></a><span class="co"># Create results DataFrame</span></span>
<span id="cb67-1966"><a href="#cb67-1966"></a>df_test_results <span class="op">=</span> df_ttd_test_full.copy()</span>
<span id="cb67-1967"><a href="#cb67-1967"></a>df_test_results[<span class="st">'pred_proba'</span>] <span class="op">=</span> pred_proba_ttd</span>
<span id="cb67-1968"><a href="#cb67-1968"></a></span>
<span id="cb67-1969"><a href="#cb67-1969"></a><span class="co"># Calculate weighted AUC for the entire TTD test set</span></span>
<span id="cb67-1970"><a href="#cb67-1970"></a>auc_ttd_weighted <span class="op">=</span> roc_auc_score(y_true_ttd, pred_proba_ttd, sample_weight<span class="op">=</span>weights_test_ttd)</span>
<span id="cb67-1971"><a href="#cb67-1971"></a><span class="bu">print</span>(<span class="ss">f"Weighted AUC on entire TTD Test Set: </span><span class="sc">{</span>auc_ttd_weighted<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb67-1972"><a href="#cb67-1972"></a></span>
<span id="cb67-1973"><a href="#cb67-1973"></a><span class="co"># Calculate unweighted AUC for the entire TTD test set</span></span>
<span id="cb67-1974"><a href="#cb67-1974"></a>auc_ttd_unweighted <span class="op">=</span> roc_auc_score(y_true_ttd, pred_proba_ttd)</span>
<span id="cb67-1975"><a href="#cb67-1975"></a><span class="bu">print</span>(<span class="ss">f"Unweighted AUC on entire TTD Test Set: </span><span class="sc">{</span>auc_ttd_unweighted<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb67-1976"><a href="#cb67-1976"></a></span>
<span id="cb67-1977"><a href="#cb67-1977"></a><span class="in">```</span></span>
<span id="cb67-1978"><a href="#cb67-1978"></a></span>
<span id="cb67-1979"><a href="#cb67-1979"></a><span class="fu">## Convert Probability (PD) to Score</span></span>
<span id="cb67-1980"><a href="#cb67-1980"></a></span>
<span id="cb67-1981"><a href="#cb67-1981"></a>**Concept:** Convert the model's predicted probability of default (PD) into a more intuitive 3-digit credit score (e.g., 300-850). Lower scores indicate higher risk.</span>
<span id="cb67-1982"><a href="#cb67-1982"></a></span>
<span id="cb67-1983"><a href="#cb67-1983"></a></span>
<span id="cb67-1984"><a href="#cb67-1984"></a>**Formulas:** </span>
<span id="cb67-1985"><a href="#cb67-1985"></a></span>
<span id="cb67-1986"><a href="#cb67-1986"></a>$Score = BaseScore - Factor \cdot \ln(OddsBad)$</span>
<span id="cb67-1987"><a href="#cb67-1987"></a></span>
<span id="cb67-1988"><a href="#cb67-1988"></a>where $OddsBad = \frac{PD}{1 - PD}$</span>
<span id="cb67-1989"><a href="#cb67-1989"></a></span>
<span id="cb67-1990"><a href="#cb67-1990"></a>$Factor = \frac{PDO}{\ln(2)}$.</span>
<span id="cb67-1991"><a href="#cb67-1991"></a></span>
<span id="cb67-1992"><a href="#cb67-1992"></a></span>
<span id="cb67-1993"><a href="#cb67-1993"></a></span>
<span id="cb67-1996"><a href="#cb67-1996"></a><span class="in">```{python}</span></span>
<span id="cb67-1997"><a href="#cb67-1997"></a><span class="co">#| label: fig-convert-pd-to-score</span></span>
<span id="cb67-1998"><a href="#cb67-1998"></a><span class="co">#| fig-cap: "Distribution of Calculated Scores (TTD Test Set)"</span></span>
<span id="cb67-1999"><a href="#cb67-1999"></a></span>
<span id="cb67-2000"><a href="#cb67-2000"></a>user_pdo <span class="op">=</span> <span class="dv">40</span></span>
<span id="cb67-2001"><a href="#cb67-2001"></a>user_basescore <span class="op">=</span> <span class="dv">680</span></span>
<span id="cb67-2002"><a href="#cb67-2002"></a></span>
<span id="cb67-2003"><a href="#cb67-2003"></a><span class="bu">print</span>(<span class="st">"Converting predicted probabilities to scores..."</span>)</span>
<span id="cb67-2004"><a href="#cb67-2004"></a><span class="co"># Use the defined helper function</span></span>
<span id="cb67-2005"><a href="#cb67-2005"></a>df_test_results[<span class="st">'score'</span>] <span class="op">=</span> calculate_score(</span>
<span id="cb67-2006"><a href="#cb67-2006"></a>    df_test_results[<span class="st">'pred_proba'</span>], </span>
<span id="cb67-2007"><a href="#cb67-2007"></a>    pdo<span class="op">=</span>user_pdo, </span>
<span id="cb67-2008"><a href="#cb67-2008"></a>    base_score<span class="op">=</span>user_basescore</span>
<span id="cb67-2009"><a href="#cb67-2009"></a>)</span>
<span id="cb67-2010"><a href="#cb67-2010"></a></span>
<span id="cb67-2011"><a href="#cb67-2011"></a><span class="bu">print</span>(<span class="st">"Scores calculated and added to test results."</span>)</span>
<span id="cb67-2012"><a href="#cb67-2012"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Score Distribution Summary:"</span>)</span>
<span id="cb67-2013"><a href="#cb67-2013"></a><span class="bu">print</span>(df_test_results[<span class="st">'score'</span>].describe())</span>
<span id="cb67-2014"><a href="#cb67-2014"></a></span>
<span id="cb67-2015"><a href="#cb67-2015"></a><span class="co"># Plot score distribution</span></span>
<span id="cb67-2016"><a href="#cb67-2016"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb67-2017"><a href="#cb67-2017"></a>sns.histplot(df_test_results[<span class="st">'score'</span>], bins<span class="op">=</span><span class="dv">50</span>, kde<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb67-2018"><a href="#cb67-2018"></a>plt.title(<span class="st">'Distribution of Calculated Scores (TTD Test Set)'</span>)</span>
<span id="cb67-2019"><a href="#cb67-2019"></a>plt.xlabel(<span class="st">'Score'</span>)</span>
<span id="cb67-2020"><a href="#cb67-2020"></a>plt.ylabel(<span class="st">'Frequency'</span>)</span>
<span id="cb67-2021"><a href="#cb67-2021"></a>plt.grid(axis<span class="op">=</span><span class="st">'y'</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb67-2022"><a href="#cb67-2022"></a>plt.show()</span>
<span id="cb67-2023"><a href="#cb67-2023"></a></span>
<span id="cb67-2024"><a href="#cb67-2024"></a></span>
<span id="cb67-2025"><a href="#cb67-2025"></a><span class="in">```</span></span>
<span id="cb67-2026"><a href="#cb67-2026"></a></span>
<span id="cb67-2027"><a href="#cb67-2027"></a><span class="fu">## Build KS Table</span></span>
<span id="cb67-2028"><a href="#cb67-2028"></a></span>
<span id="cb67-2029"><a href="#cb67-2029"></a>**Concept:** In credit scorecards, the Kolmogorov-Smirnov (KS) statistic measures how well the scorecard separates "goods" (non-defaults) from "bads" (defaults). It quantifies the maximum difference between the cumulative distribution functions of the scores for the good and bad populations across different score ranges (bins). A higher KS value indicates better separation power of the scorecard.</span>
<span id="cb67-2030"><a href="#cb67-2030"></a></span>
<span id="cb67-2031"><a href="#cb67-2031"></a>A model with high KS would generate high PD predictions for defaulted loans and low PD predictions for non-defaulted loans. Ideally, the PD predictions from the two target classes should not overlap. While perfect separation is rarely achieved in practice, a high KS signifies the model is closer to this ideal.</span>
<span id="cb67-2032"><a href="#cb67-2032"></a></span>
<span id="cb67-2035"><a href="#cb67-2035"></a><span class="in">```{python}</span></span>
<span id="cb67-2036"><a href="#cb67-2036"></a><span class="co">#| label: tbl-build-ks-table</span></span>
<span id="cb67-2037"><a href="#cb67-2037"></a><span class="co">#| tbl-cap: "KS Table for Constrained Model on TTD Test Set"</span></span>
<span id="cb67-2038"><a href="#cb67-2038"></a></span>
<span id="cb67-2039"><a href="#cb67-2039"></a></span>
<span id="cb67-2040"><a href="#cb67-2040"></a><span class="bu">print</span>(<span class="st">"Generating KS table using scores and sample weights..."</span>)</span>
<span id="cb67-2041"><a href="#cb67-2041"></a>ks_results_table <span class="op">=</span> ks_table(</span>
<span id="cb67-2042"><a href="#cb67-2042"></a>    data<span class="op">=</span>df_test_results,</span>
<span id="cb67-2043"><a href="#cb67-2043"></a>    y_true_col<span class="op">=</span><span class="st">'default_flag'</span>,</span>
<span id="cb67-2044"><a href="#cb67-2044"></a>    y_pred_col<span class="op">=</span><span class="st">'score'</span>,</span>
<span id="cb67-2045"><a href="#cb67-2045"></a>    n_bins<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb67-2046"><a href="#cb67-2046"></a>    is_score<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb67-2047"><a href="#cb67-2047"></a>    sample_weight_col<span class="op">=</span><span class="st">'sample_weight'</span></span>
<span id="cb67-2048"><a href="#cb67-2048"></a>)</span>
<span id="cb67-2049"><a href="#cb67-2049"></a>display(ks_results_table)</span>
<span id="cb67-2050"><a href="#cb67-2050"></a></span>
<span id="cb67-2051"><a href="#cb67-2051"></a><span class="in">```</span></span>
<span id="cb67-2052"><a href="#cb67-2052"></a></span>
<span id="cb67-2053"><a href="#cb67-2053"></a>Plot cumulative bad rates and good rates to visualize the KS statistic.</span>
<span id="cb67-2054"><a href="#cb67-2054"></a></span>
<span id="cb67-2057"><a href="#cb67-2057"></a><span class="in">```{python}</span></span>
<span id="cb67-2058"><a href="#cb67-2058"></a><span class="co">#| label: fig-cumulative-rates</span></span>
<span id="cb67-2059"><a href="#cb67-2059"></a><span class="co">#| fig-cap: "KS Plot: Cumulative Bad and Good Rates"</span></span>
<span id="cb67-2060"><a href="#cb67-2060"></a></span>
<span id="cb67-2061"><a href="#cb67-2061"></a></span>
<span id="cb67-2062"><a href="#cb67-2062"></a><span class="bu">print</span>(<span class="st">"Plotting cumulative bad rates and good rates to visualize the KS statistic..."</span>)</span>
<span id="cb67-2063"><a href="#cb67-2063"></a></span>
<span id="cb67-2064"><a href="#cb67-2064"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb67-2065"><a href="#cb67-2065"></a>plt.plot(ks_results_table[<span class="st">'cum_bads_pct'</span>], label<span class="op">=</span><span class="st">'Cumulative Bad Rate (%)'</span>, color<span class="op">=</span><span class="st">'red'</span>, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb67-2066"><a href="#cb67-2066"></a>plt.plot(ks_results_table[<span class="st">'cum_goods_pct'</span>], label<span class="op">=</span><span class="st">'Cumulative Good Rate (%)'</span>, color<span class="op">=</span><span class="st">'blue'</span>, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb67-2067"><a href="#cb67-2067"></a>plt.fill_between(<span class="bu">range</span>(<span class="bu">len</span>(ks_results_table)), </span>
<span id="cb67-2068"><a href="#cb67-2068"></a>                    ks_results_table[<span class="st">'cum_bads_pct'</span>], </span>
<span id="cb67-2069"><a href="#cb67-2069"></a>                    ks_results_table[<span class="st">'cum_goods_pct'</span>], </span>
<span id="cb67-2070"><a href="#cb67-2070"></a>                    color<span class="op">=</span><span class="st">'gray'</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, label<span class="op">=</span><span class="st">'KS Gap'</span>)</span>
<span id="cb67-2071"><a href="#cb67-2071"></a></span>
<span id="cb67-2072"><a href="#cb67-2072"></a><span class="co"># Highlight the maximum KS point</span></span>
<span id="cb67-2073"><a href="#cb67-2073"></a>max_ks_idx <span class="op">=</span> ks_results_table[<span class="st">'ks'</span>].idxmax()</span>
<span id="cb67-2074"><a href="#cb67-2074"></a>max_ks_value <span class="op">=</span> ks_results_table.loc[max_ks_idx, <span class="st">'ks'</span>]</span>
<span id="cb67-2075"><a href="#cb67-2075"></a>plt.axvline(x<span class="op">=</span>max_ks_idx, color<span class="op">=</span><span class="st">'green'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, label<span class="op">=</span><span class="ss">f'Max KS = </span><span class="sc">{</span>max_ks_value<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb67-2076"><a href="#cb67-2076"></a></span>
<span id="cb67-2077"><a href="#cb67-2077"></a><span class="co"># Set x-ticks to average score from each bin</span></span>
<span id="cb67-2078"><a href="#cb67-2078"></a>y_pred_col_used <span class="op">=</span> <span class="st">'score'</span> <span class="co"># The column used in ks_table call</span></span>
<span id="cb67-2079"><a href="#cb67-2079"></a>avg_score_col_name <span class="op">=</span> <span class="ss">f'avg_</span><span class="sc">{</span>y_pred_col_used<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb67-2080"><a href="#cb67-2080"></a></span>
<span id="cb67-2081"><a href="#cb67-2081"></a><span class="cf">if</span> avg_score_col_name <span class="kw">in</span> ks_results_table.columns:</span>
<span id="cb67-2082"><a href="#cb67-2082"></a>    avg_scores <span class="op">=</span> ks_results_table[avg_score_col_name]</span>
<span id="cb67-2083"><a href="#cb67-2083"></a>    plt.xticks(ticks<span class="op">=</span><span class="bu">range</span>(<span class="bu">len</span>(avg_scores)), labels<span class="op">=</span>[<span class="ss">f"</span><span class="sc">{</span>score<span class="sc">:.0f}</span><span class="ss">"</span> <span class="cf">for</span> score <span class="kw">in</span> avg_scores], rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb67-2084"><a href="#cb67-2084"></a>    plt.xlabel(<span class="ss">f'Average </span><span class="sc">{</span>y_pred_col_used<span class="sc">.</span>capitalize()<span class="sc">}</span><span class="ss"> (per Bin)'</span>) <span class="co"># Dynamic label</span></span>
<span id="cb67-2085"><a href="#cb67-2085"></a><span class="cf">else</span>:</span>
<span id="cb67-2086"><a href="#cb67-2086"></a>    <span class="bu">print</span>(<span class="ss">f"Warning: Column '</span><span class="sc">{</span>avg_score_col_name<span class="sc">}</span><span class="ss">' not found in KS table for x-axis labels."</span>)</span>
<span id="cb67-2087"><a href="#cb67-2087"></a>    plt.xlabel(<span class="st">'Bin Index'</span>) <span class="co"># Fallback label</span></span>
<span id="cb67-2088"><a href="#cb67-2088"></a></span>
<span id="cb67-2089"><a href="#cb67-2089"></a>plt.title(<span class="st">'Cumulative Bad and Good Rates with KS Statistic'</span>)</span>
<span id="cb67-2090"><a href="#cb67-2090"></a>plt.ylabel(<span class="st">'Cumulative Percentage (%)'</span>)</span>
<span id="cb67-2091"><a href="#cb67-2091"></a>plt.legend(loc<span class="op">=</span><span class="st">'best'</span>)</span>
<span id="cb67-2092"><a href="#cb67-2092"></a>plt.grid(alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb67-2093"><a href="#cb67-2093"></a>plt.tight_layout()</span>
<span id="cb67-2094"><a href="#cb67-2094"></a>plt.show()</span>
<span id="cb67-2095"><a href="#cb67-2095"></a></span>
<span id="cb67-2096"><a href="#cb67-2096"></a><span class="in">```</span></span>
<span id="cb67-2097"><a href="#cb67-2097"></a></span>
<span id="cb67-2098"><a href="#cb67-2098"></a>**Interpretation:** A KS &gt; 30 is often acceptable, &gt; 40 good, &gt; 50 excellent. These ranges are general guidelines often cited in the industry; the acceptable KS level depends heavily on the specific business context and application.</span>
<span id="cb67-2099"><a href="#cb67-2099"></a></span>
<span id="cb67-2100"><a href="#cb67-2100"></a><span class="fu"># Decision Threshold Selection</span></span>
<span id="cb67-2101"><a href="#cb67-2101"></a></span>
<span id="cb67-2102"><a href="#cb67-2102"></a><span class="fu">## Prepare Calibration Data for Thresholding</span></span>
<span id="cb67-2103"><a href="#cb67-2103"></a></span>
<span id="cb67-2104"><a href="#cb67-2104"></a>There are multiple methods to estimate the optimal decision threshold for binary classification. We explored some of those methods in previous lectures and labs:</span>
<span id="cb67-2105"><a href="#cb67-2105"></a></span>
<span id="cb67-2106"><a href="#cb67-2106"></a><span class="ss">1. </span>Maximizing F1 score</span>
<span id="cb67-2107"><a href="#cb67-2107"></a><span class="ss">2. </span>Maximizing Profit</span>
<span id="cb67-2108"><a href="#cb67-2108"></a></span>
<span id="cb67-2109"><a href="#cb67-2109"></a>In this lab, we will use the score associated with the KS statistic.</span>
<span id="cb67-2110"><a href="#cb67-2110"></a></span>
<span id="cb67-2111"><a href="#cb67-2111"></a>We will use the calibration data set to estimate the threshold.</span>
<span id="cb67-2112"><a href="#cb67-2112"></a></span>
<span id="cb67-2113"><a href="#cb67-2113"></a>First, we need to construct the TTD data for the calibration data.</span>
<span id="cb67-2114"><a href="#cb67-2114"></a></span>
<span id="cb67-2117"><a href="#cb67-2117"></a><span class="in">```{python}</span></span>
<span id="cb67-2118"><a href="#cb67-2118"></a><span class="co">#| label: tbl-ttd-calibration</span></span>
<span id="cb67-2119"><a href="#cb67-2119"></a><span class="co">#| tbl-cap: "Weighted default rates for TTD Calibration Data"</span></span>
<span id="cb67-2120"><a href="#cb67-2120"></a></span>
<span id="cb67-2121"><a href="#cb67-2121"></a>df_accepted_calib_common <span class="op">=</span> process_lending_data(</span>
<span id="cb67-2122"><a href="#cb67-2122"></a>    df<span class="op">=</span>df_accepted_calib,</span>
<span id="cb67-2123"><a href="#cb67-2123"></a>    source_type<span class="op">=</span><span class="st">'accepted'</span>,</span>
<span id="cb67-2124"><a href="#cb67-2124"></a>    fillMissing_loanamnt<span class="op">=</span>rejected_train_median_loan_amnt, <span class="co"># Use values derived from training</span></span>
<span id="cb67-2125"><a href="#cb67-2125"></a>    fillMissing_emp_length<span class="op">=</span>rejected_train_median_emp_length,</span>
<span id="cb67-2126"><a href="#cb67-2126"></a>    fillMissing_dti<span class="op">=</span>rejected_train_p90_dti,</span>
<span id="cb67-2127"><a href="#cb67-2127"></a>    fillMissing_credit_score<span class="op">=</span>rejected_train_p10_credit_score</span>
<span id="cb67-2128"><a href="#cb67-2128"></a>)</span>
<span id="cb67-2129"><a href="#cb67-2129"></a></span>
<span id="cb67-2130"><a href="#cb67-2130"></a>df_rejected_calib_common <span class="op">=</span> process_lending_data(</span>
<span id="cb67-2131"><a href="#cb67-2131"></a>    df<span class="op">=</span>df_rejected_calib,</span>
<span id="cb67-2132"><a href="#cb67-2132"></a>    source_type<span class="op">=</span><span class="st">'rejected'</span>,</span>
<span id="cb67-2133"><a href="#cb67-2133"></a>    fillMissing_loanamnt<span class="op">=</span>rejected_train_median_loan_amnt, <span class="co"># Use values derived from training</span></span>
<span id="cb67-2134"><a href="#cb67-2134"></a>    fillMissing_emp_length<span class="op">=</span>rejected_train_median_emp_length,</span>
<span id="cb67-2135"><a href="#cb67-2135"></a>    fillMissing_dti<span class="op">=</span>rejected_train_p90_dti,</span>
<span id="cb67-2136"><a href="#cb67-2136"></a>    fillMissing_credit_score<span class="op">=</span>rejected_train_p10_credit_score</span>
<span id="cb67-2137"><a href="#cb67-2137"></a>)</span>
<span id="cb67-2138"><a href="#cb67-2138"></a></span>
<span id="cb67-2139"><a href="#cb67-2139"></a><span class="co"># Create TTD Calibration Set using Fuzzy Augmentation</span></span>
<span id="cb67-2140"><a href="#cb67-2140"></a><span class="bu">print</span>(<span class="st">"Creating TTD calibration set using RI model and Fuzzy Augmentation..."</span>)</span>
<span id="cb67-2141"><a href="#cb67-2141"></a>df_ttd_calib_full <span class="op">=</span> create_TTD_data(</span>
<span id="cb67-2142"><a href="#cb67-2142"></a>    ri_model<span class="op">=</span>ri_model,</span>
<span id="cb67-2143"><a href="#cb67-2143"></a>    df_rejected<span class="op">=</span>df_rejected_calib_common,</span>
<span id="cb67-2144"><a href="#cb67-2144"></a>    df_accepted<span class="op">=</span>df_accepted_calib_common,</span>
<span id="cb67-2145"><a href="#cb67-2145"></a>    ri_features<span class="op">=</span>ri_features, <span class="co"># Features used by ri_model</span></span>
<span id="cb67-2146"><a href="#cb67-2146"></a>    modeling_features<span class="op">=</span>modeling_features, <span class="co"># Features to keep in the final TTD set</span></span>
<span id="cb67-2147"><a href="#cb67-2147"></a>    target_col<span class="op">=</span>target_col</span>
<span id="cb67-2148"><a href="#cb67-2148"></a>)</span>
<span id="cb67-2149"><a href="#cb67-2149"></a><span class="bu">print</span>(<span class="ss">f"TTD calibration set created. Shape: </span><span class="sc">{</span>df_ttd_calib_full<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb67-2150"><a href="#cb67-2150"></a><span class="co"># Summarize the TTD Calibration Set by Source using helper function</span></span>
<span id="cb67-2151"><a href="#cb67-2151"></a>summary_default_rates_calib <span class="op">=</span> summarize_ttd_by_source(</span>
<span id="cb67-2152"><a href="#cb67-2152"></a>    df_ttd<span class="op">=</span>df_ttd_calib_full,</span>
<span id="cb67-2153"><a href="#cb67-2153"></a>    target_col<span class="op">=</span>target_col,</span>
<span id="cb67-2154"><a href="#cb67-2154"></a>    weight_col<span class="op">=</span><span class="st">'sample_weight'</span>,</span>
<span id="cb67-2155"><a href="#cb67-2155"></a>    source_col<span class="op">=</span><span class="st">'source'</span></span>
<span id="cb67-2156"><a href="#cb67-2156"></a>)</span>
<span id="cb67-2157"><a href="#cb67-2157"></a></span>
<span id="cb67-2158"><a href="#cb67-2158"></a><span class="in">```</span></span>
<span id="cb67-2159"><a href="#cb67-2159"></a></span>
<span id="cb67-2160"><a href="#cb67-2160"></a><span class="fu">## Determine Score Threshold using KS Table</span></span>
<span id="cb67-2161"><a href="#cb67-2161"></a></span>
<span id="cb67-2162"><a href="#cb67-2162"></a>Second, predict each applicant's PD and convert to a score.</span>
<span id="cb67-2163"><a href="#cb67-2163"></a></span>
<span id="cb67-2166"><a href="#cb67-2166"></a><span class="in">```{python}</span></span>
<span id="cb67-2167"><a href="#cb67-2167"></a><span class="co">#| label: calculate-scores-calib</span></span>
<span id="cb67-2168"><a href="#cb67-2168"></a></span>
<span id="cb67-2169"><a href="#cb67-2169"></a>df_ttd_calib_full[<span class="st">'pred_proba'</span>] <span class="op">=</span> ag_model_constrained_wrapped.predict_proba(</span>
<span id="cb67-2170"><a href="#cb67-2170"></a>    df_ttd_calib_full[ag_model_constrained_wrapped.feature_names_]</span>
<span id="cb67-2171"><a href="#cb67-2171"></a>)[:, <span class="dv">1</span>]</span>
<span id="cb67-2172"><a href="#cb67-2172"></a></span>
<span id="cb67-2173"><a href="#cb67-2173"></a>df_ttd_calib_full[<span class="st">'score'</span>] <span class="op">=</span> calculate_score(</span>
<span id="cb67-2174"><a href="#cb67-2174"></a>    df_ttd_calib_full[<span class="st">'pred_proba'</span>], </span>
<span id="cb67-2175"><a href="#cb67-2175"></a>    pdo<span class="op">=</span>user_pdo, </span>
<span id="cb67-2176"><a href="#cb67-2176"></a>    base_score<span class="op">=</span>user_basescore</span>
<span id="cb67-2177"><a href="#cb67-2177"></a>)</span>
<span id="cb67-2178"><a href="#cb67-2178"></a><span class="in">```</span></span>
<span id="cb67-2179"><a href="#cb67-2179"></a></span>
<span id="cb67-2180"><a href="#cb67-2180"></a>Finally, find the optimal threshold using the KS statistic.</span>
<span id="cb67-2181"><a href="#cb67-2181"></a></span>
<span id="cb67-2184"><a href="#cb67-2184"></a><span class="in">```{python}</span></span>
<span id="cb67-2185"><a href="#cb67-2185"></a><span class="co">#| label: tbl-ks-table-calib</span></span>
<span id="cb67-2186"><a href="#cb67-2186"></a><span class="co">#| tbl-cap: "KS Table for TTD Calibration Data"</span></span>
<span id="cb67-2187"><a href="#cb67-2187"></a></span>
<span id="cb67-2188"><a href="#cb67-2188"></a>ks_calibration <span class="op">=</span> ks_table(</span>
<span id="cb67-2189"><a href="#cb67-2189"></a>    data<span class="op">=</span>df_ttd_calib_full,</span>
<span id="cb67-2190"><a href="#cb67-2190"></a>    y_true_col<span class="op">=</span><span class="st">'default_flag'</span>,</span>
<span id="cb67-2191"><a href="#cb67-2191"></a>    y_pred_col<span class="op">=</span><span class="st">'score'</span>,</span>
<span id="cb67-2192"><a href="#cb67-2192"></a>    n_bins<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb67-2193"><a href="#cb67-2193"></a>    is_score<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb67-2194"><a href="#cb67-2194"></a>    sample_weight_col<span class="op">=</span><span class="st">'sample_weight'</span></span>
<span id="cb67-2195"><a href="#cb67-2195"></a>)</span>
<span id="cb67-2196"><a href="#cb67-2196"></a></span>
<span id="cb67-2197"><a href="#cb67-2197"></a>display(ks_calibration)</span>
<span id="cb67-2198"><a href="#cb67-2198"></a><span class="in">```</span></span>
<span id="cb67-2199"><a href="#cb67-2199"></a></span>
<span id="cb67-2200"><a href="#cb67-2200"></a>Based on the KS table for the calibration set, the maximum KS occurs in the bin starting at score 767. We will select 767 as our decision threshold (approve if score &gt;= 767).</span>
<span id="cb67-2201"><a href="#cb67-2201"></a></span>
<span id="cb67-2202"><a href="#cb67-2202"></a>In order to simplify model predictions, we will convert the score of 767 into a probability and set the threshold inside AutoGluon.</span>
<span id="cb67-2203"><a href="#cb67-2203"></a></span>
<span id="cb67-2206"><a href="#cb67-2206"></a><span class="in">```{python}</span></span>
<span id="cb67-2207"><a href="#cb67-2207"></a><span class="co">#| label: set-predict-threshold</span></span>
<span id="cb67-2208"><a href="#cb67-2208"></a></span>
<span id="cb67-2209"><a href="#cb67-2209"></a>score_threshold <span class="op">=</span> <span class="dv">767</span></span>
<span id="cb67-2210"><a href="#cb67-2210"></a></span>
<span id="cb67-2211"><a href="#cb67-2211"></a>PD_threshold <span class="op">=</span> score_to_probability(</span>
<span id="cb67-2212"><a href="#cb67-2212"></a>    score<span class="op">=</span>score_threshold, </span>
<span id="cb67-2213"><a href="#cb67-2213"></a>    pdo<span class="op">=</span>user_pdo, </span>
<span id="cb67-2214"><a href="#cb67-2214"></a>    base_score<span class="op">=</span>user_basescore</span>
<span id="cb67-2215"><a href="#cb67-2215"></a>)</span>
<span id="cb67-2216"><a href="#cb67-2216"></a></span>
<span id="cb67-2217"><a href="#cb67-2217"></a>ag_model_constrained_wrapped.predictor.set_decision_threshold(PD_threshold)</span>
<span id="cb67-2218"><a href="#cb67-2218"></a></span>
<span id="cb67-2219"><a href="#cb67-2219"></a>ag_model_constrained_wrapped.predictor.save() <span class="co"># Save the model with the new threshold</span></span>
<span id="cb67-2220"><a href="#cb67-2220"></a><span class="in">```</span></span>
<span id="cb67-2221"><a href="#cb67-2221"></a></span>
<span id="cb67-2222"><a href="#cb67-2222"></a><span class="fu">## Evaluate Model Decisions</span></span>
<span id="cb67-2223"><a href="#cb67-2223"></a></span>
<span id="cb67-2224"><a href="#cb67-2224"></a>Now let's evaluate the threshold using the test set in a way that matches how the model would be used in production. </span>
<span id="cb67-2225"><a href="#cb67-2225"></a></span>
<span id="cb67-2226"><a href="#cb67-2226"></a>During training and calibration, we used sample weights and cloned the rejected applicants to correct for selection bias (since we don't know their true outcomes). However, when scoring **new** applicants, we do not know their outcomes and do not use sample weights or cloning.</span>
<span id="cb67-2227"><a href="#cb67-2227"></a></span>
<span id="cb67-2228"><a href="#cb67-2228"></a>Therefore, to simulate production scoring, we will evaluate the model on the test set by setting all sample weights to 1 and including each applicant only once (no cloning of rejects).</span>
<span id="cb67-2229"><a href="#cb67-2229"></a></span>
<span id="cb67-2230"><a href="#cb67-2230"></a>Note that this evaluation differs from the weighted KS calculation in Section 8. There, we assessed the model's ability to separate goods and bads according to the weighted distribution it was trained on. Here, we are simulating the practical application of the chosen score threshold (767) to new, individual applicants (without weighting or cloning) to see the resulting approval/rejection rates.</span>
<span id="cb67-2231"><a href="#cb67-2231"></a></span>
<span id="cb67-2234"><a href="#cb67-2234"></a><span class="in">```{python}</span></span>
<span id="cb67-2235"><a href="#cb67-2235"></a><span class="co">#| label: tbl-ttd-data-noCloning</span></span>
<span id="cb67-2236"><a href="#cb67-2236"></a><span class="co">#| tbl-cap: "Sample of TTD Test Data (No Cloning)"</span></span>
<span id="cb67-2237"><a href="#cb67-2237"></a></span>
<span id="cb67-2238"><a href="#cb67-2238"></a>df_ttd_test_noCloning <span class="op">=</span> create_TTD_data(</span>
<span id="cb67-2239"><a href="#cb67-2239"></a>    ri_model<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb67-2240"><a href="#cb67-2240"></a>    df_rejected<span class="op">=</span>df_rejected_test_common,</span>
<span id="cb67-2241"><a href="#cb67-2241"></a>    df_accepted<span class="op">=</span>df_accepted_test_common,</span>
<span id="cb67-2242"><a href="#cb67-2242"></a>    ri_features<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb67-2243"><a href="#cb67-2243"></a>    modeling_features<span class="op">=</span>modeling_features,</span>
<span id="cb67-2244"><a href="#cb67-2244"></a>    target_col<span class="op">=</span>target_col,</span>
<span id="cb67-2245"><a href="#cb67-2245"></a>    clone_rejected<span class="op">=</span><span class="va">False</span></span>
<span id="cb67-2246"><a href="#cb67-2246"></a>)</span>
<span id="cb67-2247"><a href="#cb67-2247"></a></span>
<span id="cb67-2248"><a href="#cb67-2248"></a>display(df_ttd_test_noCloning.sample(<span class="dv">100</span>).head())</span>
<span id="cb67-2249"><a href="#cb67-2249"></a><span class="in">```</span></span>
<span id="cb67-2250"><a href="#cb67-2250"></a></span>
<span id="cb67-2251"><a href="#cb67-2251"></a>We can score the applicants in the test set and determine how many applicants are accepted or rejected by the TTD model.</span>
<span id="cb67-2252"><a href="#cb67-2252"></a></span>
<span id="cb67-2255"><a href="#cb67-2255"></a><span class="in">```{python}</span></span>
<span id="cb67-2256"><a href="#cb67-2256"></a><span class="co">#| label: tbl-ttd-model-decisions</span></span>
<span id="cb67-2257"><a href="#cb67-2257"></a><span class="co">#| tbl-cap: "Model Decisions on TTD Test Set (No Cloning)"</span></span>
<span id="cb67-2258"><a href="#cb67-2258"></a></span>
<span id="cb67-2259"><a href="#cb67-2259"></a>df_ttd_test_noCloning[<span class="st">'model_decision'</span>] <span class="op">=</span> ag_model_constrained_wrapped.predict(</span>
<span id="cb67-2260"><a href="#cb67-2260"></a>    df_ttd_test_noCloning[ag_model_constrained_wrapped.feature_names_]</span>
<span id="cb67-2261"><a href="#cb67-2261"></a>)</span>
<span id="cb67-2262"><a href="#cb67-2262"></a></span>
<span id="cb67-2263"><a href="#cb67-2263"></a>df_ttd_test_noCloning[<span class="st">'model_decision'</span>] <span class="op">=</span> df_ttd_test_noCloning[<span class="st">'model_decision'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="st">'Model_Reject'</span> <span class="cf">if</span> x<span class="op">==</span><span class="dv">1</span> <span class="cf">else</span> <span class="st">'Model_Approve'</span>)</span>
<span id="cb67-2264"><a href="#cb67-2264"></a></span>
<span id="cb67-2265"><a href="#cb67-2265"></a><span class="co"># Compare model decisions to actual outcomes</span></span>
<span id="cb67-2266"><a href="#cb67-2266"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">--- Comparing Model Decisions to Actual Outcomes ---"</span>)</span>
<span id="cb67-2267"><a href="#cb67-2267"></a></span>
<span id="cb67-2268"><a href="#cb67-2268"></a><span class="co"># Create a cross-tabulation of model decision vs actual outcome</span></span>
<span id="cb67-2269"><a href="#cb67-2269"></a>comparison <span class="op">=</span> pd.crosstab(</span>
<span id="cb67-2270"><a href="#cb67-2270"></a>    df_ttd_test_noCloning[<span class="st">'model_decision'</span>],</span>
<span id="cb67-2271"><a href="#cb67-2271"></a>    df_ttd_test_noCloning[<span class="st">'source'</span>],</span>
<span id="cb67-2272"><a href="#cb67-2272"></a>    margins<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb67-2273"><a href="#cb67-2273"></a>    margins_name<span class="op">=</span><span class="st">'Total'</span></span>
<span id="cb67-2274"><a href="#cb67-2274"></a>)</span>
<span id="cb67-2275"><a href="#cb67-2275"></a></span>
<span id="cb67-2276"><a href="#cb67-2276"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Count of model decisions vs. actual outcomes:"</span>)</span>
<span id="cb67-2277"><a href="#cb67-2277"></a>display(comparison)</span>
<span id="cb67-2278"><a href="#cb67-2278"></a></span>
<span id="cb67-2279"><a href="#cb67-2279"></a><span class="in">```</span></span>
<span id="cb67-2280"><a href="#cb67-2280"></a></span>
<span id="cb67-2281"><a href="#cb67-2281"></a><span class="fu"># Conclusion</span></span>
<span id="cb67-2282"><a href="#cb67-2282"></a></span>
<span id="cb67-2283"><a href="#cb67-2283"></a>In this lab, we built a credit scorecard using LendingClub data. We went through the entire process, from preparing the data to evaluating the final model and choosing a cutoff score. A key part was using data from rejected applications to make the model representative of the full applicant pool (this is called reject inference).</span>
<span id="cb67-2284"><a href="#cb67-2284"></a></span>
<span id="cb67-2285"><a href="#cb67-2285"></a>**Key steps and takeaways:**</span>
<span id="cb67-2286"><a href="#cb67-2286"></a></span>
<span id="cb67-2287"><a href="#cb67-2287"></a><span class="ss">*   </span>**Preparing Data:** We loaded data for both approved and rejected loans, created useful features, and processed both datasets consistently.</span>
<span id="cb67-2288"><a href="#cb67-2288"></a><span class="ss">*   </span>**Checking Feature Trends:** We verified that features like Debt-to-Income (DTI) and credit score behaved as expected (e.g., higher credit score means lower risk). This ensures the model makes business sense.</span>
<span id="cb67-2289"><a href="#cb67-2289"></a><span class="ss">*   </span>**Using Rejected Data (Reject Inference):** We trained a simple model on approved loans to predict risk for rejected applicants. We then combined the approved and rejected data, assigning weights to the rejected applicants based on their predicted risk. This helps the main model learn from all applicants (Through-the-Door).</span>
<span id="cb67-2290"><a href="#cb67-2290"></a><span class="ss">*   </span>**Training the Scorecard Model:** We used AutoGluon to build the main model. We trained it twice: once normally, and a second time forcing it to follow the expected feature trends (monotonic constraints).</span>
<span id="cb67-2291"><a href="#cb67-2291"></a><span class="ss">*   </span>**Understanding the Model:** We used plots (PDP/ICE) to visualize how the model made predictions and to confirm it followed the trends we enforced.</span>
<span id="cb67-2292"><a href="#cb67-2292"></a><span class="ss">*   </span>**Checking Performance:** We tested the final model on data it hadn't seen before, using metrics like AUC and the KS statistic. We also converted its risk predictions into easy-to-understand 3-digit scores.</span>
<span id="cb67-2293"><a href="#cb67-2293"></a><span class="ss">*   </span>**Choosing a Cutoff Score:** We used the KS statistic on a separate 'calibration' dataset to determine the minimum score needed for loan approval. We then checked how this cutoff performed on the test data.</span>
<span id="cb67-2294"><a href="#cb67-2294"></a></span>
<span id="cb67-2295"><a href="#cb67-2295"></a>**Summary:**</span>
<span id="cb67-2296"><a href="#cb67-2296"></a>This lab demonstrated a complete process for building a practical credit scorecard. By carefully preparing the data, checking feature relationships, including information from rejected applicants, and using tools like AutoGluon with constraints, we created a model that predicts risk accurately and aligns with business logic. These techniques are valuable for credit risk modeling and other areas where fairness and model understanding are crucial.</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>