---
title: "Lab 03: Counterfactuals & SHAP"
format: 
    html:
        toc: true
        code-fold: true
        code-tools: true
        code-line-numbers: true
        page-layout: full
number-sections: true
number-figures: true
number-tables: true
execute:
    warning: false
    message: false
---

# Introduction

The lab will examine two interpretability techniques:

1.  Counterfactual Explanations
2.  SHAP (SHapley Additive exPlanations)

We will use counterfactual explanations to provide actionable feedback to applicants whose applications were rejected by the model. This is a requirement in the US for credit decisions.

We will use SHAP for both local (individual applicant) and global (overall feature importance) model explanations. SHAP values are versatile, offering alternatives to methods like permutation feature importance, partial dependence plots, ICE plots, and ALE plots.

Furthermore, this lab addresses severe class imbalance, a common challenge in fields like fraud detection. While methods like resampling (undersampling, oversampling, SMOTE) exist, they can introduce issues such as poor probability calibration and overfitting. We will instead use AutoGluon, using class weights (cost-sensitive learning) to build a model that handles imbalance directly during training, mitigating these potential problems.

```{python}
#| label: setup-imports
#| message: false

# System utilities
import os
import shutil
import random
import warnings
import time
import gc
import psutil

# Data manipulation and visualization
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import display # Explicit import for display
from scipy import stats, special
from sklearn.feature_selection import mutual_info_classif
import re
import duckdb

# Machine learning - scikit-learn
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix, classification_report
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.inspection import PartialDependenceDisplay
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.utils.validation import check_is_fitted, check_X_y, check_array
from sklearn import set_config
import torch

# Specialized ML libraries
from autogluon.tabular import TabularPredictor, TabularDataset
from autogluon.common.features.feature_metadata import FeatureMetadata # For monotonic constraints
import shap
from ydata_profiling import ProfileReport

# Counterfactual Explanations (optional, install if needed: pip install dice-ml)
try:
    import dice_ml
    from dice_ml.utils import helpers # Helper functions for DICE
except ImportError:
    print("""
    
    dice-ml not found. You need to update your conda env by running:
    
    conda activate env_AutoGluon_202502
    conda install -c conda-forge dice-ml
    
    """)
    dice_ml = None

# Settings
pd.set_option('display.max_columns', 50)
pd.set_option('display.max_rows', 100)
warnings.filterwarnings('ignore', category=FutureWarning) # Suppress specific FutureWarnings
set_config(transform_output="pandas") # Set sklearn output to pandas

def global_set_seed(seed_value=2025):
    random.seed(seed_value)
    np.random.seed(seed_value)
    torch.manual_seed(seed_value)

global_set_seed(2025)

print("Libraries imported successfully.")



```

```{python}
#| label: helper-functions

class AutoGluonSklearnWrapper(BaseEstimator, ClassifierMixin):
    """
    Scikit-learn compatible wrapper for AutoGluon TabularPredictor
    
    Inherits from scikit-learn's BaseEstimator and ClassifierMixin to provide
    full compatibility with scikit-learn tools like PartialDependenceDisplay().
    
    Parameters
    ----------
    label : str
        Name of the target column

    **predictor_args : dict
        Additional arguments passed to TabularPredictor()
        (e.g., problem_type, eval_metric, path)

    **fit_args : dict
        Additional arguments passed to TabularPredictor.fit() method
        (e.g., holdout_frac, presets, time_limit, excluded_model_types)


    Attributes
    ----------
    predictor : TabularPredictor
        The trained AutoGluon predictor

    classes_ : ndarray
        Class labels (for classification tasks)

    n_features_in_ : int
        Number of features seen during fit

    feature_names_ : list
        Feature names inferred during fitting

    is_fitted_ : bool
        Whether the estimator has been fitted
    """
    
    def __init__(self, label, predictor_args=None, fit_args=None):
        self.label = label
        self.predictor_args = predictor_args if predictor_args else {}
        self.fit_args = fit_args if fit_args else {}
        self.predictor = None
        self.classes_ = None
        self.n_features_in_ = None
        self.feature_names_ = None
        self.is_fitted_ = False

    def __sklearn_is_fitted__(self):
        """Official scikit-learn API for checking fitted status"""
        return self.is_fitted_

    def fit(self, X, y, sample_weight=None):
        """
        Fit AutoGluon model using scikit-learn interface.
        If sample_weight is provided, it is added as a column to X for AutoGluon.
        """
        self._check_feature_names(X, reset=True)
        self._check_n_features(X, reset=True)

        # Convert to DataFrame with preserved feature names
        train_data = pd.DataFrame(X, columns=self.feature_names_)
        train_data[self.label] = y

        # If sample_weight is provided, add it as a column (name must match predictor_args['sample_weight'])
        weight_col_name = self.predictor_args.get('sample_weight', None)
        if sample_weight is not None:
            if weight_col_name:
                train_data[weight_col_name] = sample_weight
            else:
                print("Warning: sample_weight provided to fit, but 'sample_weight' key not found in predictor_args. Weights will be ignored by AutoGluon.")

        train_data = TabularDataset(train_data)

        # Remove sample_weight from fit_args if present (TabularPredictor.fit does not accept it)
        fit_args_clean = {k: v for k, v in self.fit_args.items() if k != 'sample_weight'}

        self.predictor = TabularPredictor(
            label=self.label,
            **self.predictor_args
        ).fit(train_data, **fit_args_clean)

        if self.predictor.problem_type in ['binary', 'multiclass']:
            self.classes_ = np.array(self.predictor.class_labels)

        self.is_fitted_ = True
        return self

    def predict(self, X):
        """
        Make class predictions
        
        Parameters
        ----------
        X : {array-like, sparse matrix} of shape (n_samples, n_features)
            Input data
            
        Returns
        -------
        y_pred : ndarray of shape (n_samples,)
            Predicted class labels
        """
        check_is_fitted(self)
        self._check_feature_names(X)
        self._check_n_features(X)

        df = pd.DataFrame(X, columns=self.feature_names_)
        df = TabularDataset(df)

        return self.predictor.predict(df).values

    def predict_proba(self, X):
        """
        Predict class probabilities
        
        Parameters
        ----------
        X : {array-like, sparse matrix} of shape (n_samples, n_features)
            Input data
            
        Returns
        -------
        proba : ndarray of shape (n_samples, n_classes)
            Class probabilities
        """
        check_is_fitted(self)
        self._check_feature_names(X)
        self._check_n_features(X)

        df = pd.DataFrame(X, columns=self.feature_names_)
        df = TabularDataset(df)

        return self.predictor.predict_proba(df).values

    def get_params(self, deep=True):
        """Get parameters for this estimator"""
        return {
            'label': self.label,
            'predictor_args': self.predictor_args,
            'fit_args': self.fit_args
        }

    def set_params(self, **params):
        """Set parameters for this estimator"""
        for param, value in params.items():
            if param == 'label':
                self.label = value
            else:
                self.predictor_args[param] = value
        return self

    def _check_n_features(self, X, reset=False):
        """Validate number of features"""
        n_features = X.shape[1]
        if reset:
            self.n_features_in_ = n_features
        elif n_features != self.n_features_in_:
            raise ValueError(f"Expected {self.n_features_in_} features, got {n_features}")

    def _check_feature_names(self, X, reset=False):
        """Validate feature names (AutoGluon requirement)"""
        if reset:
            if isinstance(X, np.ndarray):
                self.feature_names_ = [f'feat_{i}' for i in range(X.shape[1])]
            else:
                self.feature_names_ = X.columns.tolist()
        elif hasattr(X, 'columns'):
            if list(X.columns) != self.feature_names_:
                raise ValueError("Feature names mismatch between fit and predict")

def load_autogluon(folder_path: str, persist_model: bool = False) -> AutoGluonSklearnWrapper:
    """
    Loads a pre-trained AutoGluon TabularPredictor from a specified path
    into an AutoGluonSklearnWrapper instance.

    Parameters
    ----------
    folder_path : str
        The path to the directory containing the saved AutoGluon predictor.
    persist_model : bool, default=False
        If True, calls predictor.persist() after loading to potentially
        speed up future predictions by loading models into memory.

    Returns
    -------
    AutoGluonSklearnWrapper
        An instance of the wrapper containing the loaded predictor and its metadata.

    Raises
    ------
    FileNotFoundError
        If the specified folder_path does not exist or doesn't contain a valid predictor.
    Exception
        If any other error occurs during loading or processing.
    """
    try:
        print(f"Loading AutoGluon predictor from: {folder_path}")
        predictor = TabularPredictor.load(folder_path)
        print("Predictor loaded successfully.")

        # Extract metadata from the loaded predictor
        label = predictor.label
        feature_names = predictor.feature_metadata_in.get_features() # Get feature names used during training
        n_features = len(feature_names)
        classes = None
        if predictor.problem_type in ['binary', 'multiclass']:
            classes = np.array(predictor.class_labels)

        # Instantiate the wrapper
        # Note: predictor_args and fit_args used during original training aren't easily accessible
        # We initialize them as empty dicts here.
        wrapper = AutoGluonSklearnWrapper(label=label, predictor_args={}, fit_args={})

        # Assign the loaded predictor and metadata to the wrapper
        wrapper.predictor = predictor
        wrapper.classes_ = classes
        wrapper.n_features_in_ = n_features
        wrapper.feature_names_ = feature_names
        wrapper.is_fitted_ = True # Mark as fitted

        # Optionally persist the model
        if persist_model:
            print("Persisting predictor models in memory...")
            # This loads models into memory for faster predictions, requires sufficient RAM
            wrapper.predictor.persist()
            print("Predictor models persisted.")

        print("AutoGluonSklearnWrapper created and populated successfully.")
        return wrapper

    except FileNotFoundError:
        print(f"Error: Predictor directory not found at {folder_path}")
        raise
    except Exception as e:
        print(f"An error occurred while loading the predictor or creating the wrapper: {e}")
        import traceback
        traceback.print_exc()
        raise


```

# Load Data and Pre-trained Model

```{python}
#| label: load-data

df_ttd_train = pd.read_parquet("../Data/lendingclub/lendingclub_ttd_train.parquet")
df_ttd_test = pd.read_parquet("../Data/lendingclub/lendingclub_ttd_test_cloning.parquet")
df_ttd_test_noCloning = pd.read_parquet("../Data/lendingclub/lendingclub_ttd_test_noCloning.parquet")

```

```{python}
#| label: load-model

model_path = "Lab02_ag_models_TTD_Constrained"

AutoGluon_TTD_model = load_autogluon(
    folder_path=model_path,
    persist_model=True
)
```

# Counterfactual Explanations

## Overview

**Concept:** Counterfactual explanations identify the minimal changes needed to alter a predicted label. For example, if an applicant is rejected, a counterfactual explanation would suggest how their features could be adjusted to achieve approval.

First, we need to score the training and test sets to identify the applicants that were accepted and rejected by the model. Note that this is different than the default flag, which is the target variable. The model decision is based on the **predicted** probability of default (PD) and a threshold.

Also keep in mind that `model_rejection = 1` means the applicant was rejected by the model, while `model_rejection = 0` means the applicant was accepted.

```{python}
#| label: score-data

# append predicted labels to the training and test sets

df_ttd_train['model_rejection'] = AutoGluon_TTD_model.predict(
    df_ttd_train[AutoGluon_TTD_model.feature_names_]
)

df_ttd_test['model_rejection'] = AutoGluon_TTD_model.predict(
    df_ttd_test[AutoGluon_TTD_model.feature_names_]
)
```

```{python}
#| label: tbl-training-decisions

keep_cols = AutoGluon_TTD_model.feature_names_ + ['model_rejection']

display(df_ttd_train[keep_cols].sample(20, random_state=2024).head(10))
```

```{python}
#| label: tbl-test-decisions

display(df_ttd_test[keep_cols].sample(20, random_state=2024).head(10))

```

Second, we randomly select 1 rejected applicant from the test set to explain. This applicant will be used to demonstrate the counterfactual explanation process.

```{python}
#| label: tbl-sample-applicant

sample_reject = df_ttd_test[keep_cols][df_ttd_test.model_rejection == 1].sample(1, random_state=2022)

display(sample_reject)
```

## Counterfactual Explainer

Setup an `dice-explainer` using the `dice_ml` library. The explainer will be used to generate counterfactuals for the rejected applicant.

The explainer needs two objects:

1.  Data with the predicted labels (`model_rejection`).
2.  The trained model used to generate prediction labels.

```{python}
#| label: setup-counterfactuals

dice_data = dice_ml.Data(
    dataframe=df_ttd_train[keep_cols],
    continuous_features=AutoGluon_TTD_model.feature_names_,
    outcome_name="model_rejection"
)

dice_model = dice_ml.Model(
    model=AutoGluon_TTD_model,
    backend='sklearn'
)

dice_explainer = dice_ml.Dice(
    data_interface=dice_data,
    model_interface=dice_model,
    method="kdtree"
)

```

## Counterfactual Explanations

```{python}
#| label: generate-counterfactuals

threshold = AutoGluon_TTD_model.predictor.decision_threshold # Use decision_threshold

user_permitted_range = {
    'loan_amnt': [500, sample_reject['loan_amnt'].iloc[0]],
    'dti': [0, sample_reject['dti'].iloc[0]],
    'credit_score': [sample_reject['credit_score'].iloc[0], 850],
    'emp_length': [sample_reject['emp_length'].iloc[0], 10]
}

dice_exp = dice_explainer.generate_counterfactuals(
    query_instances=sample_reject[AutoGluon_TTD_model.feature_names_],
    total_CFs=5,
    permitted_range=user_permitted_range
    )

# Extract the counterfactual features (scaled)
df_counterfactuals = dice_exp.cf_examples_list[0].final_cfs_df

```

```{python}
#| label: tbl-rejected-applicant
#| tbl-cap: "Rejected applicant"

display(sample_reject)

```

```{python}
#| label: tbl-counterfactuals
#| tbl-cap: "Counterfactuals for rejected applicant"


display(df_counterfactuals)
```

Verify that the counterfactuals provided would have generated an approval by the TTD model.

```{python}
#| label: tbl-verify-counterfactuals
#| tbl-cap: "Verify counterfactuals"

df_counterfactuals['verify_model_rejection'] = AutoGluon_TTD_model.predict(
    df_counterfactuals[AutoGluon_TTD_model.feature_names_]
)

display(df_counterfactuals)
```

**Interpretation:** The output shows alternative feature values (the counterfactuals) that would have led to an 'Approve' decision. This highlights the key factors driving the rejection and provides concrete suggestions for the applicant.

# Generate Adverse Action Codes

The script below generates a list of adverse action codes based on differences between `sample_reject` and `df_counterfactuals`. It does this by comparing the values of each feature, and if a value differs between the two datasets, it appends a predefined message to the adverse action codes list.

By comparing each feature of the original sample against the counterfactual, the code effectively identifies which factors (such as a too-high loan amount or a low credit score) contributed to the adverse decision and communicates those reasons clearly to the applicant.

```{python}
#| label: adverse-action-codes
#| tbl-cap: "Adverse Action Codes"

def generate_adverse_action_codes(df_rejected, df_counterfactuals, dict_feature_to_action, list_features_to_compare):
    """
    Generate adverse action codes based on differences between a rejected applicant and its counterfactual.

    Args:
        df_rejected (DataFrame): DataFrame with the rejected applicant's data (assumed to be a single row).
        df_counterfactuals (DataFrame): DataFrame containing counterfactual data (first row is used).
        dict_feature_to_action (dict): Mapping from feature names to their corresponding adverse action message.
        list_features_to_compare (list): List of feature names to compare between the rejected and counterfactual data.

    Returns:
        list: A list of adverse action codes as strings.
    """
    adverse_action_codes = []

    for feature in list_features_to_compare:
        original_value = df_rejected[feature].iloc[0]
        counterfactual_value = df_counterfactuals[feature].iloc[0]
        if original_value != counterfactual_value:
            adverse_action_codes.append(dict_feature_to_action[feature])

    return adverse_action_codes
```

```{python}
#| label: adverse-action-codes-example
feature_to_action_code = {
    'loan_amnt': "Loan amount too high",
    'dti': "Debt-to-income ratio too high",
    'credit_score': "Credit score too low",
    'emp_length': "Employment length too short"
}

action_codes = generate_adverse_action_codes(
    sample_reject, 
    df_counterfactuals, 
    feature_to_action_code, 
    AutoGluon_TTD_model.feature_names_
)

print("Adverse Action Codes:")
for code in action_codes:
    print(f"- {code}")
```

# Improved Adverse Action Codes

A major drawback of this approach is that the order of the generated codes depends on the order of the features in the list `AutoGluon_TTD_model.feature_names_`. Ideally (and legally), the order should be based on the importance of the features in the model decision.

We can re-order the elements in the list based on permutation feature importance (PFI) scores. The code below generates the PFI scores and re-orders the adverse action codes based on the PFI scores.

```{python}
#| label: permutation-feature-importance

pfi = AutoGluon_TTD_model.predictor.feature_importance(
    df_ttd_test[AutoGluon_TTD_model.feature_names_ + ['default_flag']],
    silent=True
)

important_features = pfi.index.tolist()
```

```{python}
#| label: improved-adverse-action-codes

improved_action_codes = generate_adverse_action_codes(
    sample_reject, 
    df_counterfactuals, 
    feature_to_action_code, 
    important_features
)

print("Improved Adverse Action Codes:")
for code in improved_action_codes:
    print(f"- {code}")
```

Permutation feature importance provides a global ranking of feature impacts in the model. By ordering the adverse action codes according to this ranking, we ensure that the most important features are highlighted first. This is crucial for effective communication with applicants, as it emphasizes the most significant factors influencing their application outcome.

However, global ranking of feature impacts are not always equal to the local ranking of feature impacts. For example, the most important feature for the model may not be the most important feature for a specific applicant. This is a limitation of the PFI approach. SHAP values can be used to generate local rankings of feature impacts.

# SHAP Values

## Concept

The difference between the predicted probability of default ($\widehat{PD_i}$) for an applicant ($i$) and the average predicted PD ($\overline{PD}$) for a given data set can be decomposed into the contributions of each feature. The contributions are called SHAP values.

Suppose a credit scorecard model had 4 features. Then, for applicant $i$, there would be 4 SHAP values, one for each feature. The SHAP values can be interpreted as the contribution of each feature to the difference between the predicted PD and the average PD.

$\widehat{PD_i} - \overline{PD} = S(i,DTI) + S(i,CreditScore) + S(i,EmpLength) + S(i,LoanAmnt)$

For a given data set (e.g., training set), $\overline{PD}$ is constant. Therefore, the SHAP values can be interpreted as the contribution of each feature to the predicted PD.

$\widehat{PD_i} = \overline{PD} + S(i,DTI) + S(i,CreditScore) + S(i,EmpLength) + S(i,LoanAmnt)$

SHAP values could be negative or positive. A negative SHAP value means that the feature decreases the predicted PD, while a positive SHAP value means that the feature increases the predicted PD.

## Setup SHAP Explainer

Initialize the SHAP explainer using the constrained model and a background dataset (usually a sample of the training data) to represent the baseline prediction.

Notice that the background dataset does not contain the true label. The SHAP explainer uses the background dataset to calculate the expected value of the model's predictions. This is important because the SHAP values are calculated as the difference between the predicted value and the expected value.

```{python}
#| label: setup-shap

background_data_sample = df_ttd_train[AutoGluon_TTD_model.feature_names_].sample(60_000, random_state=2025)

shap_explainer = shap.Explainer(lambda x: AutoGluon_TTD_model.predict_proba(x)[:, 1], background_data_sample)

```

## Local Interpretability (Waterfall Plots)

Calculate SHAP values for the 1 rejected applicant and plot the SHAP values using a waterfall plot. The waterfall plot shows the contribution of each feature to the predicted PD for that specific applicant.

```{python}
#| label: calculate-local-shap

# Generate SHAP values for the rejected applicant
shap_values = shap_explainer(sample_reject[AutoGluon_TTD_model.feature_names_])

# Display the waterfall plot for the first (and only) instance
shap.plots.waterfall(shap_values[0])
```

**Interpretation:** The baseline PD is 23.4%. The features `credit_score`, `dti`, and `emp_length` have positive SHAP values, meaning they increase the predicted PD. The feature `loan_amnt` has a negative SHAP value, meaning it decreases the predicted PD. The aggregate SHAP value for the applicant is 29%, which means that the applicant's predicted PD is 23.4% + 29% = 52.4%.

```{python}
#| label: tbl-local-shap-values

df_local_shap = pd.DataFrame({
    'feature': AutoGluon_TTD_model.feature_names_,
    'shap_value': shap_values[0].values
})
df_local_shap['abs_shap_value'] = df_local_shap['shap_value'].abs()
df_local_shap.sort_values('abs_shap_value', ascending=False, inplace=True)

display(df_local_shap)

```

When we generate a ranked list of features that affected the predicted PD, we should use the **absolute value** of the SHAP values. The absolute value of the SHAP value indicates the magnitude of the impact, regardless of whether it is positive or negative.

The feature that contributes the most to the predicted PD is `credit_score`, followed by `dti` and `loan_amnt`. The feature `emp_length` has the least impact on the predicted PD.

::: callout-warning
**Important Point: Understanding a Single SHAP Value**

Think of the SHAP values shown in the waterfall plot for one applicant:

-   **What they explain:** Each SHAP value shows how a specific feature *value* for *that particular applicant* pushed their prediction away from the average prediction (the baseline `E[f(X)]`). For instance, the positive SHAP value for `credit_score` means *this applicant's specific credit score* increased their predicted Probability of Default (PD) compared to the average.

-   **What they DON'T explain:**

    -   A single SHAP value for one applicant doesn't tell you the *overall trend* or *marginal effect* of that feature. It doesn't answer the question: "In general, if someone increases their `credit_score`, will their predicted PD decrease?" We should use dependence plots to answer this question.

    -   Nor does the SHAP value answer the question for the rejected applicant: "If the `credit_score` was 20 points higher, would the predicted PD be lower for the applicant?" We should use counterfactuals to answer this question.

:::

# Local Adverse Action Codes

The code below generates adverse action codes for the 1 rejected applicant based on the SHAP values. It uses the same logic as before, but now it uses the SHAP values to determine which features are most important for our specific applicant.

```{python}
#| label: local-adverse-action-codes


shap_local_important_features = df_local_shap['feature'].tolist()

shap_action_codes = generate_adverse_action_codes(
    sample_reject, 
    df_counterfactuals, 
    feature_to_action_code, 
    shap_local_important_features
)

print("SHAP Adverse Action Codes:")
for code in shap_action_codes:
    print(f"- {code}")
```

# Concluding Remarks for Adverse Action Codes

Subsequent sections of the lab will show other uses of SHAP values and a potpourri of data science topics. Before we address those topics, let's quickly summarize adverse action codes and the process used to generate them.

1.  Each rejected applicant must be given a list of reasons for rejection. This is a legal requirement in the US.

2.  The list of reasons should be ordered from most impactful to least impactful. This is also a legal requirement in the US.

3.  To generate the list of adverse action codes, we needed to find an example of an **approved applicant** that was very similar to the rejected applicant. This was accomplished using **counterfactual explanations**.

4.  The order of the adverse action codes was based on the **permutation feature importance** (PFI) scores, representing a global ranking of feature impacts in the model.

5.  Alternatively, we could have used **SHAP values** to generate a local ranking of feature impacts, providing a more accurate ranking for the specific applicant.

# Other Uses of SHAP Values

## Global Interpretability (Summary Plots)

Aggregate SHAP values across a larger sample (e.g., the test set) to understand overall feature importance and effects.

Bar Plot: Ranks features by their average impact (mean absolute SHAP value) across the sample. Higher bars mean more influence overall.

```{python}
#| label: fig-global-bar-chart

test_set_sample = df_ttd_test[AutoGluon_TTD_model.feature_names_].sample(1000, random_state=2025)

shap_values_global = shap_explainer(test_set_sample)

shap.plots.bar(shap_values_global, max_display=10, show=True)
```

Show the bar plot as a data frame.

```{python}
#| label: tbl-global-bar-chart

# Calculate mean absolute SHAP values
mean_abs_shap = np.abs(shap_values_global.values).mean(axis=0)

# Create DataFrame
df_shap_summary = pd.DataFrame({
    'feature': test_set_sample.columns,
    'mean_abs_shap_value': mean_abs_shap
})

# Sort by mean absolute SHAP value (descending)
df_shap_summary = df_shap_summary.sort_values('mean_abs_shap_value', ascending=False).reset_index(drop=True)

# Display the DataFrame
display(df_shap_summary)
```

Summary (Beeswarm) Plot: Shows each feature's SHAP value distribution. Each point is one instance. Color indicates the feature's value (high/low). This reveals not just importance but also the direction of the effect (e.g., high credit_score (red points) have negative SHAP values, decreasing default probability).

```{python}
#| label: fig-global-beeswarm

shap.plots.beeswarm(shap_values_global, max_display=10, show=True)
```

## SHAP Dependence Plots

Explore how a feature's impact (SHAP value) changes with its value, potentially colored by an interacting feature.

These plots show the relationship between a feature's value (x-axis) and its SHAP value (y-axis). The vertical spread, often colored by an interacting feature, highlights how the feature's impact might depend on other factors.

The SHAP dependence plot is an alternative to PDP, ICE, and ALE plots. It provides a more detailed view of how a feature's value affects the model's prediction, especially when interactions with other features are present.

```{python}
#| label: fig-shap-dependence
#| code-summary: "Generate SHAP dependence plots for key features."

features_for_dependence = AutoGluon_TTD_model.feature_names_

for feature in features_for_dependence:
    if feature in test_set_sample.columns: # Use the defined sample DataFrame
        plt.figure()
        shap.plots.scatter(shap_values_global[:, feature], color=shap_values_global, alpha=0.4, show=True)
```
